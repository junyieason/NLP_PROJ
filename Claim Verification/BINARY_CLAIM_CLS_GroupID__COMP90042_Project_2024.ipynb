{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPitXZzTsyTn",
        "outputId": "ef561ddb-70cd-47fe-b133-e9321ebfa74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VcWPFMqtJ4x"
      },
      "outputs": [],
      "source": [
        "import torchdata.datapipes as dp\n",
        "import torchtext.transforms as T\n",
        "import spacy\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "eng = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ohyt_MA86_7e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "evidences = pd.read_json('/content/drive/MyDrive/nlp/data/evidence.json', orient='index')\n",
        "train_claims = pd.read_json('/content/drive/MyDrive/nlp/data/train-claims.json', orient='index')\n",
        "dev_claims = pd.read_json('/content/drive/MyDrive/nlp/data/dev-claims.json', orient='index')\n",
        "\n",
        "#update column names\n",
        "evidences.reset_index(inplace=True)\n",
        "evidences.columns = ['evidence_id', 'evidence_text']\n",
        "\n",
        "train_claims.reset_index(inplace=True)\n",
        "train_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "dev_claims.reset_index(inplace=True)\n",
        "dev_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "evidence_id = evidences['evidence_id']\n",
        "evidence_text = evidences['evidence_text']\n",
        "evidence_idx = evidences.index.tolist()\n",
        "\n",
        "evidence_id_dict = dict(zip(evidence_id, evidence_idx))\n",
        "\n",
        "train_claims_text = train_claims['claim_text']\n",
        "train_evidence_ids = train_claims['evidences']\n",
        "train_claim_labels = train_claims['claim_label']\n",
        "#map evidence_id to their corrosponding index for faster processing\n",
        "train_evidence_idxs = train_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])\n",
        "\n",
        "dev_claims_text = dev_claims['claim_text']\n",
        "dev_claim_labels = dev_claims['claim_label']\n",
        "dev_evidence_ids = dev_claims['evidences']\n",
        "dev_evidence_idxs = dev_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_claims = pd.read_json('/content/drive/MyDrive/nlp/data/test-claims-unlabelled.json', orient='index')\n",
        "test_claims.reset_index(inplace=True)\n",
        "test_claims.columns = ['claim_id', 'claim_text']\n",
        "test_claims_text = test_claims['claim_text']\n",
        "test_claims_id = test_claims['claim_id']"
      ],
      "metadata": {
        "id": "eEmCnO6TkYRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_claim_ids = dev_claims['claim_id']"
      ],
      "metadata": {
        "id": "YV2PK7ldyQao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "dev_evidence_indices = json.load(open(\"/content/drive/MyDrive/nlp/data/reranked_indices.json\", \"r\"))\n",
        "test_evidence_indices = json.load(open(\"/content/drive/MyDrive/nlp/data/test_reranked_indices.json\", \"r\"))"
      ],
      "metadata": {
        "id": "6eWAVRHbYL_j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_k_indices = [sublist[:5] if len(sublist) >= 5 else sublist + [None] * (5 - len(sublist)) for sublist in dev_evidence_indices]\n",
        "test_k_indices = [sublist[:5] if len(sublist) >= 5 else sublist + [None] * (5 - len(sublist)) for sublist in test_evidence_indices]"
      ],
      "metadata": {
        "id": "J12VT62YYjhd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t9hCn0g-7Ae2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1034bd-c2b4-4182-e054-dba8cff9af06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_data(text):\n",
        "    tokens = tt.tokenize(text.lower())\n",
        "    return tokens\n",
        "\n",
        "train_claims_text_processed = train_claims_text.apply(preprocess_data)\n",
        "dev_claims_text_processed = dev_claims_text.apply(preprocess_data)\n",
        "evidence_text_processed = evidence_text.apply(preprocess_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_claims_text_processed = test_claims_text.apply(preprocess_data)"
      ],
      "metadata": {
        "id": "uEOtHTXWlHUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_claims_text_processed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRWZAXwYlK-7",
        "outputId": "ec7e9832-51a9-4040-8d30-3eb4d5bb57af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      [‘, this, study, goes, beyond, statistical, co...\n",
              "1      [a, recent, study, in, nature, geoscience, ,, ...\n",
              "2      [‘, arctic, ice, conditions, have, been, track...\n",
              "3      [“, the, global, reef, crisis, does, not, nece...\n",
              "4      [a, second, coat, of, paint, has, much, less, ...\n",
              "                             ...                        \n",
              "148    [the, cement, ,, iron, and, steel, ,, and, pet...\n",
              "149    [‘, we, could, be, decades, too, fast, ,, or, ...\n",
              "150    [the, alaskan, tundra, is, warming, so, quickl...\n",
              "151    [“, arctic, land, stores, about, twice, as, mu...\n",
              "152    [“, warm, weather, worsened, the, most, recent...\n",
              "Name: claim_text, Length: 153, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, OrderedDict\n",
        "def build_vocab(texts, min_freq=3):\n",
        "    # Count all the words\n",
        "    word_freq = Counter()\n",
        "    for text in texts:\n",
        "        word_freq.update(text)\n",
        "\n",
        "    # Start vocab from special tokens\n",
        "    vocab = OrderedDict({\n",
        "        \"<pad>\": 0,\n",
        "        \"<unk>\": 1,\n",
        "        \"<sos>\": 2,\n",
        "        \"<eos>\": 3\n",
        "    })\n",
        "    index = 4  # Start indexing from 4 because 0-3 are reserved for special tokens\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:  # Only include words that meet the frequency threshold\n",
        "            vocab[word] = index\n",
        "            index += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# Build vocabulary using only evidence texts and applying the frequency threshold\n",
        "vocab = build_vocab(evidence_text_processed, min_freq=3)"
      ],
      "metadata": {
        "id": "T_4DV847328f"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FHipd3IplZsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_claims_numerical = test_claims_text_processed.apply(lambda x: numericalize(x, vocab))"
      ],
      "metadata": {
        "id": "gslxstqulj6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\n",
        "    \"REFUTES\": 0,\n",
        "    \"SUPPORTS\": 1,\n",
        "    \"NOT_ENOUGH_INFO\": 2,\n",
        "    \"DISPUTED\": 3\n",
        "}\n",
        "train_claim_labels = train_claims['claim_label'].map(label_map)\n",
        "dev_claim_labels = dev_claims['claim_label'].map(label_map)"
      ],
      "metadata": {
        "id": "99CY84id6ZJN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def text_to_indices(text, vocab):\n",
        "    return [vocab.get(word, vocab[\"<unk>\"]) for word in text]\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "class BinaryClaimEvidenceDataset(Dataset):\n",
        "    def __init__(self, claims, evidence_indices, evidences, claim_labels, vocab):\n",
        "        self.claims = claims\n",
        "        self.evidence_indices = evidence_indices\n",
        "        self.evidences = evidences\n",
        "        self.claim_labels = claim_labels\n",
        "        self.vocab = vocab\n",
        "        self.pairs = self.create_pairs()\n",
        "\n",
        "    def create_pairs(self):\n",
        "        pairs = []\n",
        "        for idx, claim in enumerate(self.claims):\n",
        "            label = self.claim_labels[idx]\n",
        "            if label in [0, 1]:  # Only consider REFUTES (0) and SUPPORTS (1)\n",
        "                candidate_pos_indices = self.evidence_indices[idx]\n",
        "                positive_evidences = [self.evidences[i] for i in candidate_pos_indices]\n",
        "                for evidence in positive_evidences:\n",
        "                    pairs.append((claim, evidence, label))\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        claim, evidence, label = self.pairs[idx]\n",
        "        claim_indices = text_to_indices(claim, self.vocab)\n",
        "        evidence_indices = text_to_indices(evidence, self.vocab)\n",
        "        claim_indices = [self.vocab[\"<sos>\"]] + claim_indices + [self.vocab[\"<eos>\"]]\n",
        "        evidence_indices = [self.vocab[\"<sos>\"]] + evidence_indices + [self.vocab[\"<eos>\"]]\n",
        "        return claim_indices, evidence_indices, label\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    claims, evidences, labels = zip(*batch)\n",
        "    claims_tensor = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in claims], batch_first=True, padding_value=vocab[\"<pad>\"]).to(device)\n",
        "    evidences_tensor = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in evidences], batch_first=True, padding_value=vocab[\"<pad>\"]).to(device)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "    return claims_tensor, evidences_tensor, labels_tensor\n",
        "\n",
        "\n",
        "# Prepare the data\n",
        "train_dataset = BinaryClaimEvidenceDataset(train_claims_text_processed, train_evidence_idxs, evidence_text_processed, train_claim_labels, vocab)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
        "dev_dataset = BinaryClaimEvidenceDataset(dev_claims_text_processed, dev_evidence_idxs, evidence_text_processed, dev_claim_labels, vocab)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate_fn)\n"
      ],
      "metadata": {
        "id": "viPwRuD789ab"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW618ZKoLNNn",
        "outputId": "261c6b65-d0c2-4ae2-dd53-7587606f4c59"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[    2,  1802,  1756, 15963,  7915,  9555,   112,   534, 14083,     7,\n",
            "         14102,     7,    10, 52150,    31,     3,  3399,     7,  4881, 59233,\n",
            "          6915, 12663,     7, 12754,     7,  1807,     7, 52150,     7, 11756,\n",
            "            10,  2057,  5862, 14083,   279,  1392,    25,     1,  2060,  7387,\n",
            "         12662,   126,  5401,  1544,    10,  8777, 12697,  1267,    19,   172,\n",
            "            31,     3]], device='cuda:0'), tensor([1.], device='cuda:0'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Bi-Directional GRU with Attention"
      ],
      "metadata": {
        "id": "QMmZlSPXoeM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# class ClaimEvidenceModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
        "#         super(ClaimEvidenceModel, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "#         self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "#         self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "#         self.attention = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
        "#         self.pad_idx = pad_idx\n",
        "\n",
        "#     def forward(self, claims, evidence_lists):\n",
        "#         # Embed and encode claims\n",
        "#         embedded_claims = self.embedding(claims)\n",
        "#         claims_mask = (claims != self.pad_idx).unsqueeze(2).float()\n",
        "#         embedded_claims *= claims_mask\n",
        "\n",
        "#         _, hidden_claims = self.gru(embedded_claims)\n",
        "#         hidden_claims = torch.cat((hidden_claims[-2,:,:], hidden_claims[-1,:,:]), dim=1)\n",
        "\n",
        "#         batch_size = claims.size(0)\n",
        "#         max_logits = torch.full((batch_size, self.fc.out_features), float('-inf'), device=claims.device)\n",
        "\n",
        "#         for i in range(batch_size):\n",
        "#             evidences = evidence_lists[i]\n",
        "#             embedded_evidences = self.embedding(evidences)\n",
        "#             evidences_mask = (evidences != self.pad_idx).unsqueeze(2).float()\n",
        "#             embedded_evidences *= evidences_mask\n",
        "\n",
        "#             _, hidden_evidences = self.gru(embedded_evidences)\n",
        "#             hidden_evidences = torch.cat((hidden_evidences[-2,:,:], hidden_evidences[-1,:,:]), dim=1)\n",
        "\n",
        "#             logits = self.fc(hidden_evidences)\n",
        "#             attention_weights = F.softmax(self.attention(hidden_evidences), dim=0)\n",
        "#             evidence_representation = torch.sum(attention_weights * logits, dim=0, keepdim=True)\n",
        "\n",
        "#             max_logits[i, :] = torch.max(logits, dim=0).values\n",
        "\n",
        "#         return F.log_softmax(max_logits, dim=1)\n",
        "\n",
        "class ClaimEvidenceModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
        "        super(ClaimEvidenceModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, claims, evidences):\n",
        "        # Embed and encode claims\n",
        "        embedded_claims = self.embedding(claims)\n",
        "        claims_mask = (claims != self.pad_idx).unsqueeze(2).float()\n",
        "        embedded_claims *= claims_mask\n",
        "\n",
        "        _, hidden_claims = self.gru(embedded_claims)\n",
        "        hidden_claims = torch.cat((hidden_claims[-2,:,:], hidden_claims[-1,:,:]), dim=1)\n",
        "\n",
        "        # Embed and encode evidences\n",
        "        embedded_evidences = self.embedding(evidences)\n",
        "        evidences_mask = (evidences != self.pad_idx).unsqueeze(2).float()\n",
        "        embedded_evidences *= evidences_mask\n",
        "\n",
        "        _, hidden_evidences = self.gru(embedded_evidences)\n",
        "        hidden_evidences = torch.cat((hidden_evidences[-2,:,:], hidden_evidences[-1,:,:]), dim=1)\n",
        "\n",
        "        # Combine claim and evidence representations\n",
        "        combined_representation = hidden_claims + hidden_evidences\n",
        "        logits = self.fc(combined_representation)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Model instantiation\n",
        "model = ClaimEvidenceModel(vocab_size=len(vocab), embedding_dim=100, hidden_dim=256, output_dim=len(label_map), pad_idx=vocab['<pad>'])\n"
      ],
      "metadata": {
        "id": "ZahBwCH8BM-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Transformer with self-attention"
      ],
      "metadata": {
        "id": "KFFwgHBBo69T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch import Tensor\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(0)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: torch.Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:, :token_embedding.size(1), :])\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dim_feedforward, dropout):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(emb_size, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.linear1 = nn.Linear(emb_size, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, emb_size)\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = F.relu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None, src_key_padding_mask: torch.Tensor = None):\n",
        "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, emb_size):\n",
        "        super(AttentionPooling, self).__init__()\n",
        "        self.attention = nn.Linear(emb_size, 1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x: [batch_size, seq_len, emb_size]\n",
        "        # mask: [batch_size, seq_len]\n",
        "        scores = self.attention(x)  # [batch_size, seq_len, 1]\n",
        "        scores = scores.masked_fill(mask.unsqueeze(-1) == 0, float('-inf'))  # Mask padding tokens\n",
        "        attn_weights = F.softmax(scores, dim=1)  # [batch_size, seq_len, 1]\n",
        "        context = torch.sum(attn_weights * x, dim=1)  # [batch_size, emb_size]\n",
        "        return context\n",
        "\n",
        "\n",
        "class ClaimVerificationTransformer(nn.Module):\n",
        "    def __init__(self, num_layers: int, emb_size: int, nhead: int, vocab_size: int, dim_feedforward: int, dropout: float = 0.1, maxlen: int = 5000, pad_idx: int = 0):\n",
        "        super(ClaimVerificationTransformer, self).__init__()\n",
        "        self.embedding = TokenEmbedding(vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout, maxlen)\n",
        "        self.transformer_claim = nn.ModuleList([\n",
        "            TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.transformer_evidence = nn.ModuleList([\n",
        "            TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.attention_pooling = AttentionPooling(emb_size)\n",
        "        self.fc = nn.Linear(emb_size, 1)  # Single output for binary classification\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, claims: torch.Tensor, evidences: torch.Tensor):\n",
        "        # Embed and encode claims\n",
        "        embedded_claims = self.embedding(claims)\n",
        "        embedded_claims = self.positional_encoding(embedded_claims)\n",
        "        claims_mask = (claims != self.pad_idx)\n",
        "\n",
        "        for layer in self.transformer_claim:\n",
        "            embedded_claims = layer(embedded_claims, src_key_padding_mask=~claims_mask)\n",
        "        claims_encoded = self.attention_pooling(embedded_claims, claims_mask)  # Attention pooling\n",
        "\n",
        "        # Embed and encode evidences\n",
        "        embedded_evidences = self.embedding(evidences)\n",
        "        embedded_evidences = self.positional_encoding(embedded_evidences)\n",
        "        evidences_mask = (evidences != self.pad_idx)\n",
        "\n",
        "        for layer in self.transformer_evidence:\n",
        "            embedded_evidences = layer(embedded_evidences, src_key_padding_mask=~evidences_mask)\n",
        "        evidences_encoded = self.attention_pooling(embedded_evidences, evidences_mask)  # Attention pooling\n",
        "\n",
        "        # Combine claim and evidence representations\n",
        "        combined_representation = claims_encoded + evidences_encoded\n",
        "        logits = self.fc(combined_representation)\n",
        "        return logits\n",
        "\n",
        "# class ClaimVerificationTransformer(nn.Module):\n",
        "#     def __init__(self, num_layers: int, emb_size: int, nhead: int, vocab_size: int, dim_feedforward: int, num_classes: int, dropout: float = 0.1, maxlen: int = 5000, pad_idx: int = 0):\n",
        "#         super(ClaimVerificationTransformer, self).__init__()\n",
        "#         self.embedding = TokenEmbedding(vocab_size, emb_size)\n",
        "#         self.positional_encoding = PositionalEncoding(emb_size, dropout, maxlen)\n",
        "#         self.transformer_claim = nn.ModuleList([\n",
        "#             TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout)\n",
        "#             for _ in range(num_layers)\n",
        "#         ])\n",
        "#         self.transformer_evidence = nn.ModuleList([\n",
        "#             TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout)\n",
        "#             for _ in range(num_layers)\n",
        "#         ])\n",
        "#         self.word_attention = nn.Linear(emb_size, 1, bias=False)\n",
        "#         self.sentence_attention = nn.Linear(emb_size, 1, bias=False)\n",
        "#         self.fc = nn.Linear(emb_size, num_classes)\n",
        "#         self.pad_idx = pad_idx\n",
        "\n",
        "#     def forward(self, claims: Tensor, evidence_lists: Tensor):\n",
        "#         # Embed and encode claims\n",
        "#         embedded_claims = self.embedding(claims)\n",
        "#         embedded_claims = self.positional_encoding(embedded_claims)\n",
        "#         claims_mask = (claims == self.pad_idx)\n",
        "\n",
        "#         for layer in self.transformer_claim:\n",
        "#             embedded_claims = layer(embedded_claims, src_key_padding_mask=claims_mask)\n",
        "#         claims_encoded, _ = torch.max(embedded_claims, dim=1)  # Max pooling\n",
        "\n",
        "#         batch_size = claims.size(0)\n",
        "#         evidence_representations = []\n",
        "\n",
        "#         for i in range(batch_size):\n",
        "#             evidences = evidence_lists[i]\n",
        "#             embedded_evidences = self.embedding(evidences)\n",
        "#             embedded_evidences = self.positional_encoding(embedded_evidences)\n",
        "#             evidences_mask = (evidences == self.pad_idx)\n",
        "\n",
        "#             for layer in self.transformer_evidence:\n",
        "#                 embedded_evidences = layer(embedded_evidences, src_key_padding_mask=evidences_mask)\n",
        "#             evidences_encoded, _ = torch.max(embedded_evidences, dim=1)  # Max pooling\n",
        "\n",
        "#             # Word-level attention mechanism\n",
        "#             word_attention_weights = F.softmax(self.word_attention(evidences_encoded), dim=0)\n",
        "#             evidence_representation = torch.sum(word_attention_weights * evidences_encoded, dim=0)\n",
        "#             evidence_representations.append(evidence_representation)\n",
        "\n",
        "#         evidence_representations = torch.stack(evidence_representations)\n",
        "\n",
        "#         # Sentence-level attention mechanism\n",
        "#         sentence_attention_weights = F.softmax(self.sentence_attention(evidence_representations), dim=0)\n",
        "#         sentence_representation = torch.sum(sentence_attention_weights * evidence_representations, dim=0)\n",
        "\n",
        "#         combined_representation = sentence_representation + claims_encoded\n",
        "#         logits = self.fc(combined_representation)\n",
        "#         return F.log_softmax(logits, dim=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4kE815v_zjIB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_refutes = (train_claim_labels == 0).sum()\n",
        "num_supports = (train_claim_labels == 1).sum()\n",
        "pos_weight = num_refutes / (num_supports)\n",
        "\n",
        "pos_weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xosl2oUsbkQP",
        "outputId": "029fe351-d89e-4b83-95ec-b46cc56f18ad"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3834296724470135"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def train_model(model, train_loader, dev_loader, criterion, optimizer, device, num_epochs=10, grad_clip=1.0, threshold=0.82):\n",
        "    model = model.to(device)  # Ensure the model is on the right device\n",
        "    criterion = criterion.to(device)  # Also move the criterion to the GPU if available\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for claims, evidences, labels in train_loader:\n",
        "            claims = claims.to(device)\n",
        "            evidences = evidences.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(claims, evidences).squeeze()\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs} - Training Loss: {avg_train_loss}')\n",
        "\n",
        "        # Evaluate on the development set\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for claims, evidences, labels in dev_loader:\n",
        "                claims = claims.to(device)\n",
        "                evidences = evidences.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                logits = model(claims, evidences).squeeze()\n",
        "                val_loss = criterion(logits, labels)\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "                probs = torch.sigmoid(logits)  # Convert logits to probabilities\n",
        "                preds = torch.where(probs > threshold, 1, 0).cpu().numpy()  # Convert probabilities to 0 and 1\n",
        "\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(dev_loader)\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        f1 = classification_report(all_labels, all_preds, target_names=['REFUTES', 'SUPPORTS'], zero_division=0, output_dict=True)['macro avg']['f1-score']\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs} - Validation Loss: {avg_val_loss}')\n",
        "        print(f'Accuracy: {accuracy}')\n",
        "        print(f'F1 Score: {f1}')\n",
        "        print(classification_report(all_labels, all_preds, target_names=['REFUTES', 'SUPPORTS'], zero_division=0))\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "# Training the model\n",
        "model = ClaimVerificationTransformer(num_layers=3, emb_size=512, nhead=8, vocab_size=len(vocab), dim_feedforward=512, dropout=0.7, pad_idx=vocab[\"<pad>\"])\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight).to(device))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Assuming `device` is defined (e.g., `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`)\n",
        "train_model(model, train_loader, dev_loader, criterion, optimizer, device=device, num_epochs=15)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-oLN4jojGch",
        "outputId": "4fe3fa9e-a6e8-4f3b-a345-14764271d423"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 - Training Loss: 0.3870027944841216\n",
            "Epoch 1/15 - Validation Loss: 0.43992309272289276\n",
            "Accuracy: 0.25\n",
            "F1 Score: 0.2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.25      1.00      0.40        57\n",
            "    SUPPORTS       0.00      0.00      0.00       171\n",
            "\n",
            "    accuracy                           0.25       228\n",
            "   macro avg       0.12      0.50      0.20       228\n",
            "weighted avg       0.06      0.25      0.10       228\n",
            "\n",
            "Epoch 2/15 - Training Loss: 0.3769318875485817\n",
            "Epoch 2/15 - Validation Loss: 0.3574479669332504\n",
            "Accuracy: 0.25\n",
            "F1 Score: 0.2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.25      1.00      0.40        57\n",
            "    SUPPORTS       0.00      0.00      0.00       171\n",
            "\n",
            "    accuracy                           0.25       228\n",
            "   macro avg       0.12      0.50      0.20       228\n",
            "weighted avg       0.06      0.25      0.10       228\n",
            "\n",
            "Epoch 3/15 - Training Loss: 0.37365244816889803\n",
            "Epoch 3/15 - Validation Loss: 0.35532934963703156\n",
            "Accuracy: 0.2543859649122807\n",
            "F1 Score: 0.2065181788404848\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.25      1.00      0.40        57\n",
            "    SUPPORTS       1.00      0.01      0.01       171\n",
            "\n",
            "    accuracy                           0.25       228\n",
            "   macro avg       0.63      0.50      0.21       228\n",
            "weighted avg       0.81      0.25      0.11       228\n",
            "\n",
            "Epoch 4/15 - Training Loss: 0.3687721405145341\n",
            "Epoch 4/15 - Validation Loss: 0.35778120160102844\n",
            "Accuracy: 0.3684210526315789\n",
            "F1 Score: 0.36134453781512604\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.28      0.95      0.43        57\n",
            "    SUPPORTS       0.91      0.18      0.29       171\n",
            "\n",
            "    accuracy                           0.37       228\n",
            "   macro avg       0.59      0.56      0.36       228\n",
            "weighted avg       0.75      0.37      0.33       228\n",
            "\n",
            "Epoch 5/15 - Training Loss: 0.3442016825475524\n",
            "Epoch 5/15 - Validation Loss: 0.3829682767391205\n",
            "Accuracy: 0.39035087719298245\n",
            "F1 Score: 0.38694264435632075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.28      0.93      0.43        57\n",
            "    SUPPORTS       0.90      0.21      0.34       171\n",
            "\n",
            "    accuracy                           0.39       228\n",
            "   macro avg       0.59      0.57      0.39       228\n",
            "weighted avg       0.75      0.39      0.36       228\n",
            "\n",
            "Epoch 6/15 - Training Loss: 0.34442941857650217\n",
            "Epoch 6/15 - Validation Loss: 0.49348606169223785\n",
            "Accuracy: 0.36403508771929827\n",
            "F1 Score: 0.35629587803500845\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.28      0.95      0.43        57\n",
            "    SUPPORTS       0.91      0.17      0.29       171\n",
            "\n",
            "    accuracy                           0.36       228\n",
            "   macro avg       0.59      0.56      0.36       228\n",
            "weighted avg       0.75      0.36      0.32       228\n",
            "\n",
            "Epoch 7/15 - Training Loss: 0.34677749239237965\n",
            "Epoch 7/15 - Validation Loss: 0.44457852840423584\n",
            "Accuracy: 0.42543859649122806\n",
            "F1 Score: 0.42542754362002966\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.28      0.84      0.42        57\n",
            "    SUPPORTS       0.84      0.29      0.43       171\n",
            "\n",
            "    accuracy                           0.43       228\n",
            "   macro avg       0.56      0.56      0.43       228\n",
            "weighted avg       0.70      0.43      0.43       228\n",
            "\n",
            "Epoch 8/15 - Training Loss: 0.33491530697957606\n",
            "Epoch 8/15 - Validation Loss: 0.48868685960769653\n",
            "Accuracy: 0.6140350877192983\n",
            "F1 Score: 0.5873303167420816\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.36      0.72      0.48        57\n",
            "    SUPPORTS       0.86      0.58      0.69       171\n",
            "\n",
            "    accuracy                           0.61       228\n",
            "   macro avg       0.61      0.65      0.59       228\n",
            "weighted avg       0.74      0.61      0.64       228\n",
            "\n",
            "Epoch 9/15 - Training Loss: 0.33668348852512053\n",
            "Epoch 9/15 - Validation Loss: 0.4989929497241974\n",
            "Accuracy: 0.6052631578947368\n",
            "F1 Score: 0.5715001670564651\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.35      0.65      0.45        57\n",
            "    SUPPORTS       0.83      0.59      0.69       171\n",
            "\n",
            "    accuracy                           0.61       228\n",
            "   macro avg       0.59      0.62      0.57       228\n",
            "weighted avg       0.71      0.61      0.63       228\n",
            "\n",
            "Epoch 10/15 - Training Loss: 0.319499978185755\n",
            "Epoch 10/15 - Validation Loss: 0.48483574390411377\n",
            "Accuracy: 0.5526315789473685\n",
            "F1 Score: 0.533648832918906\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.32      0.70      0.44        57\n",
            "    SUPPORTS       0.83      0.50      0.63       171\n",
            "\n",
            "    accuracy                           0.55       228\n",
            "   macro avg       0.58      0.60      0.53       228\n",
            "weighted avg       0.71      0.55      0.58       228\n",
            "\n",
            "Epoch 11/15 - Training Loss: 0.3243222094215123\n",
            "Epoch 11/15 - Validation Loss: 0.4811682552099228\n",
            "Accuracy: 0.5263157894736842\n",
            "F1 Score: 0.5179704016913318\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.32      0.79      0.45        57\n",
            "    SUPPORTS       0.86      0.44      0.58       171\n",
            "\n",
            "    accuracy                           0.53       228\n",
            "   macro avg       0.59      0.61      0.52       228\n",
            "weighted avg       0.73      0.53      0.55       228\n",
            "\n",
            "Epoch 12/15 - Training Loss: 0.31864259200813494\n",
            "Epoch 12/15 - Validation Loss: 0.479731023311615\n",
            "Accuracy: 0.4649122807017544\n",
            "F1 Score: 0.4634259259259259\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.30      0.82      0.44        57\n",
            "    SUPPORTS       0.86      0.35      0.49       171\n",
            "\n",
            "    accuracy                           0.46       228\n",
            "   macro avg       0.58      0.58      0.46       228\n",
            "weighted avg       0.72      0.46      0.48       228\n",
            "\n",
            "Epoch 13/15 - Training Loss: 0.3164576558412704\n",
            "Epoch 13/15 - Validation Loss: 0.4807671159505844\n",
            "Accuracy: 0.49122807017543857\n",
            "F1 Score: 0.48728287841191065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.30      0.81      0.44        57\n",
            "    SUPPORTS       0.86      0.39      0.53       171\n",
            "\n",
            "    accuracy                           0.49       228\n",
            "   macro avg       0.58      0.60      0.49       228\n",
            "weighted avg       0.72      0.49      0.51       228\n",
            "\n",
            "Epoch 14/15 - Training Loss: 0.3152137322235951\n",
            "Epoch 14/15 - Validation Loss: 0.4885612428188324\n",
            "Accuracy: 0.47368421052631576\n",
            "F1 Score: 0.4710794927312094\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.30      0.81      0.43        57\n",
            "    SUPPORTS       0.85      0.36      0.51       171\n",
            "\n",
            "    accuracy                           0.47       228\n",
            "   macro avg       0.57      0.58      0.47       228\n",
            "weighted avg       0.71      0.47      0.49       228\n",
            "\n",
            "Epoch 15/15 - Training Loss: 0.31497181002545144\n",
            "Epoch 15/15 - Validation Loss: 0.49062900245189667\n",
            "Accuracy: 0.5394736842105263\n",
            "F1 Score: 0.5270176825051862\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     REFUTES       0.32      0.75      0.45        57\n",
            "    SUPPORTS       0.85      0.47      0.60       171\n",
            "\n",
            "    accuracy                           0.54       228\n",
            "   macro avg       0.59      0.61      0.53       228\n",
            "weighted avg       0.72      0.54      0.57       228\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "\n",
        "def evaluate_model(model, claims, evidence_idxs, evidences, claim_labels, vocab, pad_idx, device, support_threshold=0.5, refute_threshold=0.005):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_evidence_predictions = []\n",
        "    all_evidence_probs = []\n",
        "\n",
        "    for claim_tokens, evidence_idx_list, true_label in zip(claims, evidence_idxs, claim_labels):\n",
        "        # Numericalize the claim tokens\n",
        "        claim_indices = text_to_indices(claim_tokens, vocab)\n",
        "        claim_indices = [vocab[\"<sos>\"]] + claim_indices + [vocab[\"<eos>\"]]\n",
        "        claim_tensor = torch.tensor(claim_indices, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "        evidence_tensors = []\n",
        "        for idx in evidence_idx_list:\n",
        "            evidence_tokens = evidences[idx]\n",
        "            evidence_indices = text_to_indices(evidence_tokens, vocab)\n",
        "            evidence_indices = [vocab[\"<sos>\"]] + evidence_indices + [vocab[\"<eos>\"]]\n",
        "            evidence_tensor = torch.tensor(evidence_indices, dtype=torch.long).to(device)\n",
        "            evidence_tensors.append(evidence_tensor)\n",
        "\n",
        "        # Pad evidence tensors to the same length\n",
        "        evidence_tensors_padded = pad_sequence(evidence_tensors, batch_first=True, padding_value=pad_idx).to(device)\n",
        "\n",
        "        evidence_predictions = []\n",
        "        evidence_probs = []\n",
        "        with torch.no_grad():\n",
        "            for evidence_tensor in evidence_tensors_padded:\n",
        "                evidence_tensor = evidence_tensor.unsqueeze(0)  # Add batch dimension\n",
        "                logits = model(claim_tensor, evidence_tensor).squeeze()\n",
        "                prob = torch.sigmoid(logits).item()\n",
        "                evidence_probs.append(prob)\n",
        "                if prob > support_threshold:\n",
        "                    evidence_predictions.append(1)  # SUPPORTS\n",
        "                elif prob < refute_threshold:\n",
        "                    evidence_predictions.append(0)  # REFUTES\n",
        "                else:\n",
        "                    evidence_predictions.append(2)  # NOT ENOUGH INFO\n",
        "\n",
        "        aggregated_prediction = aggregate_predictions(evidence_predictions)\n",
        "        all_preds.append(aggregated_prediction)\n",
        "        all_labels.append(true_label)\n",
        "        all_evidence_predictions.append(evidence_predictions)\n",
        "        all_evidence_probs.append(evidence_probs)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    report = classification_report(all_labels, all_preds, target_names=['REFUTES', 'SUPPORTS', 'NOT ENOUGH INFO', 'DISPUTED'], zero_division=0)\n",
        "    return accuracy, report, all_preds, all_labels, all_evidence_predictions,all_evidence_probs\n",
        "\n",
        "# def aggregate_predictions(evidence_predictions):\n",
        "#     counter = Counter(evidence_predictions)\n",
        "#     num_supports = counter[1]\n",
        "#     num_refutes = counter[0]\n",
        "#     num_not_enough_info = counter[2]\n",
        "\n",
        "#     # Determine the majority class with a focus on conflict handling\n",
        "#     if num_supports > 0 and num_refutes > 0:\n",
        "#         if num_supports > num_refutes and num_supports > num_not_enough_info:\n",
        "#             return 1  # SUPPORTS\n",
        "#         elif num_refutes > num_supports and num_refutes > num_not_enough_info:\n",
        "#             return 0  # REFUTES\n",
        "#         else:\n",
        "#             return 3  # DISPUTED if the majority is not clear\n",
        "#     elif num_supports > num_refutes:\n",
        "#         return 1  # SUPPORTS\n",
        "#     elif num_refutes > num_supports:\n",
        "#         return 0  # REFUTES\n",
        "#     else:\n",
        "#         return 2  # NOT ENOUGH INFO\n",
        "def aggregate_predictions(evidence_predictions):\n",
        "    counter = Counter(evidence_predictions)\n",
        "    num_supports = counter[1]\n",
        "    num_refutes = counter[0]\n",
        "\n",
        "    # Handle conflicts: if both SUPPORTS and REFUTES are present\n",
        "    if num_supports > 0 and num_refutes > 0:\n",
        "        return 3  # DISPUTED if there are both SUPPORTS and REFUTES\n",
        "\n",
        "    # Determine the class if all predictions are either SUPPORTS or REFUTES (with or without NOT ENOUGH INFO)\n",
        "    if num_supports > 0 and num_refutes == 0:\n",
        "        return 1  # SUPPORTS\n",
        "\n",
        "    if num_refutes > 0 and num_supports == 0:\n",
        "        return 0  # REFUTES\n",
        "\n",
        "    # Default to NOT ENOUGH INFO if there are no SUPPORTS or REFUTES\n",
        "    return 2\n",
        "\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "accuracy, report, all_preds, all_labels, all_evidence_predictions,_ = evaluate_model(model, train_claims_text_processed, train_evidence_idxs, evidence_text_processed, train_claim_labels, vocab, vocab[\"<pad>\"], device)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV4K6bawgyGi",
        "outputId": "5b0f7ea0-2972-41b3-966a-76a6fc92694b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4584690553745928\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        REFUTES       0.47      0.42      0.44       199\n",
            "       SUPPORTS       0.52      0.74      0.61       519\n",
            "NOT ENOUGH INFO       0.31      0.24      0.27       386\n",
            "       DISPUTED       0.00      0.00      0.00       124\n",
            "\n",
            "       accuracy                           0.46      1228\n",
            "      macro avg       0.32      0.35      0.33      1228\n",
            "   weighted avg       0.39      0.46      0.42      1228\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_evidence_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-FG93HSi4Ud",
        "outputId": "a11eb8b3-ef18-4744-bc2b-fee0fda99851"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 2, 2],\n",
              " [0, 0],\n",
              " [1, 1],\n",
              " [2, 2, 2, 2, 0],\n",
              " [0, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1],\n",
              " [1],\n",
              " [0, 0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [1, 2, 1, 1],\n",
              " [1, 1],\n",
              " [2, 2],\n",
              " [2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2, 2],\n",
              " [2, 2],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2],\n",
              " [2],\n",
              " [2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [0, 0],\n",
              " [1],\n",
              " [2, 2],\n",
              " [2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [0, 0, 0],\n",
              " [1, 1, 1, 2, 2],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0],\n",
              " [2, 2],\n",
              " [1],\n",
              " [1, 1, 1, 1],\n",
              " [0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1],\n",
              " [0],\n",
              " [1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [2, 2, 2, 2],\n",
              " [2, 2],\n",
              " [1, 1],\n",
              " [0, 2, 2, 2],\n",
              " [2, 0, 2, 2, 2],\n",
              " [1, 1, 1, 1],\n",
              " [2, 1, 1],\n",
              " [1, 1],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 1, 1],\n",
              " [2],\n",
              " [2, 1, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1],\n",
              " [2, 2],\n",
              " [2, 2, 1],\n",
              " [2, 2],\n",
              " [2, 2, 2, 2],\n",
              " [0, 2],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2],\n",
              " [2],\n",
              " [1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 1, 1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 2],\n",
              " [0, 0, 0],\n",
              " [1, 1, 1],\n",
              " [2],\n",
              " [0, 0, 0, 0, 0],\n",
              " [2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2],\n",
              " [0, 0],\n",
              " [2],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1, 1, 1],\n",
              " [2, 2, 1, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0, 0],\n",
              " [0],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1],\n",
              " [1, 1, 1, 1],\n",
              " [2, 2, 2],\n",
              " [1, 1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0],\n",
              " [0, 0, 0],\n",
              " [1, 1, 1, 2, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0],\n",
              " [1, 2, 2, 1, 2],\n",
              " [1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [0, 0],\n",
              " [1],\n",
              " [2, 1, 2, 1],\n",
              " [1, 2],\n",
              " [1, 1, 1],\n",
              " [0, 0],\n",
              " [0, 0, 0, 0, 0],\n",
              " [2, 2, 2, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2],\n",
              " [2, 2],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2],\n",
              " [2, 1, 1, 1, 1],\n",
              " [1],\n",
              " [2, 2, 2],\n",
              " [2, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1],\n",
              " [0, 0, 0],\n",
              " [2, 2, 2, 2, 2],\n",
              " [0],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 1],\n",
              " [1],\n",
              " [2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [0],\n",
              " [1, 1, 2, 2, 2],\n",
              " [2, 2, 2, 1, 1],\n",
              " [2, 2],\n",
              " [2, 0, 2, 0, 0],\n",
              " [1, 1],\n",
              " [2, 1, 1, 2, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1],\n",
              " [1, 1, 2],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [2],\n",
              " [2, 2, 1, 1, 2],\n",
              " [2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 2, 2, 2, 1],\n",
              " [2, 2, 0, 2, 2],\n",
              " [2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 2, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0],\n",
              " [1, 2],\n",
              " [0, 0, 0],\n",
              " [2, 2, 1, 1, 1],\n",
              " [1, 2, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [2, 2],\n",
              " [2],\n",
              " [2, 2],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [1],\n",
              " [2, 2],\n",
              " [1, 1],\n",
              " [2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2],\n",
              " [2],\n",
              " [1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1],\n",
              " [1],\n",
              " [2, 2],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1],\n",
              " [2, 1, 2, 2],\n",
              " [1, 1, 1],\n",
              " [1],\n",
              " [2, 2, 1, 1, 1],\n",
              " [1, 1, 2, 2, 1],\n",
              " [2],\n",
              " [1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1],\n",
              " [0],\n",
              " [1, 2, 1],\n",
              " [2, 0, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [0],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [0, 0],\n",
              " [0, 0, 0],\n",
              " [1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1],\n",
              " [1],\n",
              " [2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [2, 2, 2, 1, 2],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2],\n",
              " [2, 0, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [0],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [2, 2],\n",
              " [0, 0],\n",
              " [2, 0, 0],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [2],\n",
              " [0, 0],\n",
              " [0],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1],\n",
              " [1],\n",
              " [2, 2, 2],\n",
              " [2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2],\n",
              " [1],\n",
              " [2, 2],\n",
              " [1, 2, 2, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [2],\n",
              " [1, 1, 1, 1],\n",
              " [2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 2],\n",
              " [2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [0, 0, 0, 0, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2],\n",
              " [2, 1, 2, 1, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2],\n",
              " [0, 0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 0, 0, 2, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 1],\n",
              " [1, 1],\n",
              " [1, 1, 2, 1, 2],\n",
              " [1, 1, 1],\n",
              " [2],\n",
              " [2, 2, 2, 1],\n",
              " [0, 0],\n",
              " [1, 1],\n",
              " [1, 1, 1],\n",
              " [2, 2, 1, 1, 1],\n",
              " [1, 1],\n",
              " [0, 0, 0, 0],\n",
              " [0, 0],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1],\n",
              " [1, 1],\n",
              " [0, 0, 0],\n",
              " [1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [2],\n",
              " [2],\n",
              " [2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 2, 2, 1, 1],\n",
              " [2, 1, 1, 1],\n",
              " [0, 0, 0, 0],\n",
              " [1, 1, 1, 1],\n",
              " [2, 2, 2, 2],\n",
              " [2, 2],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 2, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1],\n",
              " [2],\n",
              " [1, 1, 1, 1],\n",
              " [0, 0, 2, 0, 0],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [0, 0, 0, 0, 0],\n",
              " [0, 0],\n",
              " [1, 1],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [0, 0, 0],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 1, 2, 2, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [2, 2, 0],\n",
              " [1, 2, 1, 1, 1],\n",
              " [1],\n",
              " [1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 1, 1],\n",
              " [1, 2, 1, 1, 1],\n",
              " [1, 2, 1],\n",
              " [1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 2],\n",
              " [1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [1],\n",
              " [0],\n",
              " [2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1],\n",
              " [1],\n",
              " [1],\n",
              " [2, 2, 2, 2, 0],\n",
              " [1, 1],\n",
              " [2],\n",
              " [2],\n",
              " [2, 2, 2, 2],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [2, 2, 1, 2, 1],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 1, 2, 1, 1],\n",
              " [1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1, 1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2],\n",
              " [0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 1, 2, 2],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1],\n",
              " [1, 1],\n",
              " [1],\n",
              " [1, 1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 1, 2],\n",
              " [0, 0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [2, 2, 0, 0],\n",
              " [2],\n",
              " [0, 0],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [2],\n",
              " [0, 0, 0, 0, 0],\n",
              " [2, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [1, 1, 1],\n",
              " [0, 0, 0],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 0],\n",
              " [1, 1],\n",
              " [1, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1],\n",
              " [2, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0],\n",
              " [2, 2, 2, 1, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [2, 2, 2],\n",
              " [0, 0, 0, 0],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1],\n",
              " [0, 0],\n",
              " [0, 0, 0, 0, 2],\n",
              " [1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1],\n",
              " [1],\n",
              " [2],\n",
              " [0],\n",
              " [1, 1, 1],\n",
              " [1, 1, 2, 1, 2],\n",
              " [2],\n",
              " [1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1, 1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1],\n",
              " [1, 1],\n",
              " [1, 1],\n",
              " [1],\n",
              " [2, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [0, 0, 0],\n",
              " [1, 1, 1, 1],\n",
              " [0, 0],\n",
              " [2],\n",
              " [1],\n",
              " [1, 2, 1, 2],\n",
              " [0, 0, 0, 0, 0],\n",
              " [2, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 2, 2, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1],\n",
              " [1, 2, 2, 1, 1],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1, 1, 1],\n",
              " [1],\n",
              " [2],\n",
              " [1, 1],\n",
              " [0, 0, 0],\n",
              " [2, 2, 2, 2, 1],\n",
              " [1, 1],\n",
              " [1],\n",
              " [2, 2, 0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0],\n",
              " [2, 2, 2, 2, 2],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1, 1],\n",
              " [2],\n",
              " [0, 0],\n",
              " [0, 0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1],\n",
              " [0, 2, 2, 2, 0],\n",
              " [0, 2, 2, 2, 2],\n",
              " [0],\n",
              " [0, 2, 2, 0, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0],\n",
              " [2, 2, 2],\n",
              " [0, 0, 0],\n",
              " [1],\n",
              " [1],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2],\n",
              " [1, 1],\n",
              " [0],\n",
              " [2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2],\n",
              " [2, 2],\n",
              " [2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1],\n",
              " [2, 2, 2, 2, 1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2],\n",
              " [2, 2],\n",
              " [2],\n",
              " [2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [0, 0, 0, 0],\n",
              " [1, 1, 1],\n",
              " [1, 1, 2],\n",
              " [2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1],\n",
              " [1, 2, 1, 2, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 2, 2, 2, 2],\n",
              " [1, 2, 1, 2, 1],\n",
              " [2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 0, 0, 2, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [2, 1, 1, 2, 2],\n",
              " [1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 2, 2, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0],\n",
              " [1, 1],\n",
              " [1, 1],\n",
              " [0, 0, 0],\n",
              " [0, 0, 0, 0, 0],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 1, 1, 2, 2],\n",
              " [1, 2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [0],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 1],\n",
              " [1],\n",
              " [1, 1, 2, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [2, 2, 2, 2, 1],\n",
              " [1],\n",
              " [2, 0, 0, 0, 0],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2],\n",
              " [1],\n",
              " [1, 1, 1],\n",
              " [2, 2, 2],\n",
              " [2],\n",
              " [1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [1],\n",
              " [1],\n",
              " [1, 1, 2, 1, 1],\n",
              " [2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1],\n",
              " [2, 2, 2, 2, 1],\n",
              " [1, 1, 1, 1, 2],\n",
              " [1, 1, 1],\n",
              " [2, 1, 1],\n",
              " [2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [2, 0, 0, 0, 0],\n",
              " [0],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0],\n",
              " [2, 2],\n",
              " [1, 1],\n",
              " [2, 2, 2],\n",
              " [2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [0, 0, 0],\n",
              " [1, 1, 1],\n",
              " [2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1],\n",
              " [2, 2, 2, 2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2, 2],\n",
              " [0, 0],\n",
              " [2],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 0],\n",
              " [1, 1],\n",
              " [2, 2],\n",
              " [2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1],\n",
              " [2, 2, 2],\n",
              " [2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [2, 1, 1],\n",
              " [2, 1, 1],\n",
              " [2, 2, 0, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [2, 2, 1, 2, 1],\n",
              " [1, 1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 2, 2, 1, 1],\n",
              " [2],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [0, 0, 2, 0],\n",
              " [2, 0, 2, 0],\n",
              " [2],\n",
              " [1, 1, 1],\n",
              " [1, 1],\n",
              " [1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0, 2, 2],\n",
              " [0],\n",
              " [1],\n",
              " [1],\n",
              " [1, 1, 1],\n",
              " [2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 1, 2],\n",
              " [0, 0, 0, 0, 0],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [0, 0, 0, 0],\n",
              " [2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [2, 2],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [1, 1],\n",
              " [2],\n",
              " [2],\n",
              " [2, 2, 2, 2],\n",
              " [1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0],\n",
              " [2, 2, 2, 2, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 2, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2],\n",
              " [2, 2],\n",
              " [1, 1, 1],\n",
              " [1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2, 2, 2, 2],\n",
              " [1, 1],\n",
              " [1, 1, 2],\n",
              " [0, 0, 0, 0],\n",
              " [0, 0, 0, 2, 0],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1],\n",
              " [2, 1],\n",
              " [2, 2, 1, 1, 2],\n",
              " [1, 1, 1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [0, 0, 0],\n",
              " [1],\n",
              " [2, 2, 2],\n",
              " [1, 1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [2, 2],\n",
              " [1],\n",
              " [1, 1, 1, 1, 1],\n",
              " [1, 1, 1],\n",
              " [1, 1],\n",
              " [2],\n",
              " [2, 2, 2, 2],\n",
              " [1],\n",
              " [2],\n",
              " [1, 2, 2],\n",
              " [1, 1, 1, 1],\n",
              " [0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1],\n",
              " [1, 2],\n",
              " [1, 1, 1],\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def train_model(model, train_loader, dev_loader, criterion, optimizer, device, num_epochs=10, grad_clip=1.0):\n",
        "    model = model.to(device)  # Ensure the model is on the right device\n",
        "    criterion = criterion.to(device)  # Also move the criterion to the GPU if available\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for claims, evidences, labels in train_loader:\n",
        "            claims = claims.to(device)\n",
        "            evidences = [e.to(device) for e in evidences]  # Move each batch of evidences to GPU\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(claims, evidences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs} - Training Loss: {avg_train_loss}')\n",
        "\n",
        "        # Evaluate on the development set\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for claims, evidences, labels in dev_loader:\n",
        "                claims = claims.to(device)\n",
        "                evidences = [e.to(device) for e in evidences]\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(claims, evidences)\n",
        "                val_loss = criterion(outputs, labels)\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                all_preds.extend(predicted.cpu().numpy())  # Move predictions back to CPU for scoring\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(dev_loader)\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs} - Validation Loss: {avg_val_loss}')\n",
        "        print(classification_report(all_labels, all_preds, zero_division=0))\n",
        "\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_label_counts = train_claim_labels.value_counts()\n",
        "# Convert label counts to a Tensor\n",
        "train_label_counts_tensor = torch.tensor(train_label_counts.sort_index().values, dtype=torch.float)\n",
        "\n",
        "# Calculate weights: inversely proportional to the class frequencies\n",
        "class_weights = 1.0 / train_label_counts_tensor\n",
        "\n",
        "# Normalize weights so that the smallest weight is 1.0\n",
        "class_weights = class_weights / class_weights.min()\n",
        "\n",
        "# Move weights to the correct device (GPU or CPU)\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "# Model instantiation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model = ClaimEvidenceModel(vocab_size=len(vocab), embedding_dim=100, hidden_dim=128, output_dim=len(label_map), pad_idx=vocab['<pad>'])\n",
        "model = ClaimVerificationTransformer(\n",
        "    num_layers=2,\n",
        "    emb_size=360,\n",
        "    nhead=4,\n",
        "    vocab_size=len(vocab),\n",
        "    dim_feedforward=512,\n",
        "    num_classes=len(label_map),\n",
        "    dropout=0.5,\n",
        "    maxlen=5000,\n",
        "    pad_idx=vocab['<pad>']\n",
        ")\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "# criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=vocab['<pad>'])\n",
        "criterion = nn.NLLLoss(weight=class_weights)\n",
        "\n",
        "# Load data\n",
        "train_dataset = ClaimEvidenceDataset(train_claims_numerical, train_claim_labels, train_evidence_idxs, evidence_numerical)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=custom_collate_fn)\n",
        "dev_dataset = ClaimEvidenceDataset(dev_claims_numerical, dev_claim_labels, dev_k_indices, evidence_numerical)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "# Training and evaluating the model\n",
        "train_model(model, train_loader, dev_loader, criterion, optimizer, device=device, num_epochs=20)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTHBV9qOIKTG",
        "outputId": "f6b0955b-e1d4-4f11-bc41-4e96b21e0bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Training Loss: 2.172918736934662\n",
            "Epoch 1/20 - Validation Loss: 1.4890036582946777\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.18      1.00      0.30        27\n",
            "           2       0.00      0.00      0.00        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.18       154\n",
            "   macro avg       0.04      0.25      0.08       154\n",
            "weighted avg       0.03      0.18      0.05       154\n",
            "\n",
            "Epoch 2/20 - Training Loss: 1.4130306601524354\n",
            "Epoch 2/20 - Validation Loss: 1.6923393607139587\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.18      0.96      0.30        27\n",
            "           2       0.44      0.06      0.10        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.19       154\n",
            "   macro avg       0.16      0.26      0.10       154\n",
            "weighted avg       0.23      0.19      0.10       154\n",
            "\n",
            "Epoch 3/20 - Training Loss: 1.2916382670402526\n",
            "Epoch 3/20 - Validation Loss: 1.7335440516471863\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.23      0.52      0.32        27\n",
            "           2       0.43      0.59      0.50        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.35       154\n",
            "   macro avg       0.16      0.28      0.20       154\n",
            "weighted avg       0.23      0.35      0.28       154\n",
            "\n",
            "Epoch 4/20 - Training Loss: 1.2000791072845458\n",
            "Epoch 4/20 - Validation Loss: 1.8290648460388184\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.25      0.41      0.31        27\n",
            "           2       0.43      0.69      0.53        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.38       154\n",
            "   macro avg       0.17      0.27      0.21       154\n",
            "weighted avg       0.23      0.38      0.29       154\n",
            "\n",
            "Epoch 5/20 - Training Loss: 1.1783608794212341\n",
            "Epoch 5/20 - Validation Loss: 1.6054121255874634\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.19      0.74      0.30        27\n",
            "           2       0.49      0.29      0.37        68\n",
            "           3       0.50      0.05      0.09        41\n",
            "\n",
            "    accuracy                           0.27       154\n",
            "   macro avg       0.29      0.27      0.19       154\n",
            "weighted avg       0.38      0.27      0.24       154\n",
            "\n",
            "Epoch 6/20 - Training Loss: 1.1325671553611756\n",
            "Epoch 6/20 - Validation Loss: 1.5510473847389221\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.12      0.22      0.16        18\n",
            "           1       0.23      0.56      0.33        27\n",
            "           2       0.44      0.37      0.40        68\n",
            "           3       1.00      0.02      0.05        41\n",
            "\n",
            "    accuracy                           0.29       154\n",
            "   macro avg       0.45      0.29      0.23       154\n",
            "weighted avg       0.52      0.29      0.27       154\n",
            "\n",
            "Epoch 7/20 - Training Loss: 1.093510490655899\n",
            "Epoch 7/20 - Validation Loss: 1.9044117331504822\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.18      0.67      0.28        27\n",
            "           2       0.45      0.35      0.40        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.27       154\n",
            "   macro avg       0.16      0.25      0.17       154\n",
            "weighted avg       0.23      0.27      0.22       154\n",
            "\n",
            "Epoch 8/20 - Training Loss: 1.0418055057525635\n",
            "Epoch 8/20 - Validation Loss: 1.7834139466285706\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.21      0.41      0.28        27\n",
            "           2       0.44      0.66      0.53        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.36       154\n",
            "   macro avg       0.16      0.27      0.20       154\n",
            "weighted avg       0.23      0.36      0.28       154\n",
            "\n",
            "Epoch 9/20 - Training Loss: 1.0330120742321014\n",
            "Epoch 9/20 - Validation Loss: 1.8519176244735718\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.59      0.30        27\n",
            "           2       0.47      0.50      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.32       154\n",
            "   macro avg       0.17      0.27      0.19       154\n",
            "weighted avg       0.24      0.32      0.26       154\n",
            "\n",
            "Epoch 10/20 - Training Loss: 0.9993395924568176\n",
            "Epoch 10/20 - Validation Loss: 1.8131577968597412\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.52      0.29        27\n",
            "           2       0.45      0.56      0.50        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.34       154\n",
            "   macro avg       0.16      0.27      0.20       154\n",
            "weighted avg       0.23      0.34      0.27       154\n",
            "\n",
            "Epoch 11/20 - Training Loss: 1.0009517550468445\n",
            "Epoch 11/20 - Validation Loss: 1.861646294593811\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.21      0.52      0.30        27\n",
            "           2       0.43      0.56      0.49        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.34       154\n",
            "   macro avg       0.16      0.27      0.20       154\n",
            "weighted avg       0.23      0.34      0.27       154\n",
            "\n",
            "Epoch 12/20 - Training Loss: 0.9930105745792389\n",
            "Epoch 12/20 - Validation Loss: 1.7857908606529236\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.21      0.52      0.30        27\n",
            "           2       0.43      0.56      0.49        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.34       154\n",
            "   macro avg       0.16      0.27      0.20       154\n",
            "weighted avg       0.23      0.34      0.27       154\n",
            "\n",
            "Epoch 13/20 - Training Loss: 0.9851340293884278\n",
            "Epoch 13/20 - Validation Loss: 1.8620266914367676\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.44      0.27        27\n",
            "           2       0.41      0.56      0.47        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.32       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.21      0.32      0.26       154\n",
            "\n",
            "Epoch 14/20 - Training Loss: 1.0021137833595275\n",
            "Epoch 14/20 - Validation Loss: 1.8330886363983154\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.44      0.28        27\n",
            "           2       0.41      0.57      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.33       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.22      0.33      0.26       154\n",
            "\n",
            "Epoch 15/20 - Training Loss: 0.9901392936706543\n",
            "Epoch 15/20 - Validation Loss: 1.8163567781448364\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.44      0.28        27\n",
            "           2       0.41      0.57      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.33       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.22      0.33      0.26       154\n",
            "\n",
            "Epoch 16/20 - Training Loss: 0.9810118854045868\n",
            "Epoch 16/20 - Validation Loss: 1.8217646479606628\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.44      0.28        27\n",
            "           2       0.41      0.57      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.33       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.22      0.33      0.26       154\n",
            "\n",
            "Epoch 17/20 - Training Loss: 0.969785088300705\n",
            "Epoch 17/20 - Validation Loss: 1.8446078896522522\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.21      0.48      0.29        27\n",
            "           2       0.42      0.56      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.33       154\n",
            "   macro avg       0.16      0.26      0.19       154\n",
            "weighted avg       0.22      0.33      0.26       154\n",
            "\n",
            "Epoch 18/20 - Training Loss: 0.9827152252197265\n",
            "Epoch 18/20 - Validation Loss: 1.8482361435890198\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.44      0.28        27\n",
            "           2       0.41      0.57      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.33       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.22      0.33      0.26       154\n",
            "\n",
            "Epoch 19/20 - Training Loss: 0.9499853372573852\n",
            "Epoch 19/20 - Validation Loss: 1.8474194407463074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.19      0.44      0.26        27\n",
            "           2       0.42      0.56      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.32       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.22      0.32      0.26       154\n",
            "\n",
            "Epoch 20/20 - Training Loss: 0.969256979227066\n",
            "Epoch 20/20 - Validation Loss: 1.8468242287635803\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.19      0.44      0.26        27\n",
            "           2       0.42      0.56      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.32       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.22      0.32      0.26       154\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "dev_dataset = ClaimEvidenceDataset(dev_claims_numerical, dev_claim_labels, dev_evidence_idxs, evidence_numerical)\n",
        "\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for claims, evidences, labels in dev_loader:\n",
        "        claims = claims.to(device)\n",
        "        evidences = [e.to(device) for e in evidences]  # List comprehension to move each batch of evidences to GPU\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(claims, evidences)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)"
      ],
      "metadata": {
        "id": "sxQ7UcSRE2NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_idx_to_id_dict = {idx: id for id, idx in evidence_id_dict.items()}\n",
        "\n",
        "label_map_inverse = {v: k for k, v in label_map.items()}\n",
        "\n",
        "\n",
        "\n",
        "dev_label_predictions = [label_map_inverse[pred] for pred in all_preds]\n",
        "dev_converted_evidence_ids = [[evidence_idx_to_id_dict[idx] for idx in indices] for indices in dev_k_indices]\n",
        "\n",
        "results = {}\n",
        "for i, claim_id in enumerate(dev_claim_ids):\n",
        "    results[claim_id] = {\n",
        "        \"claim_text\": dev_claims_text[i],\n",
        "        \"claim_label\": dev_label_predictions[i],\n",
        "        \"evidences\": dev_converted_evidence_ids[i]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "8VcKIiwIFc6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/nlp/data/dev_predictions.json', 'w') as file:\n",
        "    json.dump(results, file)"
      ],
      "metadata": {
        "id": "euOKIx5myHsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ClaimEvidenceDataset(test_claims_numerical, None, test_k_indices, evidence_numerical)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for claims, evidences,_ in test_loader:  # Note that labels are not loaded\n",
        "        claims = claims.to(device)\n",
        "        evidences = [e.to(device) for e in evidences]  # Ensure evidences are moved to GPU\n",
        "        outputs = model(claims, evidences)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "# `all_preds` now contains the predicted labels for your test dataset\n"
      ],
      "metadata": {
        "id": "QADtxafjlrKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse mapping for label_map\n",
        "label_map_inverse = {v: k for k, v in label_map.items()}\n",
        "# Convert numerical predictions to label strings\n",
        "label_predictions = [label_map_inverse[pred] for pred in all_preds]\n"
      ],
      "metadata": {
        "id": "ZYzq14Mvm1C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming evidence_id_dict maps from IDs to indices, create the inverse mapping\n",
        "evidence_idx_to_id_dict = {idx: id for id, idx in evidence_id_dict.items()}\n",
        "\n",
        "# Now convert indices to IDs (assuming you have a list of indices)\n",
        "# example_indices = [1, 2, 3]  # Replace with actual indices if needed\n",
        "# converted_ids = [evidence_idx_to_id_dict[idx] for idx in example_indices]\n",
        "converted_evidence_ids = [[evidence_idx_to_id_dict[idx] for idx in indices] for indices in test_k_indices]"
      ],
      "metadata": {
        "id": "RFti-ljAnlzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converted_evidence_ids[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pep3wub4nunX",
        "outputId": "44c27e59-6fbf-40dc-9d60-e09a6e317c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['evidence-55562',\n",
              " 'evidence-1032935',\n",
              " 'evidence-60163',\n",
              " 'evidence-225665',\n",
              " 'evidence-377026']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_claims_id\n",
        "test_claims_text\n",
        "label_predictions\n",
        "converted_evidence_ids\n"
      ],
      "metadata": {
        "id": "QQiprVWjn3sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json.dump(label_predictions, open(\"/content/drive/MyDrive/nlp/data/test_label_predictions.json\", \"w\"))"
      ],
      "metadata": {
        "id": "fs8MJfNmu0MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "for i, claim_id in enumerate(test_claims_id):\n",
        "    results[claim_id] = {\n",
        "        \"claim_text\": test_claims_text[i],\n",
        "        \"claim_label\": label_predictions[i],\n",
        "        \"evidences\": converted_evidence_ids[i]\n",
        "    }"
      ],
      "metadata": {
        "id": "pgmDJsdno24Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Convert to JSON string\n",
        "json_output = json.dumps(results, indent=4)\n",
        "print(json_output)\n",
        "\n",
        "# Save to a JSON file\n",
        "with open('/content/drive/MyDrive/nlp/data/test_predictions.json', 'w') as file:\n",
        "    json.dump(results, file)"
      ],
      "metadata": {
        "id": "FNiXAD2So6dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}