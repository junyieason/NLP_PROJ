{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPitXZzTsyTn",
        "outputId": "1b4c0a78-8e56-4534-b3e6-9ecdcec85d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VcWPFMqtJ4x"
      },
      "outputs": [],
      "source": [
        "import torchdata.datapipes as dp\n",
        "import torchtext.transforms as T\n",
        "import spacy\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "eng = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohyt_MA86_7e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "evidences = pd.read_json('/content/drive/MyDrive/nlp/data/evidence.json', orient='index')\n",
        "train_claims = pd.read_json('/content/drive/MyDrive/nlp/data/train-claims.json', orient='index')\n",
        "dev_claims = pd.read_json('/content/drive/MyDrive/nlp/data/dev-claims.json', orient='index')\n",
        "\n",
        "#update column names\n",
        "evidences.reset_index(inplace=True)\n",
        "evidences.columns = ['evidence_id', 'evidence_text']\n",
        "\n",
        "train_claims.reset_index(inplace=True)\n",
        "train_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "dev_claims.reset_index(inplace=True)\n",
        "dev_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "evidence_id = evidences['evidence_id']\n",
        "evidence_text = evidences['evidence_text']\n",
        "evidence_idx = evidences.index.tolist()\n",
        "\n",
        "evidence_id_dict = dict(zip(evidence_id, evidence_idx))\n",
        "\n",
        "train_claims_text = train_claims['claim_text']\n",
        "train_evidence_ids = train_claims['evidences']\n",
        "train_claim_labels = train_claims['claim_label']\n",
        "#map evidence_id to their corrosponding index for faster processing\n",
        "train_evidence_idxs = train_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])\n",
        "\n",
        "dev_claims_text = dev_claims['claim_text']\n",
        "dev_claim_labels = dev_claims['claim_label']\n",
        "dev_evidence_ids = dev_claims['evidences']\n",
        "dev_evidence_idxs = dev_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_claims = pd.read_json('/content/drive/MyDrive/nlp/data/test-claims-unlabelled.json', orient='index')\n",
        "test_claims.reset_index(inplace=True)\n",
        "test_claims.columns = ['claim_id', 'claim_text']\n",
        "test_claims_text = test_claims['claim_text']\n",
        "test_claims_id = test_claims['claim_id']"
      ],
      "metadata": {
        "id": "eEmCnO6TkYRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_claim_ids = dev_claims['claim_id']"
      ],
      "metadata": {
        "id": "YV2PK7ldyQao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "dev_evidence_indices = json.load(open(\"/content/drive/MyDrive/nlp/data/reranked_indices.json\", \"r\"))\n",
        "test_evidence_indices = json.load(open(\"/content/drive/MyDrive/nlp/data/test_reranked_indices.json\", \"r\"))"
      ],
      "metadata": {
        "id": "6eWAVRHbYL_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_k_indices = [sublist[:5] if len(sublist) >= 5 else sublist + [None] * (5 - len(sublist)) for sublist in dev_evidence_indices]\n",
        "test_k_indices = [sublist[:5] if len(sublist) >= 5 else sublist + [None] * (5 - len(sublist)) for sublist in test_evidence_indices]"
      ],
      "metadata": {
        "id": "J12VT62YYjhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_evidence_retrieval(predicted_indices_list, actual_indices_list, k=5):\n",
        "    assert len(predicted_indices_list) == len(actual_indices_list), \"Both inputs must have the same length.\"\n",
        "\n",
        "    total_recall = 0.0\n",
        "    total_precision = 0.0\n",
        "    total_fscore = 0.0\n",
        "    num_claims = len(predicted_indices_list)\n",
        "\n",
        "    for predicted_indices, actual_indices in zip(predicted_indices_list, actual_indices_list):\n",
        "        # Convert tensors in predicted_indices to integers if they are not already\n",
        "        predicted_indices = [index.item() if isinstance(index, torch.Tensor) else index for index in predicted_indices]\n",
        "\n",
        "        # Retrieve the top k predictions\n",
        "        top_k_predicted = set(predicted_indices[:k])\n",
        "        actual_indices_set = set(actual_indices)\n",
        "\n",
        "        # Calculate the number of correct predictions\n",
        "        correct_predictions = len(top_k_predicted.intersection(actual_indices_set))\n",
        "\n",
        "        # Calculate metrics\n",
        "        if correct_predictions > 0:\n",
        "            recall = float(correct_predictions) / len(actual_indices_set)\n",
        "            precision = float(correct_predictions) / k\n",
        "            if (precision + recall) != 0:\n",
        "                fscore = 2 * (precision * recall) / (precision + recall)\n",
        "            else:\n",
        "                fscore = 0.0\n",
        "        else:\n",
        "            recall = 0.0\n",
        "            precision = 0.0\n",
        "            fscore = 0.0\n",
        "\n",
        "        # Accumulate the metrics to calculate averages later\n",
        "        total_recall += recall\n",
        "        total_precision += precision\n",
        "        total_fscore += fscore\n",
        "\n",
        "    # Calculate average metrics\n",
        "    average_recall = total_recall / num_claims\n",
        "    average_precision = total_precision / num_claims\n",
        "    average_fscore = total_fscore / num_claims\n",
        "\n",
        "    return {\n",
        "        \"average_recall\": average_recall,\n",
        "        \"average_precision\": average_precision,\n",
        "        \"average_fscore\": average_fscore\n",
        "    }\n",
        "\n",
        "results = evaluate_evidence_retrieval(dev_evidence_indices, dev_evidence_idxs)"
      ],
      "metadata": {
        "id": "q0nI_J7_YwiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiqYU-a4uRJr",
        "outputId": "49946bba-fa7b-4577-a952-98e2b27060a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'average_recall': 0.131060606060606, 'average_precision': 0.07142857142857141, 'average_fscore': 0.08493609565038136}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9hCn0g-7Ae2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdee9e9c-d653-45b6-8c6e-018fdb2d3598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_data(text):\n",
        "    tokens = tt.tokenize(text.lower())\n",
        "    return tokens\n",
        "\n",
        "train_claims_text_processed = train_claims_text.apply(preprocess_data)\n",
        "dev_claims_text_precessed = dev_claims_text.apply(preprocess_data)\n",
        "evidence_text_processed = evidence_text.apply(preprocess_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_claims_text_processed = test_claims_text.apply(preprocess_data)"
      ],
      "metadata": {
        "id": "uEOtHTXWlHUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_claims_text_processed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRWZAXwYlK-7",
        "outputId": "ec7e9832-51a9-4040-8d30-3eb4d5bb57af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      [‘, this, study, goes, beyond, statistical, co...\n",
              "1      [a, recent, study, in, nature, geoscience, ,, ...\n",
              "2      [‘, arctic, ice, conditions, have, been, track...\n",
              "3      [“, the, global, reef, crisis, does, not, nece...\n",
              "4      [a, second, coat, of, paint, has, much, less, ...\n",
              "                             ...                        \n",
              "148    [the, cement, ,, iron, and, steel, ,, and, pet...\n",
              "149    [‘, we, could, be, decades, too, fast, ,, or, ...\n",
              "150    [the, alaskan, tundra, is, warming, so, quickl...\n",
              "151    [“, arctic, land, stores, about, twice, as, mu...\n",
              "152    [“, warm, weather, worsened, the, most, recent...\n",
              "Name: claim_text, Length: 153, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, OrderedDict\n",
        "def build_vocab(texts, min_freq=3):\n",
        "    # Count all the words\n",
        "    word_freq = Counter()\n",
        "    for text in texts:\n",
        "        word_freq.update(text)\n",
        "\n",
        "    # Start vocab from special tokens\n",
        "    vocab = OrderedDict({\n",
        "        \"<pad>\": 0,\n",
        "        \"<unk>\": 1,\n",
        "        \"<sos>\": 2,\n",
        "        \"<eos>\": 3\n",
        "    })\n",
        "    index = 4  # Start indexing from 4 because 0-3 are reserved for special tokens\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:  # Only include words that meet the frequency threshold\n",
        "            vocab[word] = index\n",
        "            index += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# Build vocabulary using only evidence texts and applying the frequency threshold\n",
        "vocab = build_vocab(evidence_text_processed, min_freq=3)"
      ],
      "metadata": {
        "id": "T_4DV847328f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FHipd3IplZsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numericalize(text, vocab):\n",
        "    # Convert text to numerical representation, adding start and end tokens\n",
        "    return [vocab[\"<sos>\"]] + [vocab.get(word, vocab[\"<unk>\"]) for word in text] + [vocab[\"<eos>\"]]\n",
        "\n",
        "train_claims_numerical = train_claims_text_processed.apply(lambda x: numericalize(x, vocab))\n",
        "dev_claims_numerical = dev_claims_text_precessed.apply(lambda x: numericalize(x, vocab))\n",
        "evidence_numerical = evidence_text_processed.apply(lambda x: numericalize(x, vocab))\n"
      ],
      "metadata": {
        "id": "6usrD5Nx59Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_claims_numerical = test_claims_text_processed.apply(lambda x: numericalize(x, vocab))"
      ],
      "metadata": {
        "id": "gslxstqulj6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\n",
        "    \"DISPUTED\": 0,\n",
        "    \"REFUTES\": 1,\n",
        "    \"SUPPORTS\": 2,\n",
        "    \"NOT_ENOUGH_INFO\": 3\n",
        "}\n",
        "train_claim_labels = train_claims['claim_label'].map(label_map)\n",
        "dev_claim_labels = dev_claims['claim_label'].map(label_map)"
      ],
      "metadata": {
        "id": "99CY84id6ZJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class ClaimEvidenceDataset(Dataset):\n",
        "    def __init__(self, claims, claim_labels, evidence_indices, evidences):\n",
        "        \"\"\"\n",
        "        claims: List of numericalized claims.\n",
        "        claim_labels: List of labels for the claims.\n",
        "        evidence_indices: List of lists, where each sublist contains indices of evidences associated with the claim.\n",
        "        evidences: List of all numericalized evidences.\n",
        "        \"\"\"\n",
        "        self.claims = claims\n",
        "        self.claim_labels = claim_labels\n",
        "        self.evidence_indices = evidence_indices\n",
        "        self.evidences = evidences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.claims)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        claim = self.claims[idx]\n",
        "        #label = self.claim_labels[idx]\n",
        "        evidence_idxs = self.evidence_indices[idx]\n",
        "        evidences = [self.evidences[i] for i in evidence_idxs]\n",
        "    #     return claim, evidences, label\n",
        "        if self.claim_labels is not None:\n",
        "            label = self.claim_labels[idx]\n",
        "            return claim, evidences, label\n",
        "        else:\n",
        "            # Return None or a dummy value for label if not available\n",
        "            return claim, evidences, -1\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    claims, evidence_lists, labels = zip(*batch)\n",
        "\n",
        "    # Padding claims\n",
        "    claims_padded = pad_sequence([torch.tensor(claim) for claim in claims], batch_first=True, padding_value=vocab['<pad>'])\n",
        "\n",
        "    # Padding evidences independently for each claim\n",
        "    evidence_padded = [pad_sequence([torch.tensor(evidence) for evidence in evidences], batch_first=True, padding_value=vocab['<pad>']) for evidences in evidence_lists]\n",
        "\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return claims_padded, evidence_padded, labels_tensor\n",
        "\n",
        "train_dataset = ClaimEvidenceDataset(train_claims_numerical, train_claim_labels, train_evidence_idxs, evidence_numerical)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "viPwRuD789ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# class ClaimEvidenceModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
        "#         super(ClaimEvidenceModel, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "#         self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "#         self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "#         self.attention = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
        "\n",
        "#     def forward(self, claims, evidence_lists):\n",
        "#         # Embed and encode claims\n",
        "#         embedded_claims = self.embedding(claims)\n",
        "#         _, hidden_claims = self.gru(embedded_claims)  # [num_layers * num_directions, batch_size, hidden_dim]\n",
        "#         hidden_claims = torch.cat((hidden_claims[-2,:,:], hidden_claims[-1,:,:]), dim=1)\n",
        "\n",
        "#         batch_size = claims.size(0)\n",
        "#         # Initialize tensor to store max logits from each evidence set\n",
        "#         max_logits = torch.full((batch_size, self.fc.out_features), float('-inf'), device=claims.device)\n",
        "\n",
        "#         for i in range(batch_size):\n",
        "#             evidences = evidence_lists[i]\n",
        "#             embedded_evidences = self.embedding(evidences)\n",
        "#             _, hidden_evidences = self.gru(embedded_evidences)\n",
        "#             hidden_evidences = torch.cat((hidden_evidences[-2,:,:], hidden_evidences[-1,:,:]), dim=1)\n",
        "\n",
        "#             # Calculate logits for each evidence\n",
        "#             logits = self.fc(hidden_evidences)  # Output shape: [num_evidences, output_dim]\n",
        "\n",
        "#             # Apply softmax to get probabilities for attention (alternative approach)\n",
        "#             attention_weights = F.softmax(self.attention(hidden_evidences), dim=0)\n",
        "#             evidence_representation = torch.sum(attention_weights * logits, dim=0, keepdim=True)\n",
        "\n",
        "#             # Use max pooling across the logits\n",
        "#             max_logits[i, :] = torch.max(logits, dim=0).values\n",
        "\n",
        "#         return F.log_softmax(max_logits, dim=1)\n",
        "\n",
        "class ClaimEvidenceModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
        "        super(ClaimEvidenceModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, claims, evidence_lists):\n",
        "        # Embed and encode claims\n",
        "        embedded_claims = self.embedding(claims)\n",
        "        claims_mask = (claims != self.pad_idx).unsqueeze(2).float()\n",
        "        embedded_claims *= claims_mask\n",
        "\n",
        "        _, hidden_claims = self.gru(embedded_claims)\n",
        "        hidden_claims = torch.cat((hidden_claims[-2,:,:], hidden_claims[-1,:,:]), dim=1)\n",
        "\n",
        "        batch_size = claims.size(0)\n",
        "        max_logits = torch.full((batch_size, self.fc.out_features), float('-inf'), device=claims.device)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            evidences = evidence_lists[i]\n",
        "            embedded_evidences = self.embedding(evidences)\n",
        "            evidences_mask = (evidences != self.pad_idx).unsqueeze(2).float()\n",
        "            embedded_evidences *= evidences_mask\n",
        "\n",
        "            _, hidden_evidences = self.gru(embedded_evidences)\n",
        "            hidden_evidences = torch.cat((hidden_evidences[-2,:,:], hidden_evidences[-1,:,:]), dim=1)\n",
        "\n",
        "            logits = self.fc(hidden_evidences)\n",
        "            attention_weights = F.softmax(self.attention(hidden_evidences), dim=0)\n",
        "            evidence_representation = torch.sum(attention_weights * logits, dim=0, keepdim=True)\n",
        "\n",
        "            max_logits[i, :] = torch.max(logits, dim=0).values\n",
        "\n",
        "        return F.log_softmax(max_logits, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Model instantiation\n",
        "model = ClaimEvidenceModel(vocab_size=len(vocab), embedding_dim=100, hidden_dim=256, output_dim=len(label_map), pad_idx=vocab['<pad>'])\n"
      ],
      "metadata": {
        "id": "ZahBwCH8BM-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch import Tensor\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dim_feedforward, dropout):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(emb_size, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.linear1 = nn.Linear(emb_size, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, emb_size)\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = F.relu\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor = None, src_key_padding_mask: Tensor = None):\n",
        "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "\n",
        "\n",
        "class ClaimVerificationTransformer(nn.Module):\n",
        "    def __init__(self, num_layers: int, emb_size: int, nhead: int, vocab_size: int, dim_feedforward: int, num_classes: int, dropout: float = 0.1, maxlen: int = 5000, pad_idx: int = 0):\n",
        "        super(ClaimVerificationTransformer, self).__init__()\n",
        "        self.embedding = TokenEmbedding(vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout, maxlen)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout, batch_first=True)\n",
        "        self.transformer_claim = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.transformer_evidence = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(emb_size, num_classes)\n",
        "        self.attention = nn.Linear(emb_size, 1)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, claims: Tensor, evidence_lists: Tensor):\n",
        "        # Embed and encode claims\n",
        "        embedded_claims = self.embedding(claims)\n",
        "        embedded_claims = self.positional_encoding(embedded_claims)\n",
        "        claims_mask = (claims == self.pad_idx)\n",
        "        claims_encoded = self.transformer_claim(embedded_claims, src_key_padding_mask=claims_mask)\n",
        "        #claims_encoded = claims_encoded.mean(dim=1)  # Average pooling\n",
        "        claims_encoded, _ = torch.max(claims_encoded, dim=1)  # Max pooling\n",
        "\n",
        "        batch_size = claims.size(0)\n",
        "        evidence_representations = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            evidences = evidence_lists[i]\n",
        "            embedded_evidences = self.embedding(evidences)\n",
        "            embedded_evidences = self.positional_encoding(embedded_evidences)\n",
        "            evidences_mask = (evidences == self.pad_idx)\n",
        "            evidences_encoded = self.transformer_evidence(embedded_evidences, src_key_padding_mask=evidences_mask)\n",
        "            evidences_encoded = evidences_encoded.mean(dim=1)  # Average pooling\n",
        "            #evidences_encoded, _ = torch.max(evidences_encoded, dim=1)  # Max pooling\n",
        "\n",
        "            # Attention mechanism\n",
        "            attention_weights = F.softmax(self.attention(evidences_encoded), dim=0)\n",
        "            evidence_representation = torch.sum(attention_weights * evidences_encoded, dim=0)\n",
        "            evidence_representations.append(evidence_representation)\n",
        "\n",
        "        evidence_representations = torch.stack(evidence_representations)\n",
        "        combined_representation = evidence_representations + claims_encoded\n",
        "        logits = self.fc(combined_representation)\n",
        "        return F.log_softmax(logits, dim=1)\n",
        "\n",
        "# class ClaimVerificationTransformer(nn.Module):\n",
        "#     def __init__(self, num_layers: int, emb_size: int, nhead: int, vocab_size: int, dim_feedforward: int, num_classes: int, dropout: float = 0.1, maxlen: int = 5000, pad_idx: int = 0):\n",
        "#         super(ClaimVerificationTransformer, self).__init__()\n",
        "#         self.embedding = TokenEmbedding(vocab_size, emb_size)\n",
        "#         self.positional_encoding = PositionalEncoding(emb_size, dropout, maxlen)\n",
        "#         self.transformer_claim = nn.ModuleList([\n",
        "#             TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout)\n",
        "#             for _ in range(num_layers)\n",
        "#         ])\n",
        "#         self.transformer_evidence = nn.ModuleList([\n",
        "#             TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout)\n",
        "#             for _ in range(num_layers)\n",
        "#         ])\n",
        "#         self.word_attention = nn.Linear(emb_size, 1, bias=False)\n",
        "#         self.sentence_attention = nn.Linear(emb_size, 1, bias=False)\n",
        "#         self.fc = nn.Linear(emb_size, num_classes)\n",
        "#         self.pad_idx = pad_idx\n",
        "\n",
        "#     def forward(self, claims: Tensor, evidence_lists: Tensor):\n",
        "#         # Embed and encode claims\n",
        "#         embedded_claims = self.embedding(claims)\n",
        "#         embedded_claims = self.positional_encoding(embedded_claims)\n",
        "#         claims_mask = (claims == self.pad_idx)\n",
        "\n",
        "#         for layer in self.transformer_claim:\n",
        "#             embedded_claims = layer(embedded_claims, src_key_padding_mask=claims_mask)\n",
        "#         claims_encoded, _ = torch.max(embedded_claims, dim=1)  # Max pooling\n",
        "\n",
        "#         batch_size = claims.size(0)\n",
        "#         evidence_representations = []\n",
        "\n",
        "#         for i in range(batch_size):\n",
        "#             evidences = evidence_lists[i]\n",
        "#             embedded_evidences = self.embedding(evidences)\n",
        "#             embedded_evidences = self.positional_encoding(embedded_evidences)\n",
        "#             evidences_mask = (evidences == self.pad_idx)\n",
        "\n",
        "#             for layer in self.transformer_evidence:\n",
        "#                 embedded_evidences = layer(embedded_evidences, src_key_padding_mask=evidences_mask)\n",
        "#             evidences_encoded, _ = torch.max(embedded_evidences, dim=1)  # Max pooling\n",
        "\n",
        "#             # Word-level attention mechanism\n",
        "#             word_attention_weights = F.softmax(self.word_attention(evidences_encoded), dim=0)\n",
        "#             evidence_representation = torch.sum(word_attention_weights * evidences_encoded, dim=0)\n",
        "#             evidence_representations.append(evidence_representation)\n",
        "\n",
        "#         evidence_representations = torch.stack(evidence_representations)\n",
        "\n",
        "#         # Sentence-level attention mechanism\n",
        "#         sentence_attention_weights = F.softmax(self.sentence_attention(evidence_representations), dim=0)\n",
        "#         sentence_representation = torch.sum(sentence_attention_weights * evidence_representations, dim=0)\n",
        "\n",
        "#         combined_representation = sentence_representation + claims_encoded\n",
        "#         logits = self.fc(combined_representation)\n",
        "#         return F.log_softmax(logits, dim=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4kE815v_zjIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def train_model(model, train_loader, dev_loader, criterion, optimizer, device, num_epochs=10, grad_clip=1.0):\n",
        "    model = model.to(device)  # Ensure the model is on the right device\n",
        "    criterion = criterion.to(device)  # Also move the criterion to the GPU if available\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for claims, evidences, labels in train_loader:\n",
        "            claims = claims.to(device)\n",
        "            evidences = [e.to(device) for e in evidences]  # Move each batch of evidences to GPU\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(claims, evidences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs} - Training Loss: {avg_train_loss}')\n",
        "\n",
        "        # Evaluate on the development set\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for claims, evidences, labels in dev_loader:\n",
        "                claims = claims.to(device)\n",
        "                evidences = [e.to(device) for e in evidences]\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(claims, evidences)\n",
        "                val_loss = criterion(outputs, labels)\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                all_preds.extend(predicted.cpu().numpy())  # Move predictions back to CPU for scoring\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(dev_loader)\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs} - Validation Loss: {avg_val_loss}')\n",
        "        print(classification_report(all_labels, all_preds, zero_division=0))\n",
        "\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_label_counts = train_claim_labels.value_counts()\n",
        "# Convert label counts to a Tensor\n",
        "train_label_counts_tensor = torch.tensor(train_label_counts.sort_index().values, dtype=torch.float)\n",
        "\n",
        "# Calculate weights: inversely proportional to the class frequencies\n",
        "class_weights = 1.0 / train_label_counts_tensor\n",
        "\n",
        "# Normalize weights so that the smallest weight is 1.0\n",
        "class_weights = class_weights / class_weights.min()\n",
        "\n",
        "# Move weights to the correct device (GPU or CPU)\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "# Model instantiation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model = ClaimEvidenceModel(vocab_size=len(vocab), embedding_dim=100, hidden_dim=128, output_dim=len(label_map), pad_idx=vocab['<pad>'])\n",
        "model = ClaimVerificationTransformer(\n",
        "    num_layers=2,\n",
        "    emb_size=360,\n",
        "    nhead=4,\n",
        "    vocab_size=len(vocab),\n",
        "    dim_feedforward=512,\n",
        "    num_classes=len(label_map),\n",
        "    dropout=0.5,\n",
        "    maxlen=5000,\n",
        "    pad_idx=vocab['<pad>']\n",
        ")\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "# criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=vocab['<pad>'])\n",
        "criterion = nn.NLLLoss(weight=class_weights)\n",
        "\n",
        "# Load data\n",
        "train_dataset = ClaimEvidenceDataset(train_claims_numerical, train_claim_labels, train_evidence_idxs, evidence_numerical)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=custom_collate_fn)\n",
        "dev_dataset = ClaimEvidenceDataset(dev_claims_numerical, dev_claim_labels, dev_k_indices, evidence_numerical)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "# Training and evaluating the model\n",
        "train_model(model, train_loader, dev_loader, criterion, optimizer, device=device, num_epochs=20)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTHBV9qOIKTG",
        "outputId": "f6b0955b-e1d4-4f11-bc41-4e96b21e0bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Training Loss: 2.172918736934662\n",
            "Epoch 1/20 - Validation Loss: 1.4890036582946777\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.18      1.00      0.30        27\n",
            "           2       0.00      0.00      0.00        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.18       154\n",
            "   macro avg       0.04      0.25      0.08       154\n",
            "weighted avg       0.03      0.18      0.05       154\n",
            "\n",
            "Epoch 2/20 - Training Loss: 1.4130306601524354\n",
            "Epoch 2/20 - Validation Loss: 1.6923393607139587\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.18      0.96      0.30        27\n",
            "           2       0.44      0.06      0.10        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.19       154\n",
            "   macro avg       0.16      0.26      0.10       154\n",
            "weighted avg       0.23      0.19      0.10       154\n",
            "\n",
            "Epoch 3/20 - Training Loss: 1.2916382670402526\n",
            "Epoch 3/20 - Validation Loss: 1.7335440516471863\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.23      0.52      0.32        27\n",
            "           2       0.43      0.59      0.50        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.35       154\n",
            "   macro avg       0.16      0.28      0.20       154\n",
            "weighted avg       0.23      0.35      0.28       154\n",
            "\n",
            "Epoch 4/20 - Training Loss: 1.2000791072845458\n",
            "Epoch 4/20 - Validation Loss: 1.8290648460388184\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.25      0.41      0.31        27\n",
            "           2       0.43      0.69      0.53        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.38       154\n",
            "   macro avg       0.17      0.27      0.21       154\n",
            "weighted avg       0.23      0.38      0.29       154\n",
            "\n",
            "Epoch 5/20 - Training Loss: 1.1783608794212341\n",
            "Epoch 5/20 - Validation Loss: 1.6054121255874634\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.19      0.74      0.30        27\n",
            "           2       0.49      0.29      0.37        68\n",
            "           3       0.50      0.05      0.09        41\n",
            "\n",
            "    accuracy                           0.27       154\n",
            "   macro avg       0.29      0.27      0.19       154\n",
            "weighted avg       0.38      0.27      0.24       154\n",
            "\n",
            "Epoch 6/20 - Training Loss: 1.1325671553611756\n",
            "Epoch 6/20 - Validation Loss: 1.5510473847389221\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.12      0.22      0.16        18\n",
            "           1       0.23      0.56      0.33        27\n",
            "           2       0.44      0.37      0.40        68\n",
            "           3       1.00      0.02      0.05        41\n",
            "\n",
            "    accuracy                           0.29       154\n",
            "   macro avg       0.45      0.29      0.23       154\n",
            "weighted avg       0.52      0.29      0.27       154\n",
            "\n",
            "Epoch 7/20 - Training Loss: 1.093510490655899\n",
            "Epoch 7/20 - Validation Loss: 1.9044117331504822\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.18      0.67      0.28        27\n",
            "           2       0.45      0.35      0.40        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.27       154\n",
            "   macro avg       0.16      0.25      0.17       154\n",
            "weighted avg       0.23      0.27      0.22       154\n",
            "\n",
            "Epoch 8/20 - Training Loss: 1.0418055057525635\n",
            "Epoch 8/20 - Validation Loss: 1.7834139466285706\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.21      0.41      0.28        27\n",
            "           2       0.44      0.66      0.53        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.36       154\n",
            "   macro avg       0.16      0.27      0.20       154\n",
            "weighted avg       0.23      0.36      0.28       154\n",
            "\n",
            "Epoch 9/20 - Training Loss: 1.0330120742321014\n",
            "Epoch 9/20 - Validation Loss: 1.8519176244735718\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.59      0.30        27\n",
            "           2       0.47      0.50      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.32       154\n",
            "   macro avg       0.17      0.27      0.19       154\n",
            "weighted avg       0.24      0.32      0.26       154\n",
            "\n",
            "Epoch 10/20 - Training Loss: 0.9993395924568176\n",
            "Epoch 10/20 - Validation Loss: 1.8131577968597412\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.52      0.29        27\n",
            "           2       0.45      0.56      0.50        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.34       154\n",
            "   macro avg       0.16      0.27      0.20       154\n",
            "weighted avg       0.23      0.34      0.27       154\n",
            "\n",
            "Epoch 11/20 - Training Loss: 1.0009517550468445\n",
            "Epoch 11/20 - Validation Loss: 1.861646294593811\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.21      0.52      0.30        27\n",
            "           2       0.43      0.56      0.49        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.34       154\n",
            "   macro avg       0.16      0.27      0.20       154\n",
            "weighted avg       0.23      0.34      0.27       154\n",
            "\n",
            "Epoch 12/20 - Training Loss: 0.9930105745792389\n",
            "Epoch 12/20 - Validation Loss: 1.7857908606529236\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.21      0.52      0.30        27\n",
            "           2       0.43      0.56      0.49        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.34       154\n",
            "   macro avg       0.16      0.27      0.20       154\n",
            "weighted avg       0.23      0.34      0.27       154\n",
            "\n",
            "Epoch 13/20 - Training Loss: 0.9851340293884278\n",
            "Epoch 13/20 - Validation Loss: 1.8620266914367676\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.44      0.27        27\n",
            "           2       0.41      0.56      0.47        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.32       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.21      0.32      0.26       154\n",
            "\n",
            "Epoch 14/20 - Training Loss: 1.0021137833595275\n",
            "Epoch 14/20 - Validation Loss: 1.8330886363983154\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.44      0.28        27\n",
            "           2       0.41      0.57      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.33       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.22      0.33      0.26       154\n",
            "\n",
            "Epoch 15/20 - Training Loss: 0.9901392936706543\n",
            "Epoch 15/20 - Validation Loss: 1.8163567781448364\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.44      0.28        27\n",
            "           2       0.41      0.57      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.33       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.22      0.33      0.26       154\n",
            "\n",
            "Epoch 16/20 - Training Loss: 0.9810118854045868\n",
            "Epoch 16/20 - Validation Loss: 1.8217646479606628\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.44      0.28        27\n",
            "           2       0.41      0.57      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.33       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.22      0.33      0.26       154\n",
            "\n",
            "Epoch 17/20 - Training Loss: 0.969785088300705\n",
            "Epoch 17/20 - Validation Loss: 1.8446078896522522\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.21      0.48      0.29        27\n",
            "           2       0.42      0.56      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.33       154\n",
            "   macro avg       0.16      0.26      0.19       154\n",
            "weighted avg       0.22      0.33      0.26       154\n",
            "\n",
            "Epoch 18/20 - Training Loss: 0.9827152252197265\n",
            "Epoch 18/20 - Validation Loss: 1.8482361435890198\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.20      0.44      0.28        27\n",
            "           2       0.41      0.57      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.33       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.22      0.33      0.26       154\n",
            "\n",
            "Epoch 19/20 - Training Loss: 0.9499853372573852\n",
            "Epoch 19/20 - Validation Loss: 1.8474194407463074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.19      0.44      0.26        27\n",
            "           2       0.42      0.56      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.32       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.22      0.32      0.26       154\n",
            "\n",
            "Epoch 20/20 - Training Loss: 0.969256979227066\n",
            "Epoch 20/20 - Validation Loss: 1.8468242287635803\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        18\n",
            "           1       0.19      0.44      0.26        27\n",
            "           2       0.42      0.56      0.48        68\n",
            "           3       0.00      0.00      0.00        41\n",
            "\n",
            "    accuracy                           0.32       154\n",
            "   macro avg       0.15      0.25      0.19       154\n",
            "weighted avg       0.22      0.32      0.26       154\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "dev_dataset = ClaimEvidenceDataset(dev_claims_numerical, dev_claim_labels, dev_evidence_idxs, evidence_numerical)\n",
        "\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for claims, evidences, labels in dev_loader:\n",
        "        claims = claims.to(device)\n",
        "        evidences = [e.to(device) for e in evidences]  # List comprehension to move each batch of evidences to GPU\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(claims, evidences)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)"
      ],
      "metadata": {
        "id": "sxQ7UcSRE2NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_idx_to_id_dict = {idx: id for id, idx in evidence_id_dict.items()}\n",
        "\n",
        "label_map_inverse = {v: k for k, v in label_map.items()}\n",
        "\n",
        "\n",
        "\n",
        "dev_label_predictions = [label_map_inverse[pred] for pred in all_preds]\n",
        "dev_converted_evidence_ids = [[evidence_idx_to_id_dict[idx] for idx in indices] for indices in dev_k_indices]\n",
        "\n",
        "results = {}\n",
        "for i, claim_id in enumerate(dev_claim_ids):\n",
        "    results[claim_id] = {\n",
        "        \"claim_text\": dev_claims_text[i],\n",
        "        \"claim_label\": dev_label_predictions[i],\n",
        "        \"evidences\": dev_converted_evidence_ids[i]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "8VcKIiwIFc6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/nlp/data/dev_predictions.json', 'w') as file:\n",
        "    json.dump(results, file)"
      ],
      "metadata": {
        "id": "euOKIx5myHsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ClaimEvidenceDataset(test_claims_numerical, None, test_k_indices, evidence_numerical)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for claims, evidences,_ in test_loader:  # Note that labels are not loaded\n",
        "        claims = claims.to(device)\n",
        "        evidences = [e.to(device) for e in evidences]  # Ensure evidences are moved to GPU\n",
        "        outputs = model(claims, evidences)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "# `all_preds` now contains the predicted labels for your test dataset\n"
      ],
      "metadata": {
        "id": "QADtxafjlrKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse mapping for label_map\n",
        "label_map_inverse = {v: k for k, v in label_map.items()}\n",
        "# Convert numerical predictions to label strings\n",
        "label_predictions = [label_map_inverse[pred] for pred in all_preds]\n"
      ],
      "metadata": {
        "id": "ZYzq14Mvm1C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming evidence_id_dict maps from IDs to indices, create the inverse mapping\n",
        "evidence_idx_to_id_dict = {idx: id for id, idx in evidence_id_dict.items()}\n",
        "\n",
        "# Now convert indices to IDs (assuming you have a list of indices)\n",
        "# example_indices = [1, 2, 3]  # Replace with actual indices if needed\n",
        "# converted_ids = [evidence_idx_to_id_dict[idx] for idx in example_indices]\n",
        "converted_evidence_ids = [[evidence_idx_to_id_dict[idx] for idx in indices] for indices in test_k_indices]"
      ],
      "metadata": {
        "id": "RFti-ljAnlzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converted_evidence_ids[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pep3wub4nunX",
        "outputId": "44c27e59-6fbf-40dc-9d60-e09a6e317c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['evidence-55562',\n",
              " 'evidence-1032935',\n",
              " 'evidence-60163',\n",
              " 'evidence-225665',\n",
              " 'evidence-377026']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_claims_id\n",
        "test_claims_text\n",
        "label_predictions\n",
        "converted_evidence_ids\n"
      ],
      "metadata": {
        "id": "QQiprVWjn3sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json.dump(label_predictions, open(\"/content/drive/MyDrive/nlp/data/test_label_predictions.json\", \"w\"))"
      ],
      "metadata": {
        "id": "fs8MJfNmu0MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "for i, claim_id in enumerate(test_claims_id):\n",
        "    results[claim_id] = {\n",
        "        \"claim_text\": test_claims_text[i],\n",
        "        \"claim_label\": label_predictions[i],\n",
        "        \"evidences\": converted_evidence_ids[i]\n",
        "    }"
      ],
      "metadata": {
        "id": "pgmDJsdno24Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Convert to JSON string\n",
        "json_output = json.dumps(results, indent=4)\n",
        "print(json_output)\n",
        "\n",
        "# Save to a JSON file\n",
        "with open('/content/drive/MyDrive/nlp/data/test_predictions.json', 'w') as file:\n",
        "    json.dump(results, file)"
      ],
      "metadata": {
        "id": "FNiXAD2So6dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}