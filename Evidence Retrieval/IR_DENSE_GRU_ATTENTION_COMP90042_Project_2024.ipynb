{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPitXZzTsyTn",
        "outputId": "dd2cb8c3-790b-42f3-879a-2764d1a29b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VcWPFMqtJ4x"
      },
      "outputs": [],
      "source": [
        "import torchdata.datapipes as dp\n",
        "import torchtext.transforms as T\n",
        "import spacy\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "eng = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohyt_MA86_7e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "evidences = pd.read_json('/content/drive/MyDrive/nlp/data/evidence.json', orient='index')\n",
        "train_claims = pd.read_json('/content/drive/MyDrive/nlp/data/train-claims.json', orient='index')\n",
        "dev_claims = pd.read_json('/content/drive/MyDrive/nlp/data/dev-claims.json', orient='index')\n",
        "\n",
        "#update column names\n",
        "evidences.reset_index(inplace=True)\n",
        "evidences.columns = ['evidence_id', 'evidence_text']\n",
        "\n",
        "train_claims.reset_index(inplace=True)\n",
        "train_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "dev_claims.reset_index(inplace=True)\n",
        "dev_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "evidence_id = evidences['evidence_id']\n",
        "evidence_text = evidences['evidence_text']\n",
        "evidence_idx = evidences.index.tolist()\n",
        "\n",
        "evidence_id_dict = dict(zip(evidence_id, evidence_idx))\n",
        "\n",
        "train_claims_text = train_claims['claim_text']\n",
        "train_evidence_ids = train_claims['evidences']\n",
        "#map evidence_id to their corrosponding index for faster processing\n",
        "train_evidence_idxs = train_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])\n",
        "\n",
        "dev_claims_text = dev_claims['claim_text']\n",
        "dev_evidence_ids = dev_claims['evidences']\n",
        "dev_evidence_idxs = dev_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text preprocessing\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_data(text):\n",
        "    tokens = tt.tokenize(text)\n",
        "\n",
        "    processed_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        token = token.lower()\n",
        "        if token not in stopwords and token.isalpha():\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "            processed_tokens.append(lemma)\n",
        "\n",
        "    return processed_tokens\n",
        "\n",
        "train_claims_text_processed = train_claims_text.apply(preprocess_data)\n",
        "dev_claims_text_precessed = dev_claims_text.apply(preprocess_data)\n",
        "evidence_text_processed = evidence_text.apply(preprocess_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knrp2nykcB9_",
        "outputId": "1a1a87cb-4d83-4569-9c6a-f1f52c5d8808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=evidence_text_processed, vector_size=300, window=10, min_count=3, workers=8, sg=1, hs=0)"
      ],
      "metadata": {
        "id": "LAxRizfodbLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Word2Vec model\n",
        "model.save('/content/drive/MyDrive/nlp/data/word2vec_model.bin')\n"
      ],
      "metadata": {
        "id": "EM2DmVZRTB11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json.dump(train_claims_text_processed.tolist(), open(\"/content/drive/MyDrive/nlp/data/train_claims_text_processed.json\", \"w\"))\n",
        "json.dump(dev_claims_text_precessed.tolist(), open(\"/content/drive/MyDrive/nlp/data/dev_claims_text_precessed.json\", \"w\"))\n",
        "json.dump(evidence_text_processed.tolist(), open(\"/content/drive/MyDrive/nlp/data/evidence_text_processed.json\", \"w\"))"
      ],
      "metadata": {
        "id": "kgFTi-0XTxxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "train_claims_text_processed = json.load(open(\"/content/drive/MyDrive/nlp/data/train_claims_text_processed.json\", \"r\"))\n",
        "dev_claims_text_precessed = json.load(open(\"/content/drive/MyDrive/nlp/data/dev_claims_text_precessed.json\", \"r\"))\n",
        "evidence_text_processed = json.load(open(\"/content/drive/MyDrive/nlp/data/evidence_text_processed.json\", \"r\"))"
      ],
      "metadata": {
        "id": "zR4dnmajWB9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec.load('/content/drive/MyDrive/nlp/data/word2vec_model.bin')"
      ],
      "metadata": {
        "id": "HazfiHgeW9XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def generate_embedding(text, model):\n",
        "    # Filter out words that are not in the Word2Vec model's vocabulary\n",
        "    words = [word for word in text if word in model.wv.key_to_index]\n",
        "    if not words:  # Handle cases where none of the words are in the vocabulary\n",
        "        return np.zeros(model.vector_size)\n",
        "    # Get embeddings for each word in the text and average them\n",
        "    word_embeddings = [model.wv[word] for word in words]\n",
        "    sentence_embedding = np.mean(word_embeddings, axis=0)\n",
        "    return sentence_embedding"
      ],
      "metadata": {
        "id": "Db5C5dYpdgm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Generate embeddings for train claim_text\n",
        "train_claim_text_embeddings = [generate_embedding(text, model) for text in train_claims_text_processed]\n",
        "\n",
        "# Generate embeddings for dev claim_text\n",
        "dev_claim_text_embeddings = [generate_embedding(text, model) for text in dev_claims_text_precessed]\n",
        "\n",
        "# Generate embeddings for all evidence texts\n",
        "evidence_embeddings = [generate_embedding(text, model) for text in evidence_text_processed]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gasyfrL7dhA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to compute cosine similarity scores for all claims and evidence embeddings\n",
        "def compute_similarity_scores(claim_embeddings, evidence_embeddings):\n",
        "    similarity_scores = cosine_similarity(claim_embeddings, evidence_embeddings)\n",
        "    return similarity_scores\n",
        "\n",
        "\n",
        "# Compute cosine similarity scores for training claims and evidence embeddings\n",
        "train_similarity_scores = compute_similarity_scores(train_claim_text_embeddings, evidence_embeddings)\n",
        "\n",
        "# Compute cosine similarity scores for development claims and evidence embeddings\n",
        "dev_similarity_scores = compute_similarity_scores(dev_claim_text_embeddings, evidence_embeddings)"
      ],
      "metadata": {
        "id": "LFkd9dp_dhG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Function to compute recall at k\n",
        "def compute_recall_at_k(similarity_scores, true_indices, k):\n",
        "    recall_values = []\n",
        "\n",
        "    # Convert similarity scores to PyTorch tensor\n",
        "    similarity_scores_tensor = torch.FloatTensor(similarity_scores)\n",
        "\n",
        "    # Get top k indices for each sample\n",
        "    top_k_indices = torch.topk(similarity_scores_tensor, k, dim=-1).indices.tolist()\n",
        "\n",
        "    for i in range(len(true_indices)):\n",
        "        # Get true indices for the i-th sample\n",
        "        true_indices_i = true_indices[i]\n",
        "\n",
        "        # Calculate recall count for the i-th sample\n",
        "        recall_count = sum(1 for idx in true_indices_i if idx in top_k_indices[i])\n",
        "\n",
        "        # Calculate recall for the i-th sample\n",
        "        recall = recall_count / len(true_indices_i)\n",
        "\n",
        "        # Store recall value\n",
        "        recall_values.append(recall)\n",
        "\n",
        "    # Compute average recall over all samples\n",
        "    avg_recall = sum(recall_values) / len(recall_values)\n",
        "\n",
        "    return avg_recall\n",
        "\n",
        "# Example usage:\n",
        "# Compute recall at k for training set\n",
        "train_recall_at_k = compute_recall_at_k(train_similarity_scores, train_evidence_idxs, 15)\n",
        "print(\"Training Recall at K:\", train_recall_at_k)\n",
        "\n",
        "# Compute recall at k for dev set\n",
        "dev_recall_at_k = compute_recall_at_k(dev_similarity_scores, dev_evidence_idxs, 10)\n",
        "print(\"Dev Recall at K:\", dev_recall_at_k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_4j24NwdhJz",
        "outputId": "aa2ad93f-3d75-43bc-aafa-413fd729cb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Recall at K: 0.1888707926167208\n",
            "Dev Recall at K: 0.17651515151515146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def get_top_k_indices(similarity_scores, k=1000):\n",
        "    # Args:\n",
        "    # similarity_scores: a PyTorch tensor of shape (num_claims, num_evidences)\n",
        "    # Returns:\n",
        "    # top_k_indices: a tensor of shape (num_claims, k) containing indices of the top k elements\n",
        "\n",
        "    # Convert numpy array to PyTorch tensor if needed\n",
        "    if isinstance(similarity_scores, np.ndarray):\n",
        "        similarity_scores_tensor = torch.FloatTensor(similarity_scores)\n",
        "    else:\n",
        "        similarity_scores_tensor = similarity_scores\n",
        "\n",
        "    # Get top k indices for each sample using PyTorch\n",
        "    top_k_values, top_k_indices = torch.topk(similarity_scores_tensor, k, dim=1, largest=True, sorted=True)\n",
        "\n",
        "    return top_k_indices, top_k_values\n",
        "\n",
        "# Compute top 1000 indices for training and development sets\n",
        "train_top_indices, _ = get_top_k_indices(train_similarity_scores, k=100)\n",
        "dev_top_indices, dev_orig_scores = get_top_k_indices(dev_similarity_scores, k=100)"
      ],
      "metadata": {
        "id": "mdqhdMWbdhMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_top_indices[0]"
      ],
      "metadata": {
        "id": "ysbIZTgtYlHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "embedding_dim = model.vector_size\n",
        "vocab = {word: idx for idx, word in enumerate(model.wv.index_to_key)}\n",
        "\n",
        "# Calculate the current maximum index in the vocabulary\n",
        "max_index = max(vocab.values())\n",
        "\n",
        "\n",
        "# vocab['<cls>'] = max_index + 1\n",
        "# vocab['<sep>'] = max_index + 2\n",
        "vocab['<unk>'] = max_index + 1\n",
        "vocab['<pad>'] = max_index + 2\n",
        "\n",
        "#vocab['<unk>'] = len(vocab)\n",
        "\n",
        "# Create random embeddings for the three new special tokens\n",
        "random_embeddings = np.random.randn(1, embedding_dim)\n",
        "\n",
        "# Extend the embedding matrix with random embeddings for special tokens\n",
        "padding_embeddings = np.zeros((1, embedding_dim))  # Typically zero vector for padding\n",
        "extended_embeddings = np.vstack([\n",
        "    model.wv.vectors,  # Existing embeddings from Word2Vec\n",
        "    random_embeddings,  # Embeddings for `<unk>`\n",
        "    padding_embeddings  # Zero embeddings for `<pad>`\n",
        "])\n",
        "embedding_matrix = torch.FloatTensor(extended_embeddings)\n",
        "\n",
        "# Check new size of the embedding matrix and the maximum index in the vocabulary\n",
        "print(f\"New size of embedding matrix: {embedding_matrix.size(0)}\")\n",
        "print(f\"Maximum index in vocab: {max(vocab.values())}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-9QGjQiO45J",
        "outputId": "195e3503-6247-448b-8bed-b876f56fc01b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New size of embedding matrix: 144508\n",
            "Maximum index in vocab: 144507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(text, vocab):\n",
        "    return [vocab.get(token, vocab['<unk>']) for token in text]\n",
        "\n",
        "def convert_to_indices(train_claims, x1, x2, vocab):\n",
        "    train_claims_indices = []\n",
        "    x1_indices = []\n",
        "    x2_indices = []\n",
        "\n",
        "    # Convert train claims to indices\n",
        "    for claim in train_claims:\n",
        "        train_claims_indices.append(text_to_indices(claim, vocab))\n",
        "\n",
        "    # Convert x1 and x2 to indices\n",
        "    for evidence in x1:\n",
        "        x1_indices.append(text_to_indices(evidence, vocab))\n",
        "    for evidence in x2:\n",
        "        x2_indices.append(text_to_indices(evidence, vocab))\n",
        "\n",
        "    return train_claims_indices, x1_indices, x2_indices\n",
        "\n",
        "train_claims_indices, x1_indices, x2_indices = convert_to_indices(train_claims, x1, x2, vocab)\n",
        "\n",
        "# def convert_to_indicies(claims, evidences, vocab):\n",
        "#     train_claims_indices = []\n",
        "#     evidences_indices = []\n",
        "\n",
        "#     for claim in claims:\n",
        "#         train_claims_indices.append(text_to_indices(claim, vocab))\n",
        "\n",
        "#     for evidence in evidences:\n",
        "#         evidences_indices.append(text_to_indices(evidence, vocab))\n",
        "\n",
        "#     return train_claims_indices, evidences_indices\n",
        "\n",
        "# train_claims_indices, evidences_indices = convert_to_indicies(train_claims, train_evidences, vocab)\n"
      ],
      "metadata": {
        "id": "hrH_rKkRImjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "def text_to_indices(text, vocab):\n",
        "    #print(text)\n",
        "    return [vocab.get(token, vocab['<unk>']) for token in text]\n",
        "\n",
        "class ListwiseRankingDataset(Dataset):\n",
        "    def __init__(self, claims, evidences, true_indices, top_k_indices, k=3000, neg_samples = 32):\n",
        "        self.claims = claims\n",
        "        self.evidences = evidences\n",
        "        self.true_indices = true_indices\n",
        "        self.top_k_indices = top_k_indices\n",
        "        self.k = k\n",
        "        self.neg_samples = neg_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.claims)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        claim = self.claims[idx]\n",
        "        true_idxs = self.true_indices[idx]\n",
        "        top_k_indices = self.top_k_indices[idx][:self.k]\n",
        "        valid_indices = [i for i in true_idxs if i in top_k_indices]\n",
        "        if not valid_indices:\n",
        "            return None\n",
        "\n",
        "        pos_idx = random.choice(valid_indices)\n",
        "        pos_evidence = self.evidences[pos_idx]\n",
        "\n",
        "        neg_indices = [i for i in top_k_indices if i not in valid_indices]\n",
        "        neg_evidences = random.sample([self.evidences[neg_idx] for neg_idx in neg_indices], min(self.neg_samples, len(neg_indices))) # Ensure we do not exceed available negatives\n",
        "\n",
        "        return claim, pos_evidence, neg_evidences\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Remove None items that were skipped in the dataset\n",
        "    batch = [item for item in batch if item is not None]\n",
        "\n",
        "    if not batch:\n",
        "        # If all items are None, return None. This needs to be handled in the training loop.\n",
        "        return None\n",
        "\n",
        "    claims, pos_evidences, neg_evidences_lists = zip(*batch)\n",
        "\n",
        "    # Convert claims and evidences to indices\n",
        "    claims_indices = [text_to_indices(claim, vocab) for claim in claims]\n",
        "    pos_indices = [text_to_indices(evidence, vocab) for evidence in pos_evidences]\n",
        "    neg_indices = [text_to_indices(neg, vocab) for sublist in neg_evidences_lists for neg in sublist]\n",
        "\n",
        "    # Pad all sequences\n",
        "    claims_padded = pad_sequence([torch.tensor(ci, dtype=torch.long) for ci in claims_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "    pos_padded = pad_sequence([torch.tensor(pi, dtype=torch.long) for pi in pos_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "    neg_padded = pad_sequence([torch.tensor(ni, dtype=torch.long) for ni in neg_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "\n",
        "    # Now that neg_padded is fully defined, you can reshape it\n",
        "    if neg_padded.numel() > 0:  # Check to make sure there are elements to avoid size mismatch\n",
        "        neg_padded = neg_padded.view(len(batch), -1, neg_padded.size(1))\n",
        "\n",
        "    return claims_padded, pos_padded, neg_padded"
      ],
      "metadata": {
        "id": "gnHbMlbLRwhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "\n",
        "def text_to_indices(text, vocab):\n",
        "    #print(text)\n",
        "    return [vocab.get(token, vocab['<unk>']) for token in text]\n",
        "\n",
        "\n",
        "\n",
        "class ListwiseRankingDataset(Dataset):\n",
        "    def __init__(self, claims, evidences, true_indices):\n",
        "        self.claims = claims\n",
        "        self.evidences = evidences\n",
        "        self.pairs = self._create_pairs(true_indices)\n",
        "\n",
        "    def _create_pairs(self, true_indices):\n",
        "        # Create all possible (claim_index, pos_evidence_index) pairs\n",
        "        pairs = []\n",
        "        for claim_idx, indices in enumerate(true_indices):\n",
        "            for pos_idx in indices:\n",
        "                pairs.append((claim_idx, pos_idx))\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        claim_idx, pos_idx = self.pairs[idx]\n",
        "        claim = self.claims[claim_idx]\n",
        "        pos_evidence = self.evidences[pos_idx]\n",
        "        return claim, pos_evidence\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "    claims, pos_evidences = zip(*batch)\n",
        "\n",
        "    # Prepare claims and pos_evidences\n",
        "    claims_indices = [text_to_indices(claim, vocab) for claim in claims]\n",
        "    pos_indices = [text_to_indices(evidence, vocab) for evidence in pos_evidences]\n",
        "\n",
        "    # Pad sequences for claims and positive evidences\n",
        "    claims_padded = pad_sequence([torch.tensor(ci, dtype=torch.long) for ci in claims_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "    pos_padded = pad_sequence([torch.tensor(pi, dtype=torch.long) for pi in pos_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "\n",
        "    # Generate in-batch negatives:\n",
        "    # Each claim gets the positive samples of all other claims as its negatives.\n",
        "    neg_padded_list = []\n",
        "    for i in range(len(batch)):\n",
        "        neg_samples = [pos_indices[j] for j in range(len(batch)) if i != j]\n",
        "        neg_padded = pad_sequence([torch.tensor(ni, dtype=torch.long) for ni in neg_samples], batch_first=True, padding_value=vocab['<pad>'])\n",
        "        neg_padded_list.append(neg_padded)\n",
        "\n",
        "    # Pad the list of negative batches to ensure they all have the same shape\n",
        "    # Find the maximum length of any sequence in any batch\n",
        "    max_length = max([np.size(1) for np in neg_padded_list])\n",
        "\n",
        "    # Pad each batch of negatives to this maximum length\n",
        "    neg_padded_uniform = [pad_sequence(batch, batch_first=True, padding_value=vocab['<pad>'], max_length=max_length) for batch in neg_padded_list]\n",
        "\n",
        "    # Stack the uniformly padded negative samples\n",
        "    neg_padded_stack = torch.stack(neg_padded_uniform, dim=0)\n",
        "\n",
        "    return claims_padded, pos_padded, neg_padded_stack\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T4D9kT94EL0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# class SiameseNetwork(nn.Module):\n",
        "#     def __init__(self, embedding_matrix, hidden_dim):\n",
        "#         super().__init__()\n",
        "#         self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "#         self.gru = nn.GRU(embedding_matrix.size(1), hidden_dim, batch_first=True)\n",
        "\n",
        "#     def forward_one(self, claim, evidence):\n",
        "#         # Embed and process claim\n",
        "#         claim_emb = self.embedding(claim)\n",
        "#         _, claim_hidden = self.gru(claim_emb)\n",
        "#         claim_hidden = claim_hidden.squeeze(0)  # Ensure shape is [batch_size, hidden_dim]\n",
        "\n",
        "#         # Embed and process evidence\n",
        "#         evidence_emb = self.embedding(evidence)\n",
        "#         _, evidence_hidden = self.gru(evidence_emb)\n",
        "#         evidence_hidden = evidence_hidden.squeeze(0)  # Ensure shape is [batch_size, hidden_dim]\n",
        "\n",
        "#         # Calculate cosine similarity\n",
        "#         similarity = F.cosine_similarity(claim_hidden, evidence_hidden, dim=1)\n",
        "#         return similarity\n",
        "\n",
        "#     def forward(self, claim, evidence1, evidence2):\n",
        "#         # Compute cosine similarity for each evidence compared to the same claim\n",
        "#         similarity1 = self.forward_one(claim, evidence1)\n",
        "#         similarity2 = self.forward_one(claim, evidence2)\n",
        "#         return similarity1, similarity2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, 1)  # Adjusted for unidirectional GRU\n",
        "\n",
        "    def forward(self, outputs):\n",
        "        attn_weights = torch.tanh(self.attn(outputs))\n",
        "        attn_weights = F.softmax(attn_weights, dim=1)\n",
        "        context = (attn_weights * outputs).sum(dim=1)\n",
        "        return context\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Single layer unidirectional GRU\n",
        "        self.gru = nn.GRU(embedding_matrix.size(1), hidden_dim, num_layers=1, batch_first=True, dropout=0)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.attention = Attention(hidden_dim)  # Adjusted for unidirectional output\n",
        "\n",
        "    def forward_one(self, text):\n",
        "        text_emb = self.embedding(text)\n",
        "        text_out, _ = self.gru(text_emb)\n",
        "        text_out = self.dropout(text_out)\n",
        "        text_context = self.attention(text_out)\n",
        "        return text_context\n",
        "\n",
        "    def forward(self, claims, evidences):\n",
        "        claim_contexts = self.forward_one(claims)\n",
        "        flattened_evidences = evidences.view(-1, evidences.size(-1))\n",
        "        evidence_contexts = self.forward_one(flattened_evidences)\n",
        "        evidence_contexts = evidence_contexts.view(claims.size(0), -1, evidence_contexts.size(-1))\n",
        "        similarities = F.cosine_similarity(claim_contexts.unsqueeze(1), evidence_contexts, dim=2)\n",
        "        return similarities"
      ],
      "metadata": {
        "id": "J1oQmWvdeAKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def listwise_loss(model, claims_emb, pos_evidences_emb, neg_evidences_emb, ignore_index=vocab['<pad>']):\n",
        "    # Get scores for positive and negative evidences\n",
        "    pos_scores = model(claims_emb, pos_evidences_emb).unsqueeze(1)  # Ensure this is [batch_size, 1]\n",
        "    neg_scores = torch.stack([model(claims_emb, neg) for neg in neg_evidences_emb.transpose(0, 1)], dim=1)  # Should be [batch_size, num_negatives]\n",
        "\n",
        "    # Concatenate pos_scores and neg_scores along dim=1\n",
        "    scores = torch.cat((pos_scores, neg_scores), dim=1)  # Shape should be [batch_size, 1 + num_negatives]\n",
        "    scores = scores.squeeze(-1)  # Remove the unnecessary last dimension\n",
        "    scores = F.log_softmax(scores, dim=1)  # Apply softmax to scores\n",
        "\n",
        "    # Create target tensor where the index of positive examples is always 0\n",
        "    # Assume target can potentially have padding indices which should be ignored\n",
        "    target = torch.zeros(scores.size(0), dtype=torch.long, device=scores.device)\n",
        "    # Set padding indices manually if required, or ensure they are set before calling this function\n",
        "    # Example: target[some_condition] = ignore_index\n",
        "\n",
        "    # Calculate and return NLL Loss with ignore_index\n",
        "    return F.nll_loss(scores, target, ignore_index=ignore_index)"
      ],
      "metadata": {
        "id": "dvoNIk7gSLJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# Assume vocab and embedding_matrix are predefined\n",
        "# Assume the dataset is already loaded with ListwiseRankingDataset\n",
        "dataset = ListwiseRankingDataset(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize model\n",
        "gru_model = SiameseNetwork(embedding_matrix, hidden_dim=128).to(device)\n",
        "gru_model.train()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss Function\n",
        "margin = 1.0  # Define the margin for the margin ranking loss\n",
        "loss_fn = nn.MarginRankingLoss(margin=margin)\n",
        "\n",
        "# Training loop\n",
        "import torch\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        if batch is None:\n",
        "            continue\n",
        "\n",
        "        claims_padded, pos_padded, neg_padded = batch\n",
        "\n",
        "        # Move tensors to GPU if available\n",
        "        claims_padded = claims_padded.to(device)\n",
        "        pos_padded = pos_padded.to(device)\n",
        "        neg_padded = neg_padded.to(device)\n",
        "\n",
        "        # Compute the positive score\n",
        "        pos_score = gru_model(claims_padded, pos_padded)  # [batch_size, 1]\n",
        "\n",
        "        # Initialize loss for the batch\n",
        "        batch_loss = 0\n",
        "\n",
        "        # Iterate over each negative in the padded negatives\n",
        "        # Assume neg_padded is [batch_size, num_negatives, seq_length]\n",
        "        for i in range(neg_padded.size(1)):  # num_negatives dimension\n",
        "            neg_single_padded = neg_padded[:, i, :]  # Select the ith negative for each element in the batch\n",
        "            neg_score = gru_model(claims_padded, neg_single_padded)  # [batch_size, 1]\n",
        "\n",
        "            # Calculate the loss for the current negative\n",
        "            target = torch.ones_like(pos_score)  # [batch_size, 1]\n",
        "            loss = loss_fn(pos_score, neg_score, target)\n",
        "            batch_loss += loss\n",
        "\n",
        "        # Aggregate losses from all negatives before the backward pass\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss.backward()  # Only call backward once all losses have been accumulated\n",
        "        optimizer.step()\n",
        "        total_loss += batch_loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader)}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whSUUihUlvY0",
        "outputId": "c779ecd8-dec9-402b-fa74-c55baba9a710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 13.388742923915673\n",
            "Epoch 2/10, Loss: 8.429342570535551\n",
            "Epoch 3/10, Loss: 6.302742189620263\n",
            "Epoch 4/10, Loss: 5.817699965985062\n",
            "Epoch 5/10, Loss: 8.977933188954081\n",
            "Epoch 6/10, Loss: 5.404826521980935\n",
            "Epoch 7/10, Loss: 3.286890445671116\n",
            "Epoch 8/10, Loss: 3.8300782826084356\n",
            "Epoch 9/10, Loss: 4.4999058139868655\n",
            "Epoch 10/10, Loss: 4.582532592726728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "# Example usage\n",
        "# Define the dataset and dataloader\n",
        "dataset = ListwiseRankingDataset(train_claims_text_processed, evidence_text_processed, train_evidence_idxs)\n",
        "train_loader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "# Model and optimizer\n",
        "# gru_model = SiameseNetwork(embedding_matrix, hidden_dim=128, dropout_rate=0.5)\n",
        "# optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "gru_model = SiameseNetwork(embedding_matrix, hidden_dim=128, dropout_rate=0.5).to(device)\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "gru_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        if batch is None:  # Skip batches that are None\n",
        "            continue\n",
        "        claims, pos_evidences, neg_evidences = batch\n",
        "        claims = claims.to(device)\n",
        "        pos_evidences = pos_evidences.to(device)\n",
        "        neg_evidences = neg_evidences.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        #pos_scores = gru_model(claims, pos_evidences)\n",
        "        #neg_scores = torch.stack([gru_model(claims, neg) for neg in neg_evidences.transpose(0, 1)], dim=1)\n",
        "        loss = listwise_loss(gru_model, claims, pos_evidences, neg_evidences)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    scheduler.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
        "\n",
        "# Note: You may want to add more sophisticated error handling, validation, and model saving mechanisms.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "r5rwOaInSF3S",
        "outputId": "2229d0b4-7ad4-4a85-905d-deda3f3f5d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "pad_sequence() got an unexpected keyword argument 'max_length'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e0d675722386>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Skip batches that are None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-ce3803096b40>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Pad each batch of negatives to this maximum length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mneg_padded_uniform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneg_padded_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Stack the uniformly padded negative samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-ce3803096b40>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Pad each batch of negatives to this maximum length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mneg_padded_uniform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneg_padded_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Stack the uniformly padded negative samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: pad_sequence() got an unexpected keyword argument 'max_length'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def score_query(model, query, evidences, vocab, pad_idx):\n",
        "    # Convert query and evidences to indices using the same function as during training\n",
        "    query_indices = text_to_indices(query, vocab)  # Query to indices\n",
        "    evidence_indices = [text_to_indices(evidence, vocab) for evidence in evidences]  # Evidences to indices\n",
        "\n",
        "    # Convert lists to tensors and pad\n",
        "    query_tensor = pad_sequence([torch.tensor(query_indices)], batch_first=True, padding_value=pad_idx)\n",
        "    evidence_tensors = pad_sequence([torch.tensor(ei) for ei in evidence_indices], batch_first=True, padding_value=pad_idx)\n",
        "\n",
        "    # Assuming your model and data are on the same device\n",
        "    query_tensor = query_tensor.to(device)\n",
        "    evidence_tensors = evidence_tensors.to(device)\n",
        "\n",
        "    # Set the model to evaluation mode and disable gradient computation\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    with torch.no_grad():\n",
        "        # Process all evidences in one batch for efficiency\n",
        "        for i in range(evidence_tensors.shape[0]):\n",
        "            score = model(query_tensor, evidence_tensors[i].unsqueeze(0))\n",
        "            scores.append(score.item())\n",
        "\n",
        "    return scores\n",
        "\n",
        "dev_scores = []\n",
        "for idx in range(len(dev_claims_text_precessed)):\n",
        "    top_k_evidence_idxs = dev_top_indices[idx]\n",
        "    top_k_evidences = [evidence_text_processed[i] for i in top_k_evidence_idxs]\n",
        "    scores = score_query(gru_model, dev_claims_text_precessed[idx], top_k_evidences, vocab, vocab['<pad>'])\n",
        "    dev_scores.append(scores)\n"
      ],
      "metadata": {
        "id": "r94hwnTKFfSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_scores[0][5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH7jvWR9gHUf",
        "outputId": "b6b4433f-d955-4f94-dd3d-c3e439eb19e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.973311185836792"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_scores(original_scores, new_scores, lam=0.1):\n",
        "    \"\"\"\n",
        "    Aggregate scores from two different sources using a weighted sum approach.\n",
        "\n",
        "    Parameters:\n",
        "    - original_scores: List of lists containing scores from the original model.\n",
        "    - new_scores: List of lists containing scores from the new model.\n",
        "    - lam: Weighting factor for new_scores; 1 - lam will be the weight for the original_scores.\n",
        "\n",
        "    Returns:\n",
        "    - List of lists containing aggregated scores.\n",
        "    \"\"\"\n",
        "    aggregated_scores = []\n",
        "    for original, new in zip(original_scores, new_scores):\n",
        "        # Calculate the weighted sum of scores\n",
        "        aggregated = [(1 - lam) * o + lam * n for o, n in zip(original, new)]\n",
        "        aggregated_scores.append(aggregated)\n",
        "    return aggregated_scores\n",
        "\n",
        "scores = aggregate_scores(dev_orig_scores, dev_scores)"
      ],
      "metadata": {
        "id": "NOPiXQIIG432"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reranked_indices = []\n",
        "\n",
        "for indices, scores in zip(dev_top_indices, dev_scores):\n",
        "    # Combine indices and scores into a list of tuples\n",
        "    indexed_scores = list(zip(indices, scores))\n",
        "\n",
        "    # Sort the list of tuples by the score in descending order\n",
        "    sorted_by_score = sorted(indexed_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Extract the sorted indices\n",
        "    sorted_indices = [idx.item() for idx, _ in sorted_by_score]\n",
        "\n",
        "    # Add to the final list\n",
        "    reranked_indices.append(sorted_indices)"
      ],
      "metadata": {
        "id": "iUUrEG0xHOZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_evidence_retrieval(predicted_indices_list, actual_indices_list, k=5):\n",
        "    assert len(predicted_indices_list) == len(actual_indices_list), \"Both inputs must have the same length.\"\n",
        "\n",
        "    total_recall = 0.0\n",
        "    total_precision = 0.0\n",
        "    total_fscore = 0.0\n",
        "    num_claims = len(predicted_indices_list)\n",
        "\n",
        "    for predicted_indices, actual_indices in zip(predicted_indices_list, actual_indices_list):\n",
        "        # Convert tensors in predicted_indices to integers if they are not already\n",
        "        predicted_indices = [index.item() if isinstance(index, torch.Tensor) else index for index in predicted_indices]\n",
        "\n",
        "        # Retrieve the top k predictions\n",
        "        top_k_predicted = set(predicted_indices[:k])\n",
        "        actual_indices_set = set(actual_indices)\n",
        "\n",
        "        # Calculate the number of correct predictions\n",
        "        correct_predictions = len(top_k_predicted.intersection(actual_indices_set))\n",
        "\n",
        "        # Calculate metrics\n",
        "        if correct_predictions > 0:\n",
        "            recall = float(correct_predictions) / len(actual_indices_set)\n",
        "            precision = float(correct_predictions) / k\n",
        "            if (precision + recall) != 0:\n",
        "                fscore = 2 * (precision * recall) / (precision + recall)\n",
        "            else:\n",
        "                fscore = 0.0\n",
        "        else:\n",
        "            recall = 0.0\n",
        "            precision = 0.0\n",
        "            fscore = 0.0\n",
        "\n",
        "        # Accumulate the metrics to calculate averages later\n",
        "        total_recall += recall\n",
        "        total_precision += precision\n",
        "        total_fscore += fscore\n",
        "\n",
        "    # Calculate average metrics\n",
        "    average_recall = total_recall / num_claims\n",
        "    average_precision = total_precision / num_claims\n",
        "    average_fscore = total_fscore / num_claims\n",
        "\n",
        "    return {\n",
        "        \"average_recall\": average_recall,\n",
        "        \"average_precision\": average_precision,\n",
        "        \"average_fscore\": average_fscore\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "results = evaluate_evidence_retrieval(reranked_indices, dev_evidence_idxs)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwjww5C-HWB6",
        "outputId": "60ee20e2-ed2c-481d-874c-d0bb015cb8cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'average_recall': 0.016666666666666666, 'average_precision': 0.012987012987012986, 'average_fscore': 0.014192949907235623}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# def prepare_evaluation_set(dev_claims_text, evidence_text_processed, top_indices):\n",
        "#     eval_claims = []\n",
        "#     eval_evidences = []\n",
        "#     eval_evidences_idxs = []\n",
        "\n",
        "#     for i, claim in enumerate(dev_claims_text):\n",
        "#       claim_top_indices = top_indices[i]\n",
        "\n",
        "#       for idx in claim_top_indices:\n",
        "#         #idx = idx.item()\n",
        "#         eval_claims.append(claim)\n",
        "#         eval_evidences.append(evidence_text_processed[idx])\n",
        "#         eval_evidences_idxs.append(idx)\n",
        "\n",
        "\n",
        "#     return eval_claims, eval_evidences, eval_evidences_idxs\n",
        "\n",
        "# dev_claims, dev_evidences, dev_evidences_idxs = prepare_evaluation_set(dev_claims_text_precessed, evidence_text_processed, dev_top_indices)\n",
        "\n",
        "def convert_to_indices(claims, all_evidences, vocab):\n",
        "    claims_indices = [torch.tensor([vocab.get(token, vocab['<unk>']) for token in claim], dtype=torch.long) for claim in claims]\n",
        "    all_evidences_indices = [[torch.tensor([vocab.get(token, vocab['<unk>']) for token in evidence], dtype=torch.long) for evidence in evidences] for evidences in all_evidences]\n",
        "    return claims_indices, all_evidences_indices\n",
        "\n",
        "# dev_claims_indices, dev_evidence_indices, _ = convert_to_indices(dev_claims, dev_evidences, dev_evidences, vocab)\n",
        "\n",
        "def prepare_evaluation_set(dev_claims_text, evidence_text_processed, top_indices):\n",
        "    eval_claims = []\n",
        "    eval_all_evidences = []\n",
        "    eval_all_evidence_idxs = []\n",
        "\n",
        "    # Iterate over each claim\n",
        "    for i, claim in enumerate(dev_claims_text):\n",
        "        claim_evidences = []\n",
        "        claim_evidence_idxs = []\n",
        "\n",
        "        # Get the list of top indices for the current claim\n",
        "        claim_top_indices = top_indices[i]\n",
        "\n",
        "        # Append each evidence corresponding to the current claim\n",
        "        for idx in claim_top_indices:\n",
        "            claim_evidences.append(evidence_text_processed[idx])\n",
        "            claim_evidence_idxs.append(idx)\n",
        "\n",
        "        # Append the aggregated evidence lists and their indices for the current claim\n",
        "        eval_claims.append(claim)\n",
        "        eval_all_evidences.append(claim_evidences)\n",
        "        eval_all_evidence_idxs.append(claim_evidence_idxs)\n",
        "\n",
        "    return eval_claims, eval_all_evidences, eval_all_evidence_idxs\n",
        "\n",
        "# Example usage\n",
        "dev_claims, dev_all_evidences, dev_all_evidence_idxs = prepare_evaluation_set(dev_claims_text_precessed, evidence_text_processed, dev_top_indices)\n",
        "dev_claims_indices, dev_all_evidence_indices = convert_to_indices(dev_claims, dev_all_evidences, vocab)\n"
      ],
      "metadata": {
        "id": "YVbIWQczKTPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "class EvaluateDataset(Dataset):\n",
        "    def __init__(self, claims, all_evidences, all_evidence_idxs):\n",
        "        self.claims = claims\n",
        "        self.all_evidences = all_evidences\n",
        "        self.all_evidence_idxs = all_evidence_idxs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.claims)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.claims[idx], self.all_evidences[idx], self.all_evidence_idxs[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    claims, all_evidences, all_evidence_idxs = zip(*batch)\n",
        "    claims_padded = pad_sequence([claim for claim in claims], batch_first=True, padding_value=vocab['<pad>'])\n",
        "\n",
        "    # Handling variable number of evidences per claim\n",
        "    all_evidences_padded = [pad_sequence([evidence for evidence in evidences], batch_first=True, padding_value=vocab['<pad>']) for evidences in all_evidences]\n",
        "\n",
        "    return claims_padded, all_evidences_padded, all_evidence_idxs\n",
        "\n",
        "# DataLoader creation\n",
        "eval_dataset = EvaluateDataset(dev_claims_indices, dev_all_evidence_indices, dev_all_evidence_idxs)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "0ziWOAMkd2lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def evaluate(model, data_loader, device, k=5):\n",
        "    model.eval()\n",
        "    results = []\n",
        "    with torch.no_grad():\n",
        "        for claims, all_evidences, all_evidence_idxs in data_loader:\n",
        "            sorted_evidence_idxs = []\n",
        "\n",
        "            # Assuming each batch contains one claim with all its evidences\n",
        "            claim = claims.to(device)\n",
        "\n",
        "            # Process each evidence set\n",
        "            for evidences, evidence_idxs in zip(all_evidences, all_evidence_idxs):\n",
        "                evidence_scores = []\n",
        "                for evidence in evidences:\n",
        "                    evidence = evidence.to(device)\n",
        "                    score = model.forward_one(claim, evidence)\n",
        "                    # If the output is not scalar, adjust accordingly here:\n",
        "                    if score.shape[0] > 1:\n",
        "                        # Example adjustment, assuming the need to take the maximum, mean, or a specific element:\n",
        "                        score = score.mean()  # or score[0], or any appropriate reduction\n",
        "                    evidence_scores.append(score)\n",
        "\n",
        "                # Convert list of scores to a tensor\n",
        "                evidence_scores = torch.stack(evidence_scores).squeeze()\n",
        "                # Sort the scores to find top k\n",
        "                sorted_indices = torch.argsort(evidence_scores, descending=True)\n",
        "                top_k_indices = sorted_indices[:k]\n",
        "\n",
        "                top_k_evidence_indices = [evidence_idxs[idx].item() for idx in top_k_indices]\n",
        "                sorted_evidence_idxs.append(top_k_evidence_indices)\n",
        "\n",
        "            results.append(sorted_evidence_idxs)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Assume eval_loader is properly defined, as well as gru_model and device\n",
        "sorted_indices_per_claim = evaluate(gru_model, eval_loader, device, k=5)"
      ],
      "metadata": {
        "id": "-qtLB3KSlksb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LXQn60WB4-Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(predicted_indices, ground_truth_indices):\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1 = 0\n",
        "    num_claims = len(predicted_indices)\n",
        "\n",
        "    for predicted, true in zip(predicted_indices, ground_truth_indices):\n",
        "        true_set = set(true)\n",
        "        predicted_set = set(item for sublist in predicted for item in sublist)\n",
        "        #print(predicted_set)\n",
        "\n",
        "        # Calculate intersections for true positives\n",
        "        true_positives = len(predicted_set & true_set)\n",
        "\n",
        "        #print(true_set)\n",
        "\n",
        "        # Calculate precision and recall\n",
        "        if len(predicted_set) == 0:\n",
        "            precision = 0\n",
        "        else:\n",
        "            precision = true_positives / len(predicted_set)\n",
        "\n",
        "        if len(true_set) == 0:\n",
        "            recall = 0\n",
        "        else:\n",
        "            recall = true_positives / len(true_set)\n",
        "\n",
        "        # Calculate F1 score\n",
        "        if precision + recall == 0:\n",
        "            f1 = 0\n",
        "        else:\n",
        "            f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "        # Accumulate metrics to compute average later\n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "        total_f1 += f1\n",
        "\n",
        "    # Compute average metrics\n",
        "    average_precision = total_precision / num_claims\n",
        "    average_recall = total_recall / num_claims\n",
        "    average_f1 = total_f1 / num_claims\n",
        "\n",
        "    return average_precision, average_recall, average_f1\n",
        "\n",
        "# Usage\n",
        "average_precision, average_recall, average_f1 = calculate_metrics(sorted_indices_per_claim, dev_evidence_idxs)\n",
        "print(f\"Average Precision: {average_precision:.4f}\")\n",
        "print(f\"Average Recall: {average_recall:.4f}\")\n",
        "print(f\"Average F1 Score: {average_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3QuJ9Kt2jPs",
        "outputId": "f4c4ce59-15a3-4791-d4ef-acd7c83a3cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision: 0.0026\n",
            "Average Recall: 0.2000\n",
            "Average F1 Score: 0.0051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_evidence_idxs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T96uhGe2HLCc",
        "outputId": "d84a9725-35e0-42bd-ebf9-2dedf353fecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[996421, 1080858, 208053, 699212, 832334]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path\n",
        "save_path = '/content/drive/MyDrive/nlp/data/gru_model.bin'\n",
        "\n",
        "# Saving the model's state dictionary\n",
        "torch.save(gru_model.state_dict(), save_path)\n",
        "\n",
        "#model.save(gru_model.state_dict(), '/content/drive/MyDrive/nlp/data/gru_model.bin')"
      ],
      "metadata": {
        "id": "0lvDvHVK77Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path\n",
        "load_path = '/content/drive/MyDrive/nlp/data/gru_model.bin'\n",
        "\n",
        "# Loading the model's state dictionary\n",
        "gru_model = SiameseNetwork(embedding_matrix, hidden_dim=128)  # Re-create the model structure\n",
        "gru_model.load_state_dict(torch.load(load_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FBr3uuE_1li",
        "outputId": "03aa5757-2db0-42c1-c428-582ef966f3ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}