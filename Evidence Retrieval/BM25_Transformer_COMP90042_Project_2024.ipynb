{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPitXZzTsyTn",
        "outputId": "c560667f-0aa3-49dc-df7c-7f70daf796b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VcWPFMqtJ4x"
      },
      "outputs": [],
      "source": [
        "import torchdata.datapipes as dp\n",
        "import torchtext.transforms as T\n",
        "import spacy\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "eng = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohyt_MA86_7e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "evidences = pd.read_json('/content/drive/MyDrive/nlp/data/evidence.json', orient='index')\n",
        "train_claims = pd.read_json('/content/drive/MyDrive/nlp/data/train-claims.json', orient='index')\n",
        "dev_claims = pd.read_json('/content/drive/MyDrive/nlp/data/dev-claims.json', orient='index')\n",
        "#test_claims = pd.read_json('/content/drive/MyDrive/nlp/data/test-claims-unlabelled.json', orient='index')\n",
        "\n",
        "#update column names\n",
        "evidences.reset_index(inplace=True)\n",
        "evidences.columns = ['evidence_id', 'evidence_text']\n",
        "\n",
        "train_claims.reset_index(inplace=True)\n",
        "train_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "dev_claims.reset_index(inplace=True)\n",
        "dev_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "evidence_id = evidences['evidence_id']\n",
        "evidence_text = evidences['evidence_text']\n",
        "evidence_idx = evidences.index.tolist()\n",
        "\n",
        "evidence_id_dict = dict(zip(evidence_id, evidence_idx))\n",
        "\n",
        "train_claims_text = train_claims['claim_text']\n",
        "train_evidence_ids = train_claims['evidences']\n",
        "#map evidence_id to their corrosponding index for faster processing\n",
        "train_evidence_idxs = train_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])\n",
        "\n",
        "dev_claims_text = dev_claims['claim_text']\n",
        "dev_evidence_ids = dev_claims['evidences']\n",
        "dev_evidence_idxs = dev_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knrp2nykcB9_",
        "outputId": "bdb27875-297b-418a-e071-0c4e6634784a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "#text preprocessing\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "#lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# def preprocess_data(text):\n",
        "#     tokens = tt.tokenize(text)\n",
        "\n",
        "#     processed_tokens = []\n",
        "\n",
        "#     for token in tokens:\n",
        "#         token = token.lower()\n",
        "#         if token not in stopwords and token.isalpha():\n",
        "#             lemma = lemmatizer.lemmatize(token)\n",
        "#             processed_tokens.append(lemma)\n",
        "\n",
        "#     return processed_tokens\n",
        "\n",
        "def preprocess_data(text):\n",
        "    tokens = tt.tokenize(text)\n",
        "\n",
        "    processed_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        token = token.lower()\n",
        "        if token not in stopwords and token.isalpha():\n",
        "            stem = stemmer.stem(token)\n",
        "            processed_tokens.append(stem)\n",
        "\n",
        "    return processed_tokens\n",
        "\n",
        "train_claims_text_processed = train_claims_text.apply(preprocess_data)\n",
        "dev_claims_text_precessed = dev_claims_text.apply(preprocess_data)\n",
        "evidence_text_processed = evidence_text.apply(preprocess_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR4dnmajWB9L"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "train_claims_text_processed = json.load(open(\"/content/drive/MyDrive/nlp/data/train_claims_text_processed.json\", \"r\"))\n",
        "dev_claims_text_precessed = json.load(open(\"/content/drive/MyDrive/nlp/data/dev_claims_text_precessed.json\", \"r\"))\n",
        "evidence_text_processed = json.load(open(\"/content/drive/MyDrive/nlp/data/evidence_text_processed.json\", \"r\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGqSjE8Gqvlm"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "word2vec_model = Word2Vec.load('/content/drive/MyDrive/nlp/data/word2vec_model.bin')\n",
        "\n",
        "import numpy as np\n",
        "def generate_embedding(text, model):\n",
        "    # Filter out words that are not in the Word2Vec model's vocabulary\n",
        "    words = [word for word in text if word in model.wv.key_to_index]\n",
        "    if not words:  # Handle cases where none of the words are in the vocabulary\n",
        "        return np.zeros(model.vector_size)\n",
        "    # Get embeddings for each word in the text and average them\n",
        "    word_embeddings = [model.wv[word] for word in words]\n",
        "    sentence_embedding = np.mean(word_embeddings, axis=0)\n",
        "    return sentence_embedding\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Generate embeddings for train claim_text\n",
        "train_claim_text_embeddings = [generate_embedding(text, word2vec_model) for text in train_claims_text_processed]\n",
        "\n",
        "# Generate embeddings for dev claim_text\n",
        "dev_claim_text_embeddings = [generate_embedding(text, word2vec_model) for text in dev_claims_text_precessed]\n",
        "\n",
        "# Generate embeddings for all evidence texts\n",
        "evidence_embeddings = [generate_embedding(text, word2vec_model) for text in evidence_text_processed]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGu1i-R3q6HS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "3302561d-159c-444a-d10d-a07af8a03c08"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_claim_text_embeddings' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d4596e73dab8>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Compute cosine similarity scores for training claims and evidence embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_similarity_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_similarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_claim_text_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevidence_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdev_similarity_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_similarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_claim_text_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevidence_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_claim_text_embeddings' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to compute cosine similarity scores for all claims and evidence embeddings\n",
        "def compute_similarity_scores(claim_embeddings, evidence_embeddings):\n",
        "    similarity_scores = cosine_similarity(claim_embeddings, evidence_embeddings)\n",
        "    return similarity_scores\n",
        "\n",
        "\n",
        "# Compute cosine similarity scores for training claims and evidence embeddings\n",
        "train_similarity_scores = compute_similarity_scores(train_claim_text_embeddings, evidence_embeddings)\n",
        "dev_similarity_scores = compute_similarity_scores(dev_claim_text_embeddings, evidence_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_Q_4zSMrDvg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def get_top_k_indices(similarity_scores, k=1000):\n",
        "    # Args:\n",
        "    # similarity_scores: a PyTorch tensor of shape (num_claims, num_evidences)\n",
        "    # Returns:\n",
        "    # top_k_indices: a tensor of shape (num_claims, k) containing indices of the top k elements\n",
        "\n",
        "    # Convert numpy array to PyTorch tensor if needed\n",
        "    if isinstance(similarity_scores, np.ndarray):\n",
        "        similarity_scores_tensor = torch.FloatTensor(similarity_scores)\n",
        "    else:\n",
        "        similarity_scores_tensor = similarity_scores\n",
        "\n",
        "    # Get top k indices for each sample using PyTorch\n",
        "    top_k_values, top_k_indices = torch.topk(similarity_scores_tensor, k, dim=1, largest=True, sorted=True)\n",
        "\n",
        "    return top_k_indices,top_k_values\n",
        "\n",
        "# Compute top 1000 indices for training and development sets\n",
        "train_top_indices, _ = get_top_k_indices(train_similarity_scores, k=500)\n",
        "dev_top_indices, dev_orig_scores = get_top_k_indices(dev_similarity_scores, k=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHy3QRpa2o39"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, diags\n",
        "\n",
        "# Build inverted index\n",
        "def build_inverted_index(documents):\n",
        "    inverted_index = defaultdict(list)\n",
        "    for i, doc in enumerate(documents):\n",
        "        term_freq = defaultdict(int)\n",
        "        for term in doc:\n",
        "            term_freq[term] += 1\n",
        "        for term, freq in term_freq.items():\n",
        "            inverted_index[term].append((i, freq))\n",
        "    return inverted_index\n",
        "\n",
        "# Compute IDF values\n",
        "def compute_idf_values(inverted_index, total_documents):\n",
        "    idf_values = {}\n",
        "    for term, postings in inverted_index.items():\n",
        "        idf_values[term] = np.log((total_documents + 1) / (len(postings) + 1))\n",
        "    return idf_values\n",
        "\n",
        "# Calculate average document length\n",
        "def calculate_avg_doc_length(documents):\n",
        "    total_length = sum(len(doc) for doc in documents)\n",
        "    return total_length / len(documents)\n",
        "#k.1.0, b = 0.78 recall=0.14\n",
        "# Compute BM25 scores\n",
        "def bm25_scores(query, inverted_index, idf_values, avg_doc_length, k1=0.4, b=0.9):\n",
        "    scores = defaultdict(float)\n",
        "    for term in query:\n",
        "        if term not in inverted_index:\n",
        "            continue\n",
        "        doc_list = inverted_index[term]\n",
        "        idf = idf_values[term]\n",
        "        for doc_id, tf in doc_list:\n",
        "            # Compute BM25 score for this document\n",
        "            doc_length = len(evidence_text_processed[doc_id])\n",
        "            numerator = idf * tf * (k1 + 1)\n",
        "            denominator = tf + k1 * (1 - b + b * (doc_length / avg_doc_length))\n",
        "            scores[doc_id] += numerator / denominator\n",
        "    return scores\n",
        "\n",
        "# Build inverted index for evidence text\n",
        "inverted_index = build_inverted_index(evidence_text_processed)\n",
        "total_documents = len(evidence_text_processed)\n",
        "idf_values = compute_idf_values(inverted_index, total_documents)\n",
        "avg_doc_length = calculate_avg_doc_length(evidence_text_processed)\n",
        "\n",
        "# Example usage\n",
        "train_bm25_results = []\n",
        "for query in train_claims_text_processed:\n",
        "    scores = bm25_scores(query, inverted_index, idf_values, avg_doc_length)\n",
        "    train_bm25_results.append(scores)\n",
        "\n",
        "dev_bm25_results = []\n",
        "for query in dev_claims_text_precessed:\n",
        "    scores = bm25_scores(query, inverted_index, idf_values, avg_doc_length)\n",
        "    dev_bm25_results.append(scores)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvQlMBsR2toX"
      },
      "outputs": [],
      "source": [
        "train_reranked_indices = [[doc_id for doc_id, _ in sorted(scores.items(), key=lambda x: x[1], reverse=True)] for scores in train_bm25_results]\n",
        "train_reranked_scores = [[score for _, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)] for scores in train_bm25_results]\n",
        "\n",
        "dev_reranked_indices = [[doc_id for doc_id, _ in sorted(scores.items(), key=lambda x: x[1], reverse=True)] for scores in dev_bm25_results]\n",
        "dev_reranked_scores = [[score for _, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)] for scores in dev_bm25_results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km948Fbg2uXZ"
      },
      "outputs": [],
      "source": [
        "def topk_indices(indices, k=100):\n",
        "    return [indices[i][:min(k, len(indices[i]))] for i in range(len(indices))]\n",
        "\n",
        "train_top_indices = topk_indices(train_reranked_indices, k=50)\n",
        "dev_top_indices = topk_indices(dev_reranked_indices, k=50)\n",
        "dev_top_scores = topk_indices(dev_reranked_scores, k=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5kQvKZaUu3_",
        "outputId": "f9702c11-ecc7-4f33-b4ce-814f3823ada7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Recall: 0.8329004329004328\n"
          ]
        }
      ],
      "source": [
        "def calculate_average_recall(top_k_indices, true_indices):\n",
        "    \"\"\"\n",
        "    Calculates the average recall for each claim based on the overlap between the top-k indices and the true indices.\n",
        "\n",
        "    Args:\n",
        "    - top_k_indices (List[List[int]]): List of lists, where each sublist contains the top k indices for each claim.\n",
        "    - true_indices (List[List[int]]): List of lists, where each sublist contains the actual true indices for each claim.\n",
        "\n",
        "    Returns:\n",
        "    - float: Average recall across all claims.\n",
        "    \"\"\"\n",
        "    recall_values = []\n",
        "\n",
        "    # Iterate over each pair of top_k_indices and corresponding true indices\n",
        "    for top_indices, true_inds in zip(top_k_indices, true_indices):\n",
        "        # Calculate the number of true positives\n",
        "        true_positives = len(set(top_indices) & set(true_inds))\n",
        "\n",
        "        # Calculate recall for this claim\n",
        "        recall = true_positives / len(true_inds) if true_inds else 0  # Avoid division by zero\n",
        "\n",
        "        # Append the recall for this claim to the list\n",
        "        recall_values.append(recall)\n",
        "\n",
        "    # Compute average recall over all claims\n",
        "    avg_recall = sum(recall_values) / len(recall_values) if recall_values else 0  # Avoid division by zero if list is empty\n",
        "\n",
        "    return avg_recall\n",
        "\n",
        "avg_recall = calculate_average_recall(dev_top_indices, dev_evidence_idxs)\n",
        "print(\"Average Recall:\", avg_recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBxX0O5fYYZy"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, OrderedDict\n",
        "def build_vocab(texts, min_freq=3):\n",
        "    # Count all the words\n",
        "    word_freq = Counter()\n",
        "    for text in texts:\n",
        "        word_freq.update(text)\n",
        "\n",
        "    # Start vocab from special tokens\n",
        "    vocab = OrderedDict({\n",
        "        \"<pad>\": 0,\n",
        "        \"<unk>\": 1,\n",
        "        \"<sos>\": 2,\n",
        "        \"<eos>\": 3\n",
        "    })\n",
        "    index = 4  # Start indexing from 4 because 0-3 are reserved for special tokens\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:  # Only include words that meet the frequency threshold\n",
        "            vocab[word] = index\n",
        "            index += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# Build vocabulary using only evidence texts and applying the frequency threshold\n",
        "vocab = build_vocab(evidence_text_processed, min_freq=3)\n",
        "\n",
        "\n",
        "def numericalize(text, vocab):\n",
        "    # Convert text to numerical representation, adding start and end tokens, and padding\n",
        "    numericalized = [vocab[\"<sos>\"]] + [vocab.get(word, vocab[\"<unk>\"]) for word in text] + [vocab[\"<eos>\"]]\n",
        "    #padded_numericalized = numericalized[:max_length] + [0] * max(0, max_length - len(numericalized))\n",
        "    return numericalized\n",
        "\n",
        "train_claims_numerical = [numericalize(x, vocab) for x in train_claims_text_processed]\n",
        "dev_claims_numerical = [numericalize(x, vocab) for x in dev_claims_text_precessed]\n",
        "evidence_numerical = [numericalize(x, vocab) for x in evidence_text_processed]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOh6uKFEdM2P"
      },
      "outputs": [],
      "source": [
        "def build_vocab(texts, min_freq=3):\n",
        "    word_freq = Counter()\n",
        "    for text in texts:\n",
        "        word_freq.update(text)\n",
        "\n",
        "    vocab = OrderedDict({\n",
        "        \"<pad>\": 0,\n",
        "        \"<unk>\": 1,\n",
        "        \"<cls>\": 2,\n",
        "        \"<sep>\": 3# Adding CLS token\n",
        "    })\n",
        "    index = 4  # Start indexing from 3 because 0-2 are reserved for special tokens\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = index\n",
        "            index += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(evidence_text_processed, min_freq=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWTxOPaCaJ-S"
      },
      "source": [
        "## 1.1 Training DataLoading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDH_LBk8P9BJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class RankingDataset(Dataset):\n",
        "    def __init__(self, claims, evidences, true_indices, bm25_indices):\n",
        "        \"\"\"\n",
        "        Dataset for claim-evidence ranking with in-batch negatives handled during training.\n",
        "        :param claims: List of pre-padded and numericalized claims.\n",
        "        :param evidences: List of pre-padded and numericalized evidences.\n",
        "        :param true_indices: List of lists, each sublist contains indices of true evidences for the corresponding claim.\n",
        "        :param bm25_indices: List of lists, each sublist contains BM25-ranked evidence indices for the corresponding claim.\n",
        "        \"\"\"\n",
        "        self.claims = claims\n",
        "        self.evidences = evidences\n",
        "        self.true_indices = true_indices\n",
        "        self.bm25_indices = bm25_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.claims)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        claim = self.claims[idx]\n",
        "        #candidate_pos_indices = [i for i in self.true_indices[idx] if i in self.bm25_indices[idx]]\n",
        "\n",
        "        # if not candidate_pos_indices:\n",
        "        #     #print(f\"No true evidences found for claim {claim}\")\n",
        "        #     return None  # Skip if no true index is in the top BM25 results\n",
        "\n",
        "        candidate_pos_indices = self.true_indices[idx]\n",
        "        pos_idx = random.choice(candidate_pos_indices)  # Randomly choose one positive evidence from candidates\n",
        "        positive_evidence = self.evidences[pos_idx]\n",
        "\n",
        "        bm25_top_5 = [i for i in self.bm25_indices[idx] if i not in self.true_indices[idx]][:20]\n",
        "        if not bm25_top_5:\n",
        "            return None  # Skip if no BM25 negatives in the top 5\n",
        "\n",
        "        neg_idx = random.choice(bm25_top_5)  # Randomly choose one negative evidence from BM25 top 5\n",
        "        bm25_negative = self.evidences[neg_idx]\n",
        "\n",
        "        return claim, positive_evidence, bm25_negative\n",
        "\n",
        "\n",
        "\n",
        "def text_to_indices(text, vocab):\n",
        "    indices = [vocab[\"<sos>\"]] + [vocab.get(word, vocab[\"<unk>\"]) for word in text] + [vocab[\"<eos>\"]]\n",
        "    return indices\n",
        "\n",
        "def pad_sequences(sequences, pad_idx=0):\n",
        "    return pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in sequences], batch_first=True, padding_value=pad_idx)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Filter out None entries\n",
        "    batch = [item for item in batch if item is not None]\n",
        "\n",
        "    if len(batch) == 0:\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "    claims, positive_evidences, bm25_negatives = zip(*batch)\n",
        "\n",
        "    claims_indices = [text_to_indices(claim, vocab) for claim in claims]\n",
        "    positive_evidences_indices = [text_to_indices(evidence, vocab) for evidence in positive_evidences]\n",
        "    bm25_negatives_indices = [text_to_indices(evidence, vocab) for evidence in bm25_negatives]\n",
        "\n",
        "    claims_tensor = pad_sequences(claims_indices, pad_idx=vocab[\"<pad>\"])\n",
        "    positive_evidences_tensor = pad_sequences(positive_evidences_indices, pad_idx=vocab[\"<pad>\"])\n",
        "\n",
        "    batch_size = len(claims)\n",
        "    in_batch_positives = list(positive_evidences)\n",
        "    in_batch_negatives = list(bm25_negatives)\n",
        "\n",
        "    combined_negative_evidences = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # In-batch positives from other claims\n",
        "        in_batch_pos_negatives = [evidence for j, evidence in enumerate(in_batch_positives) if j != i]\n",
        "\n",
        "        # Randomly sample 7 gold negatives (positives of other claims)\n",
        "        gold_negatives = random.sample(in_batch_pos_negatives, min(31, len(in_batch_pos_negatives)))\n",
        "\n",
        "        # Randomly sample 7 BM25 negatives\n",
        "        bm25_negatives_sampled = random.sample(in_batch_negatives, min(32, len(in_batch_negatives)))\n",
        "\n",
        "        # Combine sampled gold negatives and BM25 negatives into a single list\n",
        "        combined_negatives = gold_negatives + bm25_negatives_sampled\n",
        "        combined_negative_texts = [text_to_indices(evidence, vocab) for evidence in combined_negatives]\n",
        "\n",
        "        combined_negatives_tensors = pad_sequences(combined_negative_texts, pad_idx=vocab[\"<pad>\"])\n",
        "        combined_negative_evidences.append(combined_negatives_tensors)\n",
        "\n",
        "    max_length_neg = max(neg.shape[1] for neg in combined_negative_evidences)\n",
        "    combined_negative_evidences_tensor = torch.stack([F.pad(neg, (0, max_length_neg - neg.shape[1]), value=vocab[\"<pad>\"]) for neg in combined_negative_evidences], dim=0)\n",
        "\n",
        "    return claims_tensor, positive_evidences_tensor, combined_negative_evidences_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3qVC0rJdeVq"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# def text_to_indices(text, vocab):\n",
        "#     indices = [vocab[\"<cls>\"]] + [vocab.get(word, vocab[\"<unk>\"]) for word in text]\n",
        "#     return indices\n",
        "\n",
        "def text_to_indices(text, vocab):\n",
        "    return [vocab.get(word, vocab[\"<unk>\"]) for word in text]\n",
        "class RankingDataset(Dataset):\n",
        "    def __init__(self, claims, evidences, true_indices, top_indices, vocab):\n",
        "        self.claims = claims\n",
        "        self.evidences = evidences\n",
        "        self.true_indices = true_indices\n",
        "        self.top_indices = top_indices\n",
        "        self.vocab = vocab\n",
        "        self.pairs = self.create_pairs()\n",
        "\n",
        "    def create_pairs(self):\n",
        "        pairs = []\n",
        "        for idx, claim in enumerate(self.claims):\n",
        "            candidate_pos_indices = self.true_indices[idx]\n",
        "            positive_evidences = [self.evidences[i] for i in candidate_pos_indices]\n",
        "\n",
        "            candidate_neg_indices = [i for i in self.top_indices[idx] if i not in candidate_pos_indices]\n",
        "            top_neg_evidences = [self.evidences[i] for i in candidate_neg_indices[:7]]\n",
        "\n",
        "            for pos_evidence in positive_evidences:\n",
        "                pairs.append((claim, pos_evidence, top_neg_evidences))\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        claim, pos_evidence, top_neg_evidences = self.pairs[idx]\n",
        "        claim_indices = text_to_indices(claim, self.vocab)\n",
        "        pos_evidence_indices = text_to_indices(pos_evidence, self.vocab)\n",
        "        top_neg_evidence_indices = [text_to_indices(neg, self.vocab) for neg in top_neg_evidences]\n",
        "\n",
        "        return claim_indices, pos_evidence_indices, top_neg_evidence_indices\n",
        "\n",
        "\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     batch = [item for item in batch if item is not None]\n",
        "\n",
        "#     if len(batch) == 0:\n",
        "#         return None, None\n",
        "\n",
        "#     sequences, labels = zip(*batch)\n",
        "\n",
        "#     sequences_tensor = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in sequences], batch_first=True, padding_value=vocab[\"<pad>\"]).to(device)\n",
        "#     labels_tensor = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "#     return sequences_tensor, labels_tensor\n",
        "def collate_fn(batch):\n",
        "    batch = [item for item in batch if item is not None]\n",
        "\n",
        "    if len(batch) == 0:\n",
        "        return None, None\n",
        "\n",
        "    claims, positive_evidences, top_negatives = zip(*batch)\n",
        "\n",
        "    # Initialize tensors\n",
        "    combined_sequences = []\n",
        "    combined_labels = []\n",
        "\n",
        "    batch_size = len(claims)\n",
        "\n",
        "    # Pad and add top negatives\n",
        "    for i in range(batch_size):\n",
        "        claim = claims[i]\n",
        "        pos_evidence = positive_evidences[i]\n",
        "        top_neg_evidences = top_negatives[i]\n",
        "\n",
        "        # Create positive pair\n",
        "        combined_sequences.append([vocab[\"<cls>\"]] + claim + [vocab[\"<sep>\"]] + pos_evidence + [vocab[\"<sep>\"]])\n",
        "        combined_labels.append(1)\n",
        "\n",
        "        # Add 7 top negatives\n",
        "        for neg_evidence in top_neg_evidences[:4]:\n",
        "            combined_sequences.append([vocab[\"<cls>\"]] + claim + [vocab[\"<sep>\"]] + neg_evidence + [vocab[\"<sep>\"]])\n",
        "            combined_labels.append(0)\n",
        "\n",
        "    # Add in-batch negatives\n",
        "    for i in range(batch_size):\n",
        "        for j in range(batch_size):\n",
        "            if i != j:\n",
        "                claim = claims[i]\n",
        "                pos_evidence = positive_evidences[j]\n",
        "\n",
        "                combined_sequences.append([vocab[\"<cls>\"]] + claim + [vocab[\"<sep>\"]] + pos_evidence + [vocab[\"<sep>\"]])\n",
        "                combined_labels.append(0)\n",
        "\n",
        "                if len(combined_labels) % 4 == 0:\n",
        "                    break\n",
        "\n",
        "    # Convert to tensors and pad\n",
        "    sequences_tensor = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in combined_sequences], batch_first=True, padding_value=vocab[\"<pad>\"]).to(device)\n",
        "    labels_tensor = torch.tensor(combined_labels, dtype=torch.float).to(device)\n",
        "\n",
        "    return sequences_tensor, labels_tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSh7G5nWa6fC"
      },
      "source": [
        "# 2.1 Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CccNJ3Vpqwvv"
      },
      "outputs": [],
      "source": [
        "def listwise_loss(model, claims, pos_evidences, neg_evidences, pos_weight=2.0):\n",
        "    batch_size = claims.size(0)\n",
        "    num_negatives = neg_evidences.shape[1]\n",
        "    # Get the scores for positive evidence\n",
        "    pos_scores = model(claims, pos_evidences).unsqueeze(1)\n",
        "    # Get the scores for negative evidence\n",
        "    neg_scores_list = [model(claims, neg_evidences[:, i, :]).unsqueeze(1) for i in range(neg_evidences.shape[1])]\n",
        "    neg_scores = torch.cat(neg_scores_list, dim=1)\n",
        "\n",
        "    # Concatenate the positive and negative scores\n",
        "    scores = torch.cat((pos_scores, neg_scores), dim=1)\n",
        "\n",
        "    # Average the scores across the embedding dimension\n",
        "    scores = scores.mean(dim=2)\n",
        "    scores = F.log_softmax(scores, dim=1)\n",
        "    target = torch.zeros(batch_size, dtype=torch.long, device=scores.device)\n",
        "\n",
        "    # Create weight tensor\n",
        "    weight = torch.ones(num_negatives + 1, device=scores.device)\n",
        "    weight[0] = pos_weight\n",
        "\n",
        "    return F.nll_loss(scores, target, weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko471OBfjxfE"
      },
      "outputs": [],
      "source": [
        "def margin_ranking_loss(model, claims, pos_evidences, neg_evidences, margin=1.5):\n",
        "    batch_size = claims.size(0)\n",
        "\n",
        "    # Get the scores for positive evidence\n",
        "    pos_scores = model(claims, pos_evidences).unsqueeze(1)\n",
        "\n",
        "    # Get the scores for negative evidence\n",
        "    neg_scores_list = [model(claims, neg_evidences[:, i, :]).unsqueeze(1) for i in range(neg_evidences.shape[1])]\n",
        "    neg_scores = torch.cat(neg_scores_list, dim=1)\n",
        "\n",
        "    # Calculate the margin ranking loss\n",
        "    target = torch.ones_like(neg_scores, device=claims.device)\n",
        "    pos_scores = pos_scores.expand_as(neg_scores)  # Expand pos_scores to match neg_scores shape\n",
        "    loss = F.margin_ranking_loss(pos_scores, neg_scores, target, margin=margin)\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amWJJOwpa98W"
      },
      "source": [
        "# 2.2.1 Ranking Models-Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wun-8zoqsXW"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.optim as optim\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(0)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: torch.Tensor):\n",
        "        token_embedding = token_embedding + self.pos_embedding[:, :token_embedding.size(1), :]\n",
        "        return self.dropout(token_embedding)\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor):\n",
        "        embeddings = self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "        return embeddings\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, emb_size: int, nhead: int, dim_feedforward: int, dropout: float):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(emb_size, nhead, dropout=dropout, batch_first=True)\n",
        "        self.linear1 = nn.Linear(emb_size, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, emb_size)\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, src_key_padding_mask: torch.Tensor):\n",
        "        src2, _ = self.self_attn(src, src, src, key_padding_mask=src_key_padding_mask)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(F.gelu(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int, nhead: int = 4, num_encoder_layers: int = 3, dim_feedforward: int = 512, dropout: float = 0.5):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, emb_size)\n",
        "        self.pos_encoder = PositionalEncoding(emb_size, dropout)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(emb_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, src_key_padding_mask: torch.Tensor):\n",
        "        src = self.token_embedding(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_key_padding_mask)\n",
        "        src = self.layer_norm(src)\n",
        "        return src\n",
        "\n",
        "class SiameseTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int):\n",
        "        super(SiameseTransformer, self).__init__()\n",
        "        self.encoder = TransformerEncoder(vocab_size, emb_size)\n",
        "\n",
        "    def forward(self, claims: torch.Tensor, evidences: torch.Tensor):\n",
        "        # Create mask on the fly\n",
        "        claims_mask = (claims == 0)  # Mask positions where index is 0 (padding)\n",
        "        evidences_mask = (evidences == 0)  # Mask positions where index is 0 (padding)\n",
        "\n",
        "        claims_enc = self.encoder(claims, claims_mask)  # Transformer expects key_padding_mask with 1s at padded positions\n",
        "        evidences_enc = self.encoder(evidences, evidences_mask)  # Transformer expects key_padding_mask with 1s at padded positions\n",
        "\n",
        "        claims_enc = torch.mean(claims_enc, dim=1)  # Mean pooling over the sequence length\n",
        "        evidences_enc = torch.mean(evidences_enc, dim=1)  # Mean pooling over the sequence length\n",
        "\n",
        "        scores_dot = torch.bmm(claims_enc.unsqueeze(1), evidences_enc.unsqueeze(2)).squeeze()  # Dot product\n",
        "        scores_l2 = -torch.cdist(claims_enc, evidences_enc, p=2)  # Negative L2 distance\n",
        "        return scores_dot + scores_l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d08A0h1uFaV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(0)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: torch.Tensor):\n",
        "        token_embedding = token_embedding + self.pos_embedding[:, :token_embedding.size(1), :]\n",
        "        return self.dropout(token_embedding)\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor):\n",
        "        embeddings = self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "        return embeddings\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, nhead=8, num_encoder_layers=3, dim_feedforward=512, dropout=0.5):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, embed_dim)\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_encoder_layers)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, nhead, dropout=dropout, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(embed_dim, 1)\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask):\n",
        "        src = self.token_embedding(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        src = self.layer_norm(src)\n",
        "        output = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attn_output, _ = self.attention(output, output, output, key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # Compute attention weights\n",
        "        attn_weights = torch.softmax(self.fc(attn_output), dim=1)\n",
        "\n",
        "        # Compute context vector as a weighted sum of the encoder outputs\n",
        "        context_vector = torch.sum(attn_weights * attn_output, dim=1)\n",
        "\n",
        "        return context_vector\n",
        "\n",
        "class SiameseTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int):\n",
        "        super(SiameseTransformer, self).__init__()\n",
        "        self.encoder = TransformerEncoder(vocab_size, emb_size)\n",
        "\n",
        "    def forward(self, claims: torch.Tensor, evidences: torch.Tensor):\n",
        "        # Create mask on the fly\n",
        "        claims_mask = (claims == 0)  # Mask positions where index is 0 (padding)\n",
        "        evidences_mask = (evidences == 0)  # Mask positions where index is 0 (padding)\n",
        "\n",
        "        claims_enc = self.encoder(claims, claims_mask)  # Transformer expects key_padding_mask with 1s at padded positions\n",
        "        evidences_enc = self.encoder(evidences, evidences_mask)  # Transformer expects key_padding_mask with 1s at padded positions\n",
        "\n",
        "        # Ensure the encodings have the correct dimensions for the operations\n",
        "        claims_enc = claims_enc.unsqueeze(1)  # Shape: (batch_size, 1, embed_dim)\n",
        "        evidences_enc = evidences_enc.unsqueeze(2)  # Shape: (batch_size, embed_dim, 1)\n",
        "\n",
        "        # Compute dot product and L2 distance\n",
        "        scores_dot = torch.bmm(claims_enc, evidences_enc).squeeze()  # Dot product\n",
        "        scores_l2 = -torch.cdist(claims_enc.squeeze(1), evidences_enc.squeeze(2), p=2)  # Negative L2 distance\n",
        "\n",
        "\n",
        "        return scores_dot + scores_l2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ihp1jADecJX"
      },
      "source": [
        "# Binary Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uym1Xp13ee6Q"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(0)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: torch.Tensor):\n",
        "        token_embedding = token_embedding + self.pos_embedding[:, :token_embedding.size(1), :]\n",
        "        return self.dropout(token_embedding)\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor):\n",
        "        embeddings = self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "        return embeddings\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, nhead=8, num_encoder_layers=2, dim_feedforward=1024, dropout=0.3):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, embed_dim)\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_encoder_layers)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask):\n",
        "        src = self.token_embedding(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        src = self.layer_norm(src)\n",
        "        output = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
        "        return output\n",
        "\n",
        "class SiameseTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int):\n",
        "        super(SiameseTransformer, self).__init__()\n",
        "        self.encoder = TransformerEncoder(vocab_size, emb_size)\n",
        "        self.fc = nn.Linear(emb_size, 1)  # Binary classification output\n",
        "\n",
        "    def forward(self, sequences: torch.Tensor):\n",
        "        # Create mask on the fly\n",
        "        mask = (sequences == 0)  # Mask positions where index is 0 (padding)\n",
        "\n",
        "        encodings = self.encoder(sequences, mask)  # Transformer expects key_padding_mask with 1s at padded positions\n",
        "\n",
        "        # Extract the [CLS] token representation (first token)\n",
        "        cls_encoding = encodings[:, 0, :]  # Take the [CLS] token representation\n",
        "\n",
        "        scores = self.fc(cls_encoding)  # Apply the linear layer for binary classification\n",
        "        return scores.squeeze() # Squeeze the output to get a 1D tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Obt77tfBeR"
      },
      "source": [
        "# 2.2.2 GRU+Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xtZHRSJfHO2"
      },
      "outputs": [],
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_dim):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.gru = nn.GRU(embed_size, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def attention_net(self, gru_output, mask):\n",
        "        attn_weights = self.attention(gru_output)  # (batch_size, seq_len, 1)\n",
        "        attn_weights = attn_weights.squeeze(2)  # (batch_size, seq_len)\n",
        "\n",
        "        # Apply mask - set attention weights to a large negative value (-inf) where mask is 0\n",
        "        attn_weights.masked_fill_(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_weights = torch.softmax(attn_weights, dim=1)  # (batch_size, seq_len)\n",
        "        attn_weights = attn_weights.unsqueeze(2)  # (batch_size, seq_len, 1)\n",
        "\n",
        "        context = torch.bmm(attn_weights.transpose(1, 2), gru_output)  # (batch_size, 1, hidden_dim * 2)\n",
        "        context = context.squeeze(1)  # (batch_size, hidden_dim * 2)\n",
        "        return context\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x_emb = self.embedding(x)\n",
        "        gru_output, hidden = self.gru(x_emb)\n",
        "\n",
        "        mask = (x != 0).float()  # (batch_size, seq_len)\n",
        "\n",
        "        attn_output = self.attention_net(gru_output, mask)  # (batch_size, hidden_dim * 2)\n",
        "        output = F.relu(self.fc(attn_output))  # (batch_size, hidden_dim)\n",
        "        return output\n",
        "\n",
        "    def forward(self, claims, evidences):\n",
        "        claims_enc = self.forward_one(claims)\n",
        "        evidences_enc = self.forward_one(evidences)\n",
        "\n",
        "        scores_dot = torch.bmm(claims_enc.unsqueeze(1), evidences_enc.unsqueeze(2)).squeeze()  # Dot product\n",
        "        scores_l2 = -torch.cdist(claims_enc, evidences_enc, p=2)  # Negative L2 distance\n",
        "        return scores_dot + scores_l2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQUFTDuebl3T"
      },
      "source": [
        "# 2.3 Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfLJyA5wbo1F",
        "outputId": "4809e35a-c08b-4027-a061-834862eaa4ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Average Loss: 0.8608104250369928\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 1 Evaluation - Recall: 0.131060606060606, Precision: 0.07142857142857141, F1 Score: 0.08640486497629353\n",
            "Epoch 2/30, Average Loss: 0.4559433765900441\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 2 Evaluation - Recall: 0.13755411255411254, Precision: 0.07012987012987013, F1 Score: 0.08547722119150691\n",
            "Epoch 3/30, Average Loss: 0.31614012404894215\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 3 Evaluation - Recall: 0.1362554112554112, Precision: 0.07012987012987013, F1 Score: 0.08520923520923518\n",
            "Epoch 4/30, Average Loss: 0.2520669845816417\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 4 Evaluation - Recall: 0.11493506493506486, Precision: 0.06493506493506494, F1 Score: 0.07641207998350856\n",
            "Epoch 5/30, Average Loss: 0.18203756385124648\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 5 Evaluation - Recall: 0.14588744588744582, Precision: 0.07402597402597398, F1 Score: 0.09015151515151512\n",
            "Epoch 6/30, Average Loss: 0.14629715021986228\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 6 Evaluation - Recall: 0.1616883116883116, Precision: 0.08311688311688306, F1 Score: 0.10110801896516182\n",
            "Epoch 7/30, Average Loss: 0.1327503720919291\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 7 Evaluation - Recall: 0.1688311688311688, Precision: 0.08311688311688306, F1 Score: 0.10278808493094206\n",
            "Epoch 8/30, Average Loss: 0.11570663492266949\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 8 Evaluation - Recall: 0.15303030303030302, Precision: 0.07792207792207789, F1 Score: 0.0959544423830138\n",
            "Epoch 9/30, Average Loss: 0.12628627320130667\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 9 Evaluation - Recall: 0.14177489177489172, Precision: 0.06883116883116881, F1 Score: 0.0853174603174603\n",
            "Epoch 10/30, Average Loss: 0.09745400035992646\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 10 Evaluation - Recall: 0.14729437229437223, Precision: 0.07662337662337658, F1 Score: 0.09374871160585445\n",
            "Epoch 11/30, Average Loss: 0.07095630495594098\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 11 Evaluation - Recall: 0.13658008658008652, Precision: 0.0701298701298701, F1 Score: 0.08599773242630386\n",
            "Epoch 12/30, Average Loss: 0.06834047039349873\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 12 Evaluation - Recall: 0.14448051948051943, Precision: 0.07142857142857138, F1 Score: 0.08853844568130281\n",
            "Epoch 13/30, Average Loss: 0.06373158855459247\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 13 Evaluation - Recall: 0.13917748917748912, Precision: 0.06623376623376621, F1 Score: 0.08272005772005772\n",
            "Epoch 14/30, Average Loss: 0.05450894514051003\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 14 Evaluation - Recall: 0.14491341991341986, Precision: 0.074025974025974, F1 Score: 0.0906720263863121\n",
            "Epoch 15/30, Average Loss: 0.05246225056739954\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 15 Evaluation - Recall: 0.1395021645021645, Precision: 0.0701298701298701, F1 Score: 0.0861626468769326\n",
            "Epoch 16/30, Average Loss: 0.04077881741791199\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 16 Evaluation - Recall: 0.1481601731601731, Precision: 0.07662337662337658, F1 Score: 0.09407338693052976\n",
            "Epoch 17/30, Average Loss: 0.03557332304234688\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 17 Evaluation - Recall: 0.14902597402597398, Precision: 0.07922077922077916, F1 Score: 0.09563492063492063\n",
            "Epoch 18/30, Average Loss: 0.03807585672117197\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 18 Evaluation - Recall: 0.1492424242424242, Precision: 0.08181818181818178, F1 Score: 0.09824778396206967\n",
            "Epoch 19/30, Average Loss: 0.027286722515829098\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 19 Evaluation - Recall: 0.15378787878787878, Precision: 0.08051948051948049, F1 Score: 0.09715007215007214\n",
            "Epoch 20/30, Average Loss: 0.028838830313478142\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 20 Evaluation - Recall: 0.15530303030303028, Precision: 0.08311688311688306, F1 Score: 0.10098433312719027\n",
            "Epoch 21/30, Average Loss: 0.02532981586857484\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 21 Evaluation - Recall: 0.14080086580086582, Precision: 0.0805194805194805, F1 Score: 0.09552669552669553\n",
            "Epoch 22/30, Average Loss: 0.019283406770764254\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 22 Evaluation - Recall: 0.14837662337662333, Precision: 0.08181818181818176, F1 Score: 0.09792310863739434\n",
            "Epoch 23/30, Average Loss: 0.015196988753114756\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 23 Evaluation - Recall: 0.1489177489177489, Precision: 0.08051948051948048, F1 Score: 0.0964285714285714\n",
            "Epoch 24/30, Average Loss: 0.017768226951981585\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 24 Evaluation - Recall: 0.14296536796536796, Precision: 0.07922077922077919, F1 Score: 0.09444444444444444\n",
            "Epoch 25/30, Average Loss: 0.016063369273279723\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 25 Evaluation - Recall: 0.14253246753246748, Precision: 0.07532467532467528, F1 Score: 0.09076479076479076\n",
            "Epoch 26/30, Average Loss: 0.01250014285174891\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 26 Evaluation - Recall: 0.14307359307359307, Precision: 0.0753246753246753, F1 Score: 0.09094516594516595\n",
            "Epoch 27/30, Average Loss: 0.012745517350804921\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 27 Evaluation - Recall: 0.15238095238095234, Precision: 0.08311688311688306, F1 Score: 0.09935064935064933\n",
            "Epoch 28/30, Average Loss: 0.010254152411690507\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 28 Evaluation - Recall: 0.1479437229437229, Precision: 0.07922077922077916, F1 Score: 0.09527417027417025\n",
            "Epoch 29/30, Average Loss: 0.009137282750975246\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 29 Evaluation - Recall: 0.14502164502164497, Precision: 0.07662337662337658, F1 Score: 0.09253246753246752\n",
            "Epoch 30/30, Average Loss: 0.007881895809147794\n",
            "Current Learning Rate: 0.001\n",
            "Epoch 30 Evaluation - Recall: 0.14686147186147183, Precision: 0.07792207792207789, F1 Score: 0.094011544011544\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 300\n",
        "hidden_dim = 512\n",
        "dataset = RankingDataset(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "#model = SiameseTransformer(vocab_size, embed_dim)\n",
        "model_path = '/content/drive/MyDrive/nlp/data/best_model.pth'\n",
        "model = SiameseNetwork(vocab_size, embed_dim, hidden_dim)\n",
        "model = model.to(device)\n",
        "criterion = margin_ranking_loss#listwise_loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "num_epochs = 30\n",
        "\n",
        "def evaluate_model(model, claims, evidence_texts, top_k, vocab, pad_idx):\n",
        "    dev_top_indices = topk_indices(dev_reranked_indices, k=top_k)\n",
        "    #dev_top_scores = topk_indices(dev_reranked_scores, k=top_k)\n",
        "\n",
        "    dev_scores = []\n",
        "    for idx in range(len(claims)):\n",
        "        top_k_evidence_idxs = dev_top_indices[idx]\n",
        "        top_k_evidences = [evidence_texts[i] for i in top_k_evidence_idxs]\n",
        "        scores = score_query(model, claims[idx], top_k_evidences, vocab, pad_idx)\n",
        "        dev_scores.append(scores)\n",
        "\n",
        "    reranked_indices = []\n",
        "    for indices, scores in zip(dev_top_indices, dev_scores):\n",
        "        indexed_scores = list(zip(indices, scores))\n",
        "        sorted_by_score = sorted(indexed_scores, key=lambda x: x[1], reverse=True)\n",
        "        sorted_indices = [idx for idx, _ in sorted_by_score]\n",
        "        reranked_indices.append(sorted_indices)\n",
        "\n",
        "    return reranked_indices\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    best_f1 = 0.0\n",
        "    for batch in dataloader:\n",
        "        if batch is None:\n",
        "            continue\n",
        "\n",
        "        claim, pos_evidences, neg_evidences = batch\n",
        "        claim = claim.to(device)\n",
        "        pos_evidences = pos_evidences.to(device)\n",
        "        neg_evidences = neg_evidences.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model, claim, pos_evidences, neg_evidences)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss}')\n",
        "\n",
        "    scheduler.step(avg_epoch_loss)\n",
        "    current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "    print(f\"Current Learning Rate: {current_lr}\")\n",
        "\n",
        "    # Evaluation at the end of each epoch\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        reranked_indices = evaluate_model(model, dev_claims_text_precessed, evidence_text_processed, top_k=15, vocab=vocab, pad_idx=vocab['<pad>'])\n",
        "        results = evaluate_evidence_retrieval(reranked_indices, dev_evidence_idxs, k=5)\n",
        "        f1 = results['average_fscore']\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        print(f\"Epoch {epoch+1} Evaluation - Recall: {results['average_recall']}, Precision: {results['average_precision']}, F1 Score: {results['average_fscore']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "yVY23evyehTz",
        "outputId": "5b8aaa14-27ec-4e8d-a8d7-0cde81484c1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 0.3572\n",
            "Epoch 2/30, Loss: 0.3489\n",
            "Epoch 3/30, Loss: 0.3484\n",
            "Epoch 4/30, Loss: 0.3483\n",
            "Epoch 5/30, Loss: 0.3481\n",
            "Epoch 6/30, Loss: 0.3478\n",
            "Epoch 7/30, Loss: 0.3480\n",
            "Epoch 8/30, Loss: 0.3480\n",
            "Epoch 9/30, Loss: 0.3479\n",
            "Epoch 10/30, Loss: 0.3473\n",
            "Epoch 11/30, Loss: 0.3469\n",
            "Epoch 12/30, Loss: 0.3458\n",
            "Epoch 13/30, Loss: 0.3463\n",
            "Epoch 14/30, Loss: 0.3458\n",
            "Epoch 15/30, Loss: 0.3465\n",
            "Epoch 16/30, Loss: 0.3457\n",
            "Epoch 17/30, Loss: 0.3463\n",
            "Epoch 18/30, Loss: 0.3456\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-859061a59525>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-859061a59525>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, scheduler, epochs, clip_value)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msequences_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_classification_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mdynamo_config_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0mper_device_and_dtype_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zero_grad_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         self.record = torch.ops.profiler._record_function_enter_new(\n\u001b[0m\u001b[1;32m    606\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import torch.nn.utils\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "def binary_classification_loss(model, sequences, labels):\n",
        "    outputs = model(sequences)\n",
        "    loss = F.binary_cross_entropy_with_logits(outputs, labels)\n",
        "    return loss\n",
        "\n",
        "def train(model, dataloader, optimizer, scheduler, epochs=10, clip_value=1.0):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for sequences_tensor, labels_tensor in dataloader:\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Compute the loss\n",
        "            loss = binary_classification_loss(model, sequences_tensor, labels_tensor)\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "            # Accumulate the loss for this epoch\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Adjust the learning rate based on the epoch loss\n",
        "        scheduler.step(epoch_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")\n",
        "\n",
        "# Initialize the dataset and dataloader\n",
        "dataset = RankingDataset(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices, vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
        "\n",
        "# Initialize the model, optimizer, and training\n",
        "model = SiameseTransformer(len(vocab), emb_size=320).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize the learning rate scheduler\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "# Train the model\n",
        "train(model, dataloader, optimizer, scheduler, epochs=30)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBykLPZoSxYX",
        "outputId": "23cc655d-40dd-4199-eb75-2620dd9df02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r94hwnTKFfSJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "61d80fbc-3836-4e38-9805-77a487930862"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-4c1aaf030404>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtop_k_evidence_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_top_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtop_k_evidences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mevidence_text_processed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_k_evidence_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_claims_text_precessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k_evidences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mdev_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-4c1aaf030404>\u001b[0m in \u001b[0;36mscore_query\u001b[0;34m(model, query, evidences, vocab, pad_idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mevidence_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevidence_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m#evidence_mask = evidence_masks[i].unsqueeze(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevidence_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# query_mask.unsqueeze(0), evidence_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-39968871f44a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, claims, evidences)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mclaims_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclaims_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Transformer expects key_padding_mask with 1s at padded positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mevidences_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevidences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevidences_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Transformer expects key_padding_mask with 1s at padded positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Ensure the encodings have the correct dimensions for the operations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-39968871f44a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_key_padding_mask)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Apply attention mechanism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask_for_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_nested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1678\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1679\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_buffers'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dev_top_indices = topk_indices(dev_reranked_indices, k=10)\n",
        "dev_top_scores = topk_indices(dev_reranked_scores, k=10)\n",
        "#dev_top_indices, dev_orig_scores = get_top_k_indices(dev_similarity_scores, k=50)\n",
        "def text_to_indices(text, vocab):\n",
        "    indices = [vocab[\"<sos>\"]] + [vocab.get(word, vocab[\"<unk>\"]) for word in text] + [vocab[\"<eos>\"]]\n",
        "    return indices\n",
        "\n",
        "\n",
        "def create_mask(tensor, pad_idx=vocab['<pad>']):\n",
        "    return (tensor == pad_idx).to(tensor.device)\n",
        "def score_query(model, query, evidences, vocab, pad_idx):\n",
        "    query_indices = text_to_indices(query, vocab)\n",
        "    evidence_indices = [text_to_indices(evidence, vocab) for evidence in evidences]\n",
        "\n",
        "    query_tensor = pad_sequence([torch.tensor(query_indices)], batch_first=True, padding_value=pad_idx)\n",
        "    evidence_tensors = pad_sequence([torch.tensor(ei) for ei in evidence_indices], batch_first=True, padding_value=pad_idx)\n",
        "\n",
        "    # query_mask = create_mask(query_tensor, pad_idx).squeeze(0)\n",
        "    # evidence_masks = create_mask(evidence_tensors, pad_idx)\n",
        "\n",
        "    query_tensor = query_tensor.to(device)\n",
        "    evidence_tensors = evidence_tensors.to(device)\n",
        "    # query_mask = query_mask.to(device)\n",
        "    # evidence_masks = evidence_masks.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(evidence_tensors.shape[0]):\n",
        "            evidence_tensor = evidence_tensors[i].unsqueeze(0)\n",
        "            #evidence_mask = evidence_masks[i].unsqueeze(0)\n",
        "            score = model(query_tensor, evidence_tensor)# query_mask.unsqueeze(0), evidence_mask\n",
        "            scores.append(score.item())\n",
        "\n",
        "    return scores\n",
        "\n",
        "dev_scores = []\n",
        "for idx in range(len(dev_claims_text_precessed)):\n",
        "    top_k_evidence_idxs = dev_top_indices[idx]\n",
        "    top_k_evidences = [evidence_text_processed[i] for i in top_k_evidence_idxs]\n",
        "    scores = score_query(model, dev_claims_text_precessed[idx], top_k_evidences, vocab, vocab['<pad>'])\n",
        "    dev_scores.append(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHYYl-2VoAMQ"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# def text_to_indices(text, vocab):\n",
        "#     indices = [vocab[\"<cls>\"]] + [vocab.get(word, vocab[\"<unk>\"]) for word in text]\n",
        "#     return indices\n",
        "dev_top_indices = topk_indices(dev_reranked_indices, k=20)\n",
        "dev_top_scores = topk_indices(dev_reranked_scores, k=20)\n",
        "\n",
        "def score_query(model, query, evidences, vocab, pad_idx):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    query_indices = text_to_indices(query, vocab)\n",
        "    probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for evidence in evidences:\n",
        "            evidence_indices = text_to_indices(evidence, vocab)\n",
        "            combined_indices = [vocab[\"<cls>\"]] + query_indices + [vocab[\"<sep>\"]] + evidence_indices + [vocab[\"<sep>\"]]\n",
        "\n",
        "            combined_tensor = pad_sequence([torch.tensor(combined_indices, dtype=torch.long)], batch_first=True, padding_value=pad_idx)\n",
        "            combined_tensor = combined_tensor.to(device)\n",
        "\n",
        "            logits = model(combined_tensor)\n",
        "            prob = torch.sigmoid(logits).item()  # Apply sigmoid to get probability\n",
        "            probabilities.append(prob)\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "# Example usage for evaluation\n",
        "dev_scores = []\n",
        "for idx in range(len(dev_claims_text_precessed)):\n",
        "    top_k_evidence_idxs = dev_top_indices[idx]\n",
        "    top_k_evidences = [evidence_text_processed[i] for i in top_k_evidence_idxs]\n",
        "    scores = score_query(model, dev_claims_text_precessed[idx], top_k_evidences, vocab, vocab['<pad>'])\n",
        "    dev_scores.append(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMkWU06-svRR",
        "outputId": "d916254a-e333-4548-f1b2-d9de175bbc2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.10233554989099503,\n",
              " 0.059117142111063004,\n",
              " 0.06067594140768051,\n",
              " 0.08072149008512497,\n",
              " 0.0576542466878891,\n",
              " 0.09556568413972855,\n",
              " 0.08637840300798416,\n",
              " 0.06461432576179504,\n",
              " 0.06730613112449646,\n",
              " 0.05402364209294319,\n",
              " 0.06346571445465088,\n",
              " 0.06596881151199341,\n",
              " 0.08334793895483017,\n",
              " 0.06876970082521439,\n",
              " 0.11062392592430115,\n",
              " 0.05371873825788498,\n",
              " 0.05371873825788498,\n",
              " 0.06277293711900711,\n",
              " 0.0736880674958229,\n",
              " 0.06580596417188644,\n",
              " 0.07230368256568909,\n",
              " 0.06807564944028854,\n",
              " 0.05645347386598587,\n",
              " 0.06784305721521378,\n",
              " 0.05965680629014969,\n",
              " 0.08406560868024826,\n",
              " 0.07724510878324509,\n",
              " 0.056744612753391266,\n",
              " 0.05356026440858841,\n",
              " 0.05608903616666794,\n",
              " 0.05610428750514984,\n",
              " 0.05978047847747803,\n",
              " 0.05610302463173866,\n",
              " 0.07313989102840424,\n",
              " 0.062173787504434586,\n",
              " 0.058304812759160995,\n",
              " 0.054597407579422,\n",
              " 0.062362659722566605,\n",
              " 0.08891953527927399,\n",
              " 0.062082018703222275,\n",
              " 0.06124025583267212,\n",
              " 0.1053486317396164,\n",
              " 0.06375909596681595,\n",
              " 0.0992824137210846,\n",
              " 0.06311468034982681,\n",
              " 0.08286812901496887,\n",
              " 0.08459626883268356,\n",
              " 0.06282968819141388,\n",
              " 0.0960589051246643,\n",
              " 0.07250665873289108]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "dev_scores[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOPiXQIIG432"
      },
      "outputs": [],
      "source": [
        "def aggregate_scores(original_scores, new_scores, lam=0.7):\n",
        "    \"\"\"\n",
        "    Aggregate scores from two different sources using a weighted sum approach.\n",
        "\n",
        "    Parameters:\n",
        "    - original_scores: List of lists containing scores from the original model.\n",
        "    - new_scores: List of lists containing scores from the new model.\n",
        "    - lam: Weighting factor for new_scores; 1 - lam will be the weight for the original_scores.\n",
        "\n",
        "    Returns:\n",
        "    - List of lists containing aggregated scores.\n",
        "    \"\"\"\n",
        "    aggregated_scores = []\n",
        "    for original, new in zip(original_scores, new_scores):\n",
        "        # Calculate the weighted sum of scores\n",
        "        aggregated = [(1 - lam) * o + lam * n for o, n in zip(original, new)]\n",
        "        aggregated_scores.append(aggregated)\n",
        "    return aggregated_scores\n",
        "\n",
        "scores = aggregate_scores(dev_top_scores, dev_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUUrEG0xHOZU"
      },
      "outputs": [],
      "source": [
        "reranked_indices = []\n",
        "\n",
        "for indices, scores in zip(dev_top_indices, dev_scores):\n",
        "    # Combine indices and scores into a list of tuples\n",
        "    indexed_scores = list(zip(indices, scores))\n",
        "\n",
        "    # Sort the list of tuples by the score in descending order\n",
        "    sorted_by_score = sorted(indexed_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Extract the sorted indices\n",
        "    sorted_indices = [idx for idx, _ in sorted_by_score]\n",
        "\n",
        "    # Add to the final list\n",
        "    reranked_indices.append(sorted_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwjww5C-HWB6",
        "outputId": "1e62e67a-baad-4799-da90-c3997076ce95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'average_recall': 0.1841991341991341, 'average_precision': 0.10129870129870122, 'average_fscore': 0.12142857142857143}\n"
          ]
        }
      ],
      "source": [
        "def evaluate_evidence_retrieval(predicted_indices_list, actual_indices_list, k=5):\n",
        "    #assert len(predicted_indices_list) == len(actual_indices_list), \"Both inputs must have the same length.\"\n",
        "\n",
        "    total_recall = 0.0\n",
        "    total_precision = 0.0\n",
        "    total_fscore = 0.0\n",
        "    num_claims = len(predicted_indices_list)\n",
        "\n",
        "    for predicted_indices, actual_indices in zip(predicted_indices_list, actual_indices_list):\n",
        "        # Convert tensors in predicted_indices to integers if they are not already\n",
        "        predicted_indices = [index.item() if isinstance(index, torch.Tensor) else index for index in predicted_indices]\n",
        "\n",
        "        # Retrieve the top k predictions\n",
        "        top_k_predicted = set(predicted_indices[:k])\n",
        "        actual_indices_set = set(actual_indices)\n",
        "\n",
        "        # Calculate the number of correct predictions\n",
        "        correct_predictions = len(top_k_predicted.intersection(actual_indices_set))\n",
        "\n",
        "        # Calculate metrics\n",
        "        if correct_predictions > 0:\n",
        "            recall = float(correct_predictions) / len(actual_indices_set)\n",
        "            precision = float(correct_predictions) / k\n",
        "            if (precision + recall) != 0:\n",
        "                fscore = 2 * (precision * recall) / (precision + recall)\n",
        "            else:\n",
        "                fscore = 0.0\n",
        "        else:\n",
        "            recall = 0.0\n",
        "            precision = 0.0\n",
        "            fscore = 0.0\n",
        "\n",
        "        # Accumulate the metrics to calculate averages later\n",
        "        total_recall += recall\n",
        "        total_precision += precision\n",
        "        total_fscore += fscore\n",
        "\n",
        "    # Calculate average metrics\n",
        "    average_recall = total_recall / num_claims\n",
        "    average_precision = total_precision / num_claims\n",
        "    average_fscore = total_fscore / num_claims\n",
        "\n",
        "    return {\n",
        "        \"average_recall\": average_recall,\n",
        "        \"average_precision\": average_precision,\n",
        "        \"average_fscore\": average_fscore\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "results = evaluate_evidence_retrieval(reranked_indices, dev_evidence_idxs)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXQn60WB4-Kz"
      },
      "outputs": [],
      "source": [
        "test_claims = pd.read_json('/content/drive/MyDrive/nlp/data/test-claims-unlabelled.json', orient='index')\n",
        "test_claims.reset_index(inplace=True)\n",
        "test_claims.columns = ['claim_id', 'claim_text']\n",
        "test_claims_id = test_claims['claim_id']\n",
        "test_claims_text = test_claims['claim_text']\n",
        "\n",
        "test_claims_text_processed = test_claims_text.apply(preprocess_data)\n",
        "\n",
        "test_bm25_results = []\n",
        "for query in test_claims_text_processed:\n",
        "    scores = bm25_scores(query, inverted_index, idf_values, avg_doc_length)\n",
        "    test_bm25_results.append(scores)\n",
        "\n",
        "\n",
        "test_reranked_indices = [[doc_id for doc_id, _ in sorted(scores.items(), key=lambda x: x[1], reverse=True)] for scores in test_bm25_results]\n",
        "test_reranked_scores = [[score for _, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)] for scores in test_bm25_results]\n",
        "\n",
        "test_top_indices = topk_indices(test_reranked_indices, k=50)\n",
        "test_top_scores = topk_indices(test_reranked_scores, k=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J3uPcPn41-d"
      },
      "outputs": [],
      "source": [
        "json.dump(reranked_indices, open(\"/content/drive/MyDrive/nlp/data/reranked_indices.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sos_TwoMjpgY"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_scores = []\n",
        "for idx in range(len(test_claims_text_processed)):\n",
        "    top_k_evidence_idxs = test_top_indices[idx]\n",
        "    top_k_evidences = [evidence_text_processed[i] for i in top_k_evidence_idxs]\n",
        "    scores = score_query(model, test_claims_text_processed[idx], top_k_evidences, vocab, vocab['<pad>'])\n",
        "    test_scores.append(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDQUdxIyj_B7"
      },
      "outputs": [],
      "source": [
        "#test_scores_agg = aggregate_scores(dev_top_scores, dev_scores)\n",
        "\n",
        "test_reranked_indices = []\n",
        "\n",
        "for indices, scores in zip(test_top_indices, test_scores):\n",
        "    # Combine indices and scores into a list of tuples\n",
        "    indexed_scores = list(zip(indices, scores))\n",
        "\n",
        "    # Sort the list of tuples by the score in descending order\n",
        "    sorted_by_score = sorted(indexed_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Extract the sorted indices\n",
        "    sorted_indices = [idx for idx, _ in sorted_by_score]\n",
        "\n",
        "    # Add to the final list\n",
        "    test_reranked_indices.append(sorted_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1m3a1Xyo99Q"
      },
      "outputs": [],
      "source": [
        "test_indices_pred = topk_indices(test_reranked_indices, k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16F5gUfe3eLn"
      },
      "outputs": [],
      "source": [
        "json.dump(test_reranked_indices, open(\"/content/drive/MyDrive/nlp/data/test_reranked_indices.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvXBo5u6kC7j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}