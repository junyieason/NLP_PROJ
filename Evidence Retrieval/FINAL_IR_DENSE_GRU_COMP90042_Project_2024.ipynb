{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPitXZzTsyTn",
        "outputId": "9483a31c-aceb-4dfc-b110-020633348a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VcWPFMqtJ4x"
      },
      "outputs": [],
      "source": [
        "import torchdata.datapipes as dp\n",
        "import torchtext.transforms as T\n",
        "import spacy\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "eng = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohyt_MA86_7e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "evidences = pd.read_json('/content/drive/MyDrive/nlp/data/evidence.json', orient='index')\n",
        "train_claims = pd.read_json('/content/drive/MyDrive/nlp/data/train-claims.json', orient='index')\n",
        "dev_claims = pd.read_json('/content/drive/MyDrive/nlp/data/dev-claims.json', orient='index')\n",
        "\n",
        "#update column names\n",
        "evidences.reset_index(inplace=True)\n",
        "evidences.columns = ['evidence_id', 'evidence_text']\n",
        "\n",
        "train_claims.reset_index(inplace=True)\n",
        "train_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "dev_claims.reset_index(inplace=True)\n",
        "dev_claims.rename(columns={'index': 'claim_id'}, inplace=True)\n",
        "\n",
        "evidence_id = evidences['evidence_id']\n",
        "evidence_text = evidences['evidence_text']\n",
        "evidence_idx = evidences.index.tolist()\n",
        "\n",
        "evidence_id_dict = dict(zip(evidence_id, evidence_idx))\n",
        "\n",
        "train_claims_text = train_claims['claim_text']\n",
        "train_evidence_ids = train_claims['evidences']\n",
        "#map evidence_id to their corrosponding index for faster processing\n",
        "train_evidence_idxs = train_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])\n",
        "\n",
        "dev_claims_text = dev_claims['claim_text']\n",
        "dev_evidence_ids = dev_claims['evidences']\n",
        "dev_evidence_idxs = dev_evidence_ids.apply(lambda x: [evidence_id_dict[evidence_id] for evidence_id in x])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text preprocessing\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_data(text):\n",
        "    tokens = tt.tokenize(text)\n",
        "\n",
        "    processed_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        token = token.lower()\n",
        "        if token not in stopwords and token.isalpha():\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "            processed_tokens.append(lemma)\n",
        "\n",
        "    return processed_tokens\n",
        "\n",
        "train_claims_text_processed = train_claims_text.apply(preprocess_data)\n",
        "dev_claims_text_precessed = dev_claims_text.apply(preprocess_data)\n",
        "evidence_text_processed = evidence_text.apply(preprocess_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knrp2nykcB9_",
        "outputId": "1a1a87cb-4d83-4569-9c6a-f1f52c5d8808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=evidence_text_processed, vector_size=300, window=10, min_count=3, workers=8, sg=1, hs=0)"
      ],
      "metadata": {
        "id": "LAxRizfodbLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Word2Vec model\n",
        "model.save('/content/drive/MyDrive/nlp/data/word2vec_model.bin')\n"
      ],
      "metadata": {
        "id": "EM2DmVZRTB11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json.dump(train_claims_text_processed.tolist(), open(\"/content/drive/MyDrive/nlp/data/train_claims_text_processed.json\", \"w\"))\n",
        "json.dump(dev_claims_text_precessed.tolist(), open(\"/content/drive/MyDrive/nlp/data/dev_claims_text_precessed.json\", \"w\"))\n",
        "json.dump(evidence_text_processed.tolist(), open(\"/content/drive/MyDrive/nlp/data/evidence_text_processed.json\", \"w\"))"
      ],
      "metadata": {
        "id": "kgFTi-0XTxxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "train_claims_text_processed = json.load(open(\"/content/drive/MyDrive/nlp/data/train_claims_text_processed.json\", \"r\"))\n",
        "dev_claims_text_precessed = json.load(open(\"/content/drive/MyDrive/nlp/data/dev_claims_text_precessed.json\", \"r\"))\n",
        "evidence_text_processed = json.load(open(\"/content/drive/MyDrive/nlp/data/evidence_text_processed.json\", \"r\"))"
      ],
      "metadata": {
        "id": "zR4dnmajWB9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec.load('/content/drive/MyDrive/nlp/data/word2vec_model.bin')"
      ],
      "metadata": {
        "id": "HazfiHgeW9XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "evidence_text_array = np.array(evidence_text_processed, dtype=object)\n",
        "\n",
        "flattened_train_evidence_idxs = list(set([idx for sublist in train_evidence_idxs for idx in sublist]))\n",
        "\n",
        "train_evidence_text = evidence_text_array[flattened_train_evidence_idxs]"
      ],
      "metadata": {
        "id": "YS8T5uQW0gu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_evidence_text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESi8zhnr1co7",
        "outputId": "e80d1050-75f2-4e69-a79a-3717aca41938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['heat',\n",
              " 'likely',\n",
              " 'increase',\n",
              " 'risk',\n",
              " 'mortality',\n",
              " 'city',\n",
              " 'northern',\n",
              " 'part',\n",
              " 'country',\n",
              " 'southern',\n",
              " 'region',\n",
              " 'country']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=train_evidence_text, vector_size=300, window=10, min_count=3, workers=8, sg=1, hs=0)"
      ],
      "metadata": {
        "id": "Xsojxees05dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def generate_embedding(text, model):\n",
        "    # Filter out words that are not in the Word2Vec model's vocabulary\n",
        "    words = [word for word in text if word in model.wv.key_to_index]\n",
        "    if not words:  # Handle cases where none of the words are in the vocabulary\n",
        "        return np.zeros(model.vector_size)\n",
        "    # Get embeddings for each word in the text and average them\n",
        "    word_embeddings = [model.wv[word] for word in words]\n",
        "    sentence_embedding = np.mean(word_embeddings, axis=0)\n",
        "    return sentence_embedding"
      ],
      "metadata": {
        "id": "Db5C5dYpdgm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Generate embeddings for train claim_text\n",
        "train_claim_text_embeddings = [generate_embedding(text, model) for text in train_claims_text_processed]\n",
        "\n",
        "# Generate embeddings for dev claim_text\n",
        "dev_claim_text_embeddings = [generate_embedding(text, model) for text in dev_claims_text_precessed]\n",
        "\n",
        "# Generate embeddings for all evidence texts\n",
        "evidence_embeddings = [generate_embedding(text, model) for text in evidence_text_processed]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gasyfrL7dhA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to compute cosine similarity scores for all claims and evidence embeddings\n",
        "def compute_similarity_scores(claim_embeddings, evidence_embeddings):\n",
        "    similarity_scores = cosine_similarity(claim_embeddings, evidence_embeddings)\n",
        "    return similarity_scores\n",
        "\n",
        "\n",
        "# Compute cosine similarity scores for training claims and evidence embeddings\n",
        "train_similarity_scores = compute_similarity_scores(train_claim_text_embeddings, evidence_embeddings)\n",
        "dev_similarity_scores = compute_similarity_scores(dev_claim_text_embeddings, evidence_embeddings)"
      ],
      "metadata": {
        "id": "LFkd9dp_dhG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Function to compute recall at k\n",
        "def compute_recall_at_k(similarity_scores, true_indices, k):\n",
        "    recall_values = []\n",
        "\n",
        "    # Convert similarity scores to PyTorch tensor\n",
        "    similarity_scores_tensor = torch.FloatTensor(similarity_scores)\n",
        "\n",
        "    # Get top k indices for each sample\n",
        "    top_k_indices = torch.topk(similarity_scores_tensor, k, dim=-1).indices.tolist()\n",
        "\n",
        "    for i in range(len(true_indices)):\n",
        "        # Get true indices for the i-th sample\n",
        "        true_indices_i = true_indices[i]\n",
        "\n",
        "        # Calculate recall count for the i-th sample\n",
        "        recall_count = sum(1 for idx in true_indices_i if idx in top_k_indices[i])\n",
        "\n",
        "        # Calculate recall for the i-th sample\n",
        "        recall = recall_count / len(true_indices_i)\n",
        "\n",
        "        # Store recall value\n",
        "        recall_values.append(recall)\n",
        "\n",
        "    # Compute average recall over all samples\n",
        "    avg_recall = sum(recall_values) / len(recall_values)\n",
        "\n",
        "    return avg_recall\n",
        "\n",
        "# Example usage:\n",
        "# Compute recall at k for training set\n",
        "train_recall_at_k = compute_recall_at_k(train_similarity_scores, train_evidence_idxs, 500)\n",
        "print(\"Training Recall at K:\", train_recall_at_k)\n",
        "\n",
        "# Compute recall at k for dev set\n",
        "dev_recall_at_k = compute_recall_at_k(dev_similarity_scores, dev_evidence_idxs, 6)\n",
        "print(\"Dev Recall at K:\", dev_recall_at_k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_4j24NwdhJz",
        "outputId": "b3757fec-e9f7-49f5-f055-abe02d154d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Recall at K: 0.1245928338762216\n",
            "Dev Recall at K: 0.03398268398268398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def get_top_k_indices(similarity_scores, k=1000):\n",
        "    # Args:\n",
        "    # similarity_scores: a PyTorch tensor of shape (num_claims, num_evidences)\n",
        "    # Returns:\n",
        "    # top_k_indices: a tensor of shape (num_claims, k) containing indices of the top k elements\n",
        "\n",
        "    # Convert numpy array to PyTorch tensor if needed\n",
        "    if isinstance(similarity_scores, np.ndarray):\n",
        "        similarity_scores_tensor = torch.FloatTensor(similarity_scores)\n",
        "    else:\n",
        "        similarity_scores_tensor = similarity_scores\n",
        "\n",
        "    # Get top k indices for each sample using PyTorch\n",
        "    top_k_values, top_k_indices = torch.topk(similarity_scores_tensor, k, dim=1, largest=True, sorted=True)\n",
        "\n",
        "    return top_k_indices\n",
        "\n",
        "# Compute top 1000 indices for training and development sets\n",
        "train_top_indices = get_top_k_indices(train_similarity_scores, k=1000)\n",
        "dev_top_indices = get_top_k_indices(dev_similarity_scores, k=10)"
      ],
      "metadata": {
        "id": "mdqhdMWbdhMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_top_indices[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysbIZTgtYlHI",
        "outputId": "8ff4486c-c5e3-46af-bef8-911132231e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 611464,   60163,  288364,  845051,  601231, 1032935,  395937,  776992,\n",
              "        1029773,  519496,  167527,  609207, 1174437,  711638, 1154814,   21686,\n",
              "         714969,  198948,  749808,  490550,  442946,  500376, 1019366,   67788,\n",
              "         903458,  454739, 1145495,  872650, 1070491,  159167,  225665,  131293,\n",
              "         716982,  239651,   55562,  140540,  363502, 1089159,   56861,  264363,\n",
              "         371301,  804201,  512209, 1038450,  423643,  907938, 1076085,  885476,\n",
              "         439640,  377026,  544335,  501796, 1194317, 1028873,  963266,  682716,\n",
              "        1178054,  627012,  385414, 1110053,  113137, 1079703, 1091017, 1114128,\n",
              "         292570,  305506,  515748,  757808,  398396,  196321,  573869, 1165038,\n",
              "         995391,  363157,  881617,  343426,  384911,  218151, 1023876,  383923,\n",
              "        1134540,  810093, 1145973,  580126,  585382, 1015851,   74910,  204199,\n",
              "         595520,   26898, 1178854,  298082,  494161,  109299,  398589,  914091,\n",
              "         922561,  431652, 1138728,  141132,  806084,  630400,  409365, 1034818,\n",
              "        1022792,  499734, 1182164,  590324,  213457,  588973,  719864,  452564,\n",
              "        1067438,  997176,  792895,  105428,  920120,  481750,  480185,  830606,\n",
              "         803519, 1148608,   18767,  566933,  911303,  188007, 1032719,  345869,\n",
              "         538520, 1087369, 1119440,     526, 1075505,  839492,  419826, 1154586,\n",
              "         273498,  892616,  130047,  690259,  766238, 1163576,  119367,  670217,\n",
              "         184128,  312719,  398578,  348897,  130760, 1128720,  290195,  242918,\n",
              "        1077703,  563141,  616116,  424570, 1100468,  541023,  674550,  731868,\n",
              "         365252, 1132374, 1009038,  416189,  977483,  349143,  978683,  812466,\n",
              "         249155,  134668,  731987,  905792,  165607, 1018507,  165243,  104586,\n",
              "         533551,  233071,  688733,  894408,  493838,  378803,  202388,  297405,\n",
              "         474559,  302646,  774885, 1201845,  616423,  211431,  430923,  683706,\n",
              "          82008, 1141658,  544673, 1170996,  773860,  902179,  577794,  564520])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def prepare_train(train_claims_text, evidence_text_processed):\n",
        "  train_claims = []\n",
        "  train_evidences = []\n",
        "  train_labels = []\n",
        "\n",
        "  for i, claim in enumerate(train_claims_text):\n",
        "    positive_evidence_idxs = train_evidence_idxs[i]\n",
        "    negative_candidate_idxs = list(set(train_top_indices[i]) - set(positive_evidence_idxs))\n",
        "    negative_evidence_idxs = random.sample(negative_candidate_idxs, 20)\n",
        "\n",
        "    for idx in positive_evidence_idxs:\n",
        "      train_claims.append(claim)\n",
        "      train_evidences.append(evidence_text_processed[idx])\n",
        "      train_labels.append(1) #relevant evidence\n",
        "\n",
        "    for idx in negative_evidence_idxs:\n",
        "      idx = int(idx)\n",
        "      train_claims.append(claim)\n",
        "      train_evidences.append(evidence_text_processed[idx])\n",
        "      train_labels.append(-1) #irrelevant evidence\n",
        "\n",
        "  return train_claims, train_evidences, train_labels\n",
        "\n",
        "train_claims, train_evidences, train_labels = prepare_train(train_claims_text_processed, evidence_text_processed)\n"
      ],
      "metadata": {
        "id": "TmF767B5jaQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def prepare_evaluation_set(dev_claims_text, evidence_text_processed, evidence_idxs, top_indices):\n",
        "    eval_claims = []\n",
        "    eval_evidences = []\n",
        "    eval_labels = []\n",
        "\n",
        "    for i, claim in enumerate(dev_claims_text):\n",
        "        # Get all indices from the top indices\n",
        "        candidate_idxs = top_indices[i]\n",
        "\n",
        "        # Ground truth indices\n",
        "        positive_evidence_idxs = evidence_idxs[i]\n",
        "\n",
        "        for idx in candidate_idxs:\n",
        "            idx = idx.item()\n",
        "            eval_claims.append(claim)\n",
        "            eval_evidences.append(evidence_text_processed[idx])\n",
        "            # Check if this index is in the positive labels\n",
        "            if idx in positive_evidence_idxs:\n",
        "                eval_labels.append(1)  # Relevant evidence\n",
        "            else:\n",
        "                eval_labels.append(0)  # Irrelevant evidence\n",
        "\n",
        "    return eval_claims, eval_evidences, eval_labels\n",
        "\n",
        "dev_claims, dev_evidences, dev_labels = prepare_evaluation_set(dev_claims_text, evidence_text_processed, dev_evidence_idxs, dev_top_indices)\n"
      ],
      "metadata": {
        "id": "2djDgHXon69m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def prepare_training_pairs(claims, evidences, ground_truth_indices, top_indices):\n",
        "    train_claims = []\n",
        "    x1 = []\n",
        "    x2 = []\n",
        "    y = []\n",
        "\n",
        "    for i, claim in enumerate(claims):\n",
        "      positive_evidence_idxs = train_evidence_idxs[i]\n",
        "      negative_candidate_idxs = list(set(train_top_indices[i]) - set(positive_evidence_idxs))\n",
        "      #negative_evidence_idxs = random.sample(negative_candidate_idxs, 20)\n",
        "\n",
        "      for pos_idx in positive_evidence_idxs:\n",
        "        sampled_negatives = random.sample(negative_candidate_idxs, 10)\n",
        "        for neg_idx in sampled_negatives:\n",
        "          train_claims.append(claim)\n",
        "          x1.append(evidences[pos_idx])\n",
        "          x2.append(evidences[neg_idx])\n",
        "          y.append(1)\n",
        "\n",
        "    return train_claims, x1, x2, y\n",
        "train_claims, x1, x2, y = prepare_training_pairs(train_claims_text_processed, evidence_text_processed, train_evidence_idxs, train_top_indices)"
      ],
      "metadata": {
        "id": "N3Rt0BKo183J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "embedding_dim = model.vector_size\n",
        "vocab = {word: idx for idx, word in enumerate(model.wv.index_to_key)}\n",
        "\n",
        "# Calculate the current maximum index in the vocabulary\n",
        "max_index = max(vocab.values())\n",
        "\n",
        "\n",
        "# vocab['<cls>'] = max_index + 1\n",
        "# vocab['<sep>'] = max_index + 2\n",
        "vocab['<unk>'] = max_index + 1\n",
        "vocab['<pad>'] = max_index + 2\n",
        "\n",
        "#vocab['<unk>'] = len(vocab)\n",
        "\n",
        "# Create random embeddings for the three new special tokens\n",
        "random_embeddings = np.random.randn(1, embedding_dim)\n",
        "\n",
        "# Extend the embedding matrix with random embeddings for special tokens\n",
        "padding_embeddings = np.zeros((1, embedding_dim))  # Typically zero vector for padding\n",
        "extended_embeddings = np.vstack([\n",
        "    model.wv.vectors,  # Existing embeddings from Word2Vec\n",
        "    random_embeddings,  # Embeddings for `<unk>`\n",
        "    padding_embeddings  # Zero embeddings for `<pad>`\n",
        "])\n",
        "embedding_matrix = torch.FloatTensor(extended_embeddings)\n",
        "\n",
        "# Check new size of the embedding matrix and the maximum index in the vocabulary\n",
        "print(f\"New size of embedding matrix: {embedding_matrix.size(0)}\")\n",
        "print(f\"Maximum index in vocab: {max(vocab.values())}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-9QGjQiO45J",
        "outputId": "5a95b3e0-19c1-4195-9b71-182caf14b1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New size of embedding matrix: 144508\n",
            "Maximum index in vocab: 144507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(text, vocab):\n",
        "    return [vocab.get(token, vocab['<unk>']) for token in text]\n",
        "\n",
        "def convert_to_indices(train_claims, x1, x2, vocab):\n",
        "    train_claims_indices = []\n",
        "    x1_indices = []\n",
        "    x2_indices = []\n",
        "\n",
        "    # Convert train claims to indices\n",
        "    for claim in train_claims:\n",
        "        train_claims_indices.append(text_to_indices(claim, vocab))\n",
        "\n",
        "    # Convert x1 and x2 to indices\n",
        "    for evidence in x1:\n",
        "        x1_indices.append(text_to_indices(evidence, vocab))\n",
        "    for evidence in x2:\n",
        "        x2_indices.append(text_to_indices(evidence, vocab))\n",
        "\n",
        "    return train_claims_indices, x1_indices, x2_indices\n",
        "\n",
        "train_claims_indices, x1_indices, x2_indices = convert_to_indices(train_claims, x1, x2, vocab)\n",
        "\n",
        "# def convert_to_indicies(claims, evidences, vocab):\n",
        "#     train_claims_indices = []\n",
        "#     evidences_indices = []\n",
        "\n",
        "#     for claim in claims:\n",
        "#         train_claims_indices.append(text_to_indices(claim, vocab))\n",
        "\n",
        "#     for evidence in evidences:\n",
        "#         evidences_indices.append(text_to_indices(evidence, vocab))\n",
        "\n",
        "#     return train_claims_indices, evidences_indices\n",
        "\n",
        "# train_claims_indices, evidences_indices = convert_to_indicies(train_claims, train_evidences, vocab)\n"
      ],
      "metadata": {
        "id": "hrH_rKkRImjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class EvidenceRankingDataset(Dataset):\n",
        "    def __init__(self, claims, evidences1, evidences2, labels):\n",
        "        self.claims = claims\n",
        "        self.evidences1 = evidences1\n",
        "        self.evidences2 = evidences2\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.claims[idx], self.evidences1[idx], self.evidences2[idx], self.labels[idx])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    claims, evidences1, evidences2, labels = zip(*batch)\n",
        "    claims_padded = pad_sequence([torch.tensor(claim, dtype=torch.long) for claim in claims], batch_first=True, padding_value=vocab['<pad>'])\n",
        "    evidences1_padded = pad_sequence([torch.tensor(evidence1, dtype=torch.long) for evidence1 in evidences1], batch_first=True, padding_value=vocab['<pad>'])\n",
        "    evidences2_padded = pad_sequence([torch.tensor(evidence2, dtype=torch.long) for evidence2 in evidences2], batch_first=True, padding_value=vocab['<pad>'])\n",
        "    labels = torch.tensor(labels, dtype=torch.float)\n",
        "    return claims_padded, evidences1_padded, evidences2_padded, labels\n",
        "\n",
        "\n",
        "train_dataset = EvidenceRankingDataset(train_claims_indices, x1_indices, x2_indices, y)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "cWy4peJhd9Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.gru = nn.GRU(embedding_matrix.size(1), hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward_one(self, claim, evidence):\n",
        "        # Embed and process claim\n",
        "        claim_emb = self.embedding(claim)\n",
        "        _, claim_hidden = self.gru(claim_emb)\n",
        "        claim_hidden = claim_hidden.squeeze(0)  # Ensure shape is [batch_size, hidden_dim]\n",
        "\n",
        "        # Embed and process evidence\n",
        "        evidence_emb = self.embedding(evidence)\n",
        "        _, evidence_hidden = self.gru(evidence_emb)\n",
        "        evidence_hidden = evidence_hidden.squeeze(0)  # Ensure shape is [batch_size, hidden_dim]\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarity = F.cosine_similarity(claim_hidden, evidence_hidden, dim=1)\n",
        "        return similarity\n",
        "\n",
        "    def forward(self, claim, evidence1, evidence2):\n",
        "        # Compute cosine similarity for each evidence compared to the same claim\n",
        "        similarity1 = self.forward_one(claim, evidence1)\n",
        "        similarity2 = self.forward_one(claim, evidence2)\n",
        "        return similarity1, similarity2\n",
        "\n"
      ],
      "metadata": {
        "id": "J1oQmWvdeAKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn import MarginRankingLoss\n",
        "import torch.optim as optim\n",
        "\n",
        "# DataLoader\n",
        "#dataset = EvidenceRankingDataset(train_claims_indices, evidence_pairs, labels)\n",
        "#train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gru_model = SiameseNetwork(embedding_matrix, hidden_dim=1024).to(device)\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.005)\n",
        "criterion = MarginRankingLoss(margin=0.7)\n",
        "\n",
        "# Training loop\n",
        "def train(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for claims, evidences1, evidences2, labels in loader:\n",
        "        claims, evidences1, evidences2, labels = claims.to(device), evidences1.to(device), evidences2.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        scores1, scores2 = model(claims, evidences1, evidences2)\n",
        "        loss = criterion(scores1, scores2, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "for epoch in range(10):\n",
        "    epoch_loss = train(gru_model, train_loader, optimizer, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V_-Eg1x16M0",
        "outputId": "d12ea205-58f3-4cb6-ec16-35ff4e3ffe28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6997\n",
            "Epoch 2, Loss: 0.5854\n",
            "Epoch 3, Loss: 0.4496\n",
            "Epoch 4, Loss: 0.4108\n",
            "Epoch 5, Loss: 0.3654\n",
            "Epoch 6, Loss: 0.3287\n",
            "Epoch 7, Loss: 0.3149\n",
            "Epoch 8, Loss: 0.2557\n",
            "Epoch 9, Loss: 0.2396\n",
            "Epoch 10, Loss: 0.2347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_evidence_retrieval(predicted_indices_list, actual_indices_list, k=5):\n",
        "    assert len(predicted_indices_list) == len(actual_indices_list), \"Both inputs must have the same length.\"\n",
        "\n",
        "    total_recall = 0.0\n",
        "    total_precision = 0.0\n",
        "    total_fscore = 0.0\n",
        "    num_claims = len(predicted_indices_list)\n",
        "\n",
        "    for predicted_indices, actual_indices in zip(predicted_indices_list, actual_indices_list):\n",
        "        # Retrieve the top k predictions\n",
        "        top_k_predicted = set(predicted_indices[:k])\n",
        "        actual_indices_set = set(actual_indices)\n",
        "\n",
        "        # Calculate the number of correct predictions\n",
        "        correct_predictions = len(top_k_predicted.intersection(actual_indices_set))\n",
        "\n",
        "        # Calculate metrics\n",
        "        if correct_predictions > 0:\n",
        "            recall = float(correct_predictions) / len(actual_indices_set)\n",
        "            precision = float(correct_predictions) / k\n",
        "            if (precision + recall) != 0:\n",
        "                fscore = 2 * (precision * recall) / (precision + recall)\n",
        "            else:\n",
        "                fscore = 0.0\n",
        "        else:\n",
        "            recall = 0.0\n",
        "            precision = 0.0\n",
        "            fscore = 0.0\n",
        "\n",
        "        # Accumulate the metrics to calculate averages later\n",
        "        total_recall += recall\n",
        "        total_precision += precision\n",
        "        total_fscore += fscore\n",
        "\n",
        "    # Calculate average metrics\n",
        "    average_recall = total_recall / num_claims\n",
        "    average_precision = total_precision / num_claims\n",
        "    average_fscore = total_fscore / num_claims\n",
        "\n",
        "    return {\n",
        "        \"average_recall\": average_recall,\n",
        "        \"average_precision\": average_precision,\n",
        "        \"average_fscore\": average_fscore\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "predicted_indices_list = [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n",
        "actual_indices_list = [[1, 2, 3], [7, 11, 8, 10]]\n",
        "results = evaluate_evidence_retrieval(predicted_indices_list, actual_indices_list)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB2EW6M6ukpe",
        "outputId": "c14f0215-732e-4dd3-aae6-990da6bdda48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'average_recall': 0.75, 'average_precision': 0.5, 'average_fscore': 0.5972222222222222}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# def prepare_evaluation_set(dev_claims_text, evidence_text_processed, top_indices):\n",
        "#     eval_claims = []\n",
        "#     eval_evidences = []\n",
        "#     eval_evidences_idxs = []\n",
        "\n",
        "#     for i, claim in enumerate(dev_claims_text):\n",
        "#       claim_top_indices = top_indices[i]\n",
        "\n",
        "#       for idx in claim_top_indices:\n",
        "#         #idx = idx.item()\n",
        "#         eval_claims.append(claim)\n",
        "#         eval_evidences.append(evidence_text_processed[idx])\n",
        "#         eval_evidences_idxs.append(idx)\n",
        "\n",
        "\n",
        "#     return eval_claims, eval_evidences, eval_evidences_idxs\n",
        "\n",
        "# dev_claims, dev_evidences, dev_evidences_idxs = prepare_evaluation_set(dev_claims_text_precessed, evidence_text_processed, dev_top_indices)\n",
        "\n",
        "def convert_to_indices(claims, all_evidences, vocab):\n",
        "    claims_indices = [torch.tensor([vocab.get(token, vocab['<unk>']) for token in claim], dtype=torch.long) for claim in claims]\n",
        "    all_evidences_indices = [[torch.tensor([vocab.get(token, vocab['<unk>']) for token in evidence], dtype=torch.long) for evidence in evidences] for evidences in all_evidences]\n",
        "    return claims_indices, all_evidences_indices\n",
        "\n",
        "# dev_claims_indices, dev_evidence_indices, _ = convert_to_indices(dev_claims, dev_evidences, dev_evidences, vocab)\n",
        "\n",
        "def prepare_evaluation_set(dev_claims_text, evidence_text_processed, top_indices):\n",
        "    eval_claims = []\n",
        "    eval_all_evidences = []\n",
        "    eval_all_evidence_idxs = []\n",
        "\n",
        "    # Iterate over each claim\n",
        "    for i, claim in enumerate(dev_claims_text):\n",
        "        claim_evidences = []\n",
        "        claim_evidence_idxs = []\n",
        "\n",
        "        # Get the list of top indices for the current claim\n",
        "        claim_top_indices = top_indices[i]\n",
        "\n",
        "        # Append each evidence corresponding to the current claim\n",
        "        for idx in claim_top_indices:\n",
        "            claim_evidences.append(evidence_text_processed[idx])\n",
        "            claim_evidence_idxs.append(idx)\n",
        "\n",
        "        # Append the aggregated evidence lists and their indices for the current claim\n",
        "        eval_claims.append(claim)\n",
        "        eval_all_evidences.append(claim_evidences)\n",
        "        eval_all_evidence_idxs.append(claim_evidence_idxs)\n",
        "\n",
        "    return eval_claims, eval_all_evidences, eval_all_evidence_idxs\n",
        "\n",
        "# Example usage\n",
        "dev_claims, dev_all_evidences, dev_all_evidence_idxs = prepare_evaluation_set(dev_claims_text_precessed, evidence_text_processed, dev_top_indices)\n",
        "dev_claims_indices, dev_all_evidence_indices = convert_to_indices(dev_claims, dev_all_evidences, vocab)\n"
      ],
      "metadata": {
        "id": "YVbIWQczKTPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "class EvaluateDataset(Dataset):\n",
        "    def __init__(self, claims, all_evidences, all_evidence_idxs):\n",
        "        self.claims = claims\n",
        "        self.all_evidences = all_evidences\n",
        "        self.all_evidence_idxs = all_evidence_idxs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.claims)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.claims[idx], self.all_evidences[idx], self.all_evidence_idxs[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    claims, all_evidences, all_evidence_idxs = zip(*batch)\n",
        "    claims_padded = pad_sequence([claim for claim in claims], batch_first=True, padding_value=vocab['<pad>'])\n",
        "\n",
        "    # Handling variable number of evidences per claim\n",
        "    all_evidences_padded = [pad_sequence([evidence for evidence in evidences], batch_first=True, padding_value=vocab['<pad>']) for evidences in all_evidences]\n",
        "\n",
        "    return claims_padded, all_evidences_padded, all_evidence_idxs\n",
        "\n",
        "# DataLoader creation\n",
        "eval_dataset = EvaluateDataset(dev_claims_indices, dev_all_evidence_indices, dev_all_evidence_idxs)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "0ziWOAMkd2lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, device, k=5):\n",
        "    model.eval()\n",
        "    results = []\n",
        "    with torch.no_grad():\n",
        "        for claims, all_evidences, all_evidence_idxs in data_loader:\n",
        "            scores = []\n",
        "            sorted_evidence_idxs = []\n",
        "\n",
        "            # Assuming each batch contains one claim with all its evidences\n",
        "            claim = claims.to(device)\n",
        "\n",
        "            # Process each evidence set\n",
        "            for evidences, evidence_idxs in zip(all_evidences, all_evidence_idxs):\n",
        "                evidence_scores = []\n",
        "                for evidence in evidences:\n",
        "                    evidence = evidence.to(device)\n",
        "                    score = model.forward_one(claim, evidence)\n",
        "                    evidence_scores.append(score)\n",
        "\n",
        "                evidence_scores = torch.tensor(evidence_scores)\n",
        "                #probabilities = F.softmax(evidence_scores, dim=0)\n",
        "\n",
        "                #zip(evidence_scores, evidence_idxs)\n",
        "                sorted_indices = torch.argsort(evidence_scores, descending=True)\n",
        "                top_k_indices = sorted_indices[:k]\n",
        "\n",
        "                top_k_evidence_indices = [evidence_idxs[idx].item() for idx in top_k_indices]\n",
        "                sorted_evidence_idxs.append(top_k_evidence_indices)\n",
        "\n",
        "            results.append(sorted_evidence_idxs)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Assume eval_loader is properly defined, as well as gru_model and device\n",
        "sorted_indices_per_claim = evaluate(gru_model, eval_loader, device, k=5)"
      ],
      "metadata": {
        "id": "-qtLB3KSlksb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_indices_per_claim[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fn1TgOKEY0N",
        "outputId": "5b3b6b2c-a9e0-4d1a-ffb7-24337102985d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[208053, 142506, 36224, 213568, 1099333]]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LXQn60WB4-Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(predicted_indices, ground_truth_indices):\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1 = 0\n",
        "    num_claims = len(predicted_indices)\n",
        "\n",
        "    for predicted, true in zip(predicted_indices, ground_truth_indices):\n",
        "        true_set = set(true)\n",
        "        predicted_set = set(item for sublist in predicted for item in sublist)\n",
        "        #print(predicted_set)\n",
        "\n",
        "        # Calculate intersections for true positives\n",
        "        true_positives = len(predicted_set & true_set)\n",
        "\n",
        "        #print(true_set)\n",
        "\n",
        "        # Calculate precision and recall\n",
        "        if len(predicted_set) == 0:\n",
        "            precision = 0\n",
        "        else:\n",
        "            precision = true_positives / len(predicted_set)\n",
        "\n",
        "        if len(true_set) == 0:\n",
        "            recall = 0\n",
        "        else:\n",
        "            recall = true_positives / len(true_set)\n",
        "\n",
        "        # Calculate F1 score\n",
        "        if precision + recall == 0:\n",
        "            f1 = 0\n",
        "        else:\n",
        "            f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "        # Accumulate metrics to compute average later\n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "        total_f1 += f1\n",
        "\n",
        "    # Compute average metrics\n",
        "    average_precision = total_precision / num_claims\n",
        "    average_recall = total_recall / num_claims\n",
        "    average_f1 = total_f1 / num_claims\n",
        "\n",
        "    return average_precision, average_recall, average_f1\n",
        "\n",
        "# Usage\n",
        "average_precision, average_recall, average_f1 = calculate_metrics(sorted_indices_per_claim, dev_evidence_idxs)\n",
        "print(f\"Average Precision: {average_precision:.4f}\")\n",
        "print(f\"Average Recall: {average_recall:.4f}\")\n",
        "print(f\"Average F1 Score: {average_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3QuJ9Kt2jPs",
        "outputId": "0a37be71-7e41-4da2-c660-82bde60a13c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision: 0.0455\n",
            "Average Recall: 0.0742\n",
            "Average F1 Score: 0.0526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_evidence_idxs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T96uhGe2HLCc",
        "outputId": "d84a9725-35e0-42bd-ebf9-2dedf353fecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[996421, 1080858, 208053, 699212, 832334]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path\n",
        "save_path = '/content/drive/MyDrive/nlp/data/gru_model.bin'\n",
        "\n",
        "# Saving the model's state dictionary\n",
        "torch.save(gru_model.state_dict(), save_path)\n",
        "\n",
        "#model.save(gru_model.state_dict(), '/content/drive/MyDrive/nlp/data/gru_model.bin')"
      ],
      "metadata": {
        "id": "0lvDvHVK77Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path\n",
        "load_path = '/content/drive/MyDrive/nlp/data/gru_model.bin'\n",
        "\n",
        "# Loading the model's state dictionary\n",
        "gru_model = SiameseNetwork(embedding_matrix, hidden_dim=128)  # Re-create the model structure\n",
        "gru_model.load_state_dict(torch.load(load_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FBr3uuE_1li",
        "outputId": "03aa5757-2db0-42c1-c428-582ef966f3ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}